<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google Professional Machine Learning Engneer問題集 01</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="quiz-container">
        <div id="quiz-data" style="display: none;">


<div class='question' data-multiple='false' data-question='問題12<br>あなたのチームは、何百万人もの顧客が利用するグローバルな銀行のアプリケーションを構築しています。あなたは、3日先の顧客の口座残高を予測する予測モデルを構築しました。あなたのチームはその結果を、口座残高が25ドル以下になりそうなときにユーザに通知する新機能で使用します。<br>どのように予測を提供すべきですか？' data-answer='2' data-explanation='解説<br>正解は「1.Firebase上に通知システムを構築します<br>2.Firebase Cloud MessagingサーバーにユーザIDを登録し、ユーザーの口座残高が$25の閾値を下回るとモデルが予測したときに通知を送信します」です。<br>この問題では、銀行アプリケーションという大規模な顧客ベースを持つシステムで、特定の条件下でユーザーに通知を送るための最適な手段を選択することが求められています。重要な要点は口座残高が25ドル以下になりそうなときに個々のユーザーに通知する必要がある、という点であり、一方で特定の全体的な条件（例えば、全アカウントの残高予測の平均など）で通知を送るというシナリオは考慮する必要がありません。また、FirebaseやPub/Sub、Cloud Functionなどの独自の通知機構に関する知識も必要とされます。<br>基本的な概念や原則：<br>Firebase：Google CloudのモバイルおよびWebアプリケーション開発プラットフォームです。ユーザ認証、データベース、アナリティクス、クラウドメッセージングなどのサービスを提供します。<br>Firebase Cloud Messaging：Firebaseの一部として提供される、プッシュ通知とメッセージングのサービスです。デバイス間でメッセージを配信し、アプリがバックグラウンドにある場合や終了した状態でも通知を送ることができます。<br>Pub/Sub：Google Cloudのリアルタイムメッセージングサービスです。システム間またはサービス間でメッセージを非同期に交換します。<br>Cloud Functions：Google Cloudのイベント駆動型のサーバレス計算サービスです。特定のイベントに応じてコードを自動的に実行することができます。<br>App Engine：Google Cloudのフルマネージドなアプリケーション開発とホスティングのプラットフォームです。スケーリング、アプリケーションの健全性の監視、セキュリティーなどの機能があります。<br>予測モデル：機械学習により作成される、特定の出力を予想するための数学的モデルです。このケースでは、顧客の口座残高を予測するために使用します。<br>ユーザ通知：アプリケーションがユーザに対して重要な情報を伝えるための手段です。このケースでは、口座残高が一定の閾値を下回る可能性があるときに通知を送ります。<br>正解についての説明：<br>（選択肢）<br>・1.Firebase上に通知システムを構築します<br>2.Firebase Cloud MessagingサーバーにユーザIDを登録し、ユーザーの口座残高が$25の閾値を下回るとモデルが予測したときに通知を送信します<br>この選択肢が正解の理由は以下の通りです。<br>まず、Firebaseはモバイルアプリケーション開発のためのプラットフォームで、別に用意する必要のない機能の豊富さと柔軟性が備わっており、このような通知システムの構築に適しています。Firebase上に通知システムを構築することで、現在のアプリケーションのアーキテクチャを大幅に変更することなく、新機能を追加することが可能になります。<br>さらに、Firebase Cloud Messagingサーバーは、ユーザーにプッシュ通知を送信するためのサービスです。このサービスを使い、ユーザーの口座残高が$25以下になる可能性があるとモデルが予測した際に、ユーザーに通知を送信することで、予測結果をリアルタイムにユーザーに伝えることが可能になります。<br>これらの要素は、一貫したユーザー体験の提供と、必要な情報をユーザーに効率的に伝えるためには、最適な解決策であると言えます。<br>不正解の選択肢についての説明：<br>選択肢：1.各ユーザにPub/Subトピックを作成します<br>2.ユーザーのアカウント残高が閾値 $25を下回るとモデルが予測したときに通知を送信するCloud Functionsを展開します<br>この選択肢が正しくない理由は以下の通りです。<br>Pub/Subトピックを各ユーザに作成するというアプローチは、数百万の顧客を持つ大規模なサービスには適していません。それは大量のリソースを要するだけでなく、管理も複雑化します。それに比べ、Firebaseを使うことでスケーラビリティを確保しつつ通知システムを効率的に構築できます。<br>選択肢：1.各ユーザにPub/Subトピックを作成します<br>2.App Engineスタンダード環境にアプリケーションをデプロイします。このアプリケーションは、あるユーザーのアカウント残高が閾値 $25を下回るとモデルが予測したときに通知を送信します<br>この選択肢が正しくない理由は以下の通りです。<br>各ユーザーにPub/Subトピックを作成するアプローチは、何百万人ものユーザーがいる場合には効率的ではありません。大量のトピックを管理するための運用コストが増大します。<br>一方、Firebaseはスケール可能で、ユーザ固有の通知を簡単に送信できます。<br>選択肢：1.Firebase上に通知システムを構築します<br>2.Firebase Cloud MessagingサーバーにユーザIDを登録し、全アカウントの残高予測の平均が$25の閾値を下回ったら通知を送ります<br>この選択肢が正しくない理由は以下の通りです。<br>全アカウントの残高予測の平均を利用すると、個々のユーザーの口座残高に基づいた通知を提供できません。通知は個々のユーザが25ドル以下になりそうなときに送るべきです。<br>したがって、全体の平均ではなく個々の予測を利用するべきです。'>
<div class='choice'><br>1.Firebase上に通知システムを構築します<br>2.Firebase Cloud MessagingサーバーにユーザIDを登録し、全アカウントの残高予測の平均が$25の閾値を下回ったら通知を送ります</div>
<div class='choice'><br>1.各ユーザにPub/Subトピックを作成します<br>2.ユーザーのアカウント残高が閾値 $25を下回るとモデルが予測したときに通知を送信するCloud Functionsを展開します</div>
<div class='choice'><br>1.Firebase上に通知システムを構築します<br>2.Firebase Cloud MessagingサーバーにユーザIDを登録し、ユーザーの口座残高が$25の閾値を下回るとモデルが予測したときに通知を送信します</div>
<div class='choice'><br>1.各ユーザにPub/Subトピックを作成します<br>2.App Engineスタンダード環境にアプリケーションをデプロイします。このアプリケーションは、あるユーザーのアカウント残高が閾値 $25を下回るとモデルが予測したときに通知を送信します</div>
</div>

<div class='question' data-multiple='false' data-question='問題13<br>あなたは会社の主任MLエンジニアとして、スキャンされた顧客フォームをデジタル化するためのMLモデルを構築する責任を負っています。スキャンされた画像をテキストに変換し、Cloud Storageに保存するTensorFlowモデルを開発しました。手作業による介入を最小限に抑えながら、毎日の終わりに収集された集約データに対してMLモデルを使用する必要があります。<br>この要件を満たすために、どうすればよいですか？' data-answer='2' data-explanation='解説<br>正解は「AI Platformのバッチ予測機能を利用します」です。<br>この問題では、機械学習モデルの活用方法と組織の運用要件を理解することが求められます。ここで重要な要件は、毎日の終わりに集約データ全体に対してモデルを適用し、手作業の介入を最小限に抑えるという点です。したがって、一度に大量のデータを処理でき、かつ自動化が可能な解決策を選ぶことが重要です。このような要件に最適な選択肢を探すことで、適切な解答を選ぶことができます。<br>基本的な概念や原則：<br>AI Platform：Google Cloudの機械学習モデルの開発と管理を支援するサービスです。モデルのトレーニング、評価、予測などの機能を提供します。<br>バッチ予測：一度に大規模なデータセットに対して予測を行うモードです。複数の入力データへ一斉にモデルの予測を適用します。AI Platformの機能の一部です。<br>TensorFlow：Googleが開発したオープンソースの機械学習フレームワークです。ディープラーニングモデルの設計、トレーニング、テストが可能です。<br>Cloud Storage：Google Cloudのオブジェクトストレージサービスです。大量のデータを安全に保管し、いつでもどこでもアクセス可能です。<br>Cloud Functions：Google Cloudのサーバレス実行環境です。単一のイベントに応じて小規模なコード断片を実行します。<br>オンライン予測：リアルタイムのデータに対する予測を行うモードです。リアルタイム要件に対応しますが、連続的な大量のデータに対応するためにはコストが増えます。AI Platformの機能の一部です。<br>正解についての説明：<br>（選択肢）<br>・AI Platformのバッチ予測機能を利用します<br>この選択肢が正解の理由は以下の通りです。<br>まず、AI Platformのバッチ予測機能を使用することで、一度に大量のデータに対してMLモデルを適用でき、結果をCloud Storageに保存できます。これは、毎日の終わりに収集された集約データに対してMLモデルを一括で適用するために最適な方法です。バッチ予測機能は、既存のMLモデルに対する大量の入力データに対する予測を非同期で処理します。これにより、一連のフォーム全体を一度に処理し、結果を保存することができます。<br>さらに、これは手作業による介入を最小限に抑えることができます。AI Platformのバッチ予測機能は、管理が必要なインフラストラクチャを設定することなく、スケーラブルな環境で大量の予測を行うことを可能にします。これにより、エンジニアの作業負荷が軽減され、手作業による介入が最小限に抑えられます。<br>不正解の選択肢についての説明：<br>選択肢：Compute Engineに予測用のサービングパイプラインを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>Compute Engineに予測用のサービングパイプラインを作成すると、手動での管理とメンテナンスが必要になり、手作業による介入を最小限に抑えるという要件に反します。<br>一方、AI Platformのバッチ予測機能を利用すると、需要に応じてスケールする予測ジョブをモデルから自動的に生成し運用することができます。<br>選択肢：新しいデータポイントが取り込まれるたびに、Cloud Functionsを使って予測を行います<br>この選択肢が正しくない理由は以下の通りです。<br>新しいデータポイントが取り込まれるたびにCloud Functionsを使うと、即時性が必要なシナリオに適していますが、毎日の終わりにデータを集約して処理したい場合には適していません。<br>一方、AI Platformのバッチ予測機能は複数のデータをまとめて予測するため、日次の大量データを扱うには適しています。<br>選択肢：AI Platform上にモデルをデプロイし、オンライン推論用のバージョンを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>オンライン推論はリアルタイムの推論要求に対応するためのものであり、毎日の終わりに集約されたデータへの一括処理には適していません。<br>それに対して、AI Platformのバッチ予測は大量のデータを一度に処理するのに適合した機能であり、ここでの要件に合致します。'>
<div class='choice'> AI Platform上にモデルをデプロイし、オンライン推論用のバージョンを作成します</div>
<div class='choice'> 新しいデータポイントが取り込まれるたびに、Cloud Functionsを使って予測を行います</div>
<div class='choice'> AI Platformのバッチ予測機能を利用します</div>
<div class='choice'> Compute Engineに予測用のサービングパイプラインを作成します</div>
</div>

<div class='question' data-multiple='false' data-question='問題14<br>Vertex AIにリアルタイム推論用のモデルを導入しました。オンライン予測リクエスト中に"Out of Memory"エラーが発生しました。<br>あなたは、トラブルシューティングとして何をすればよいですか？' data-answer='0' data-explanation='解説<br>正解は「インスタンスのバッチを減らしてリクエストを再送します」です。<br>この問題では、Vertex AIのモデルの運用中にリアルタイムの予測リクエストが"Out of Memory"エラーにより失敗した際の対応策について問われています。エラーメッセージはメモリ不足を示しているため、各選択肢がこの問題をどのように解決できるのか、または解決できないのかに注目することが求められます。適切なトラブルシューティングの戦略を選択するためには、メモリリソースをどのように効率的に使用するかに焦点を当てる必要があります。<br>基本的な概念や原則：<br>インスタンスのバッチ：予測リクエストを一括処理するためにグループ化されたリクエストのことです。一度に多くのリクエストを処理すると、メモリ不足が発生することがあります。<br>Out of Memoryエラー：使用可能な物理メモリまたは仮想メモリが不足している状態を指します。このエラーは、プログラムが多くのデータを消費しているときや、リソースが適切にクリーンアップされていない場合に発生します。<br>Vertex AI：Google Cloudの統一されたAI Platformで、モデルのトレーニングからデプロイまでを統合的に管理できます。<br>リアルタイム推論：予測リクエストがリアルタイムで処理されることを指します。スピーディな結果が必要な場合に利用されます。<br>バッチ予測：大量の予測リクエストを一度に処理することを指します。結果の取得速度がそれほど重要でない場合に利用されます。<br>base64エンコード：バイナリデータをテキスト形式に変換する方法の一つです。データのサイズが増加するため、メモリ不足を引き起こす可能性があります。<br>リソースの割り当て：システムリソース（CPU、メモリ、ディスク容量など）の利用可能量を設定することです。不足している場合、割り当てを増やすことでエラーを解消できる場合があります。<br>正解についての説明：<br>（選択肢）<br>・インスタンスのバッチを減らしてリクエストを再送します<br>この選択肢が正解の理由は以下の通りです。<br>Out of Memoryエラーは、リクエストを処理するためのメモリが不足していることを示します。これは、送信されたリクエストが多すぎるか、リクエストサイズが大きすぎるために発生する可能性があります。問題を解決するための一般的な方法としては、リクエストのバッチサイズを減らすことが挙げられます。これは、一度に処理されるリクエストの数を減らすことで、メモリ使用量を減少させることができます。<br>Vertex AIのモデルは一度に複数の予測リクエストを処理することができますが、これらのリクエストが一度にメモリにロードされるため、メモリから溢れてエラーが発生することがあります。<br>したがって、イラストのバッチを減らしてリクエストを再送することで、メモリ使用量を制御しながら予測を続けることができ、"Out of Memory"エラーの解消につながります。<br>不正解の選択肢についての説明：<br>選択肢：オンラインモードではなく、バッチ予測モードを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>オンラインモードからバッチ予測モードへの切り替えは、予測のリアルタイム性を失わせます。問題のエラーはメモリ不足が原因で、予測モードの変更ではなく、単純にインスタンスのバッチ量を減らすことで解消できます。<br>選択肢：予測に使用する前に、base64を使用してデータをエンコードします<br>この選択肢が正しくない理由は以下の通りです。<br>Out of Memoryエラーはメモリ不足により発生する問題であり、base64を使用してデータをエンコードすることはこの問題を解消するものではありません。データのエンコードはデータの形式を変更するもので、これによりメモリ使用量が減るとは限らず、さらに可能性としてはメモリ使用量が増える可能性もあります。問題を解決するために、正解の選択肢にあるようにインスタンスのバッチを減らすことでメモリ使用量を抑えるべきです。<br>選択肢：予想リクエスト数の割り当ての増加を申請します<br>この選択肢が正しくない理由は以下の通りです。<br>Out of Memoryエラーはメモリ不足を指すため、予想リクエスト数の割り当てを増やすことは解決策になりません。<br>一方、インスタンスのバッチを減らすことで、1回あたりのリクエストがメモリ内に収まるようになり、エラーを解決します。'>
<div class='choice'> インスタンスのバッチを減らしてリクエストを再送します</div>
<div class='choice'> 予想リクエスト数の割り当ての増加を申請します</div>
<div class='choice'> オンラインモードではなく、バッチ予測モードを使用します</div>
<div class='choice'> 予測に使用する前に、base64を使用してデータをエンコードします</div>
</div>

<div class='question' data-multiple='false' data-question='問題15<br>あなたはResNet50アーキテクチャに基づくPyTorchを使った画像認識モデルを開発しています。あなたのコードはローカルのノートパソコンで小さなサブサンプルで問題なく動作しています。完全なデータセットには20万枚のラベル付き画像があります。あなたは、コストを最小限に抑えながら、学習ワークロードを迅速に拡張したいと考えています。また、4つのV100 GPUを使用する予定です。<br>この要件を満たすために、どうすればよいですか？' data-answer='2' data-explanation='解説<br>正解は「Setuptoolsを使用してコードをパッケージ化し、ビルド済みのコンテナを使用します。必要なGPUを含むカスタム階層を使用して、Vertex AIでモデルをトレーニングします」です。<br>この問題では、大規模な画像データを使った機械学習モデルの訓練をコストを抑えつつ効率的に行う最適な方法が求められています。機械学習のワークロードを効率的にスケールアップするためには、適切な機械学習サービスとGPUの利用が鍵です。また、解答を見つける上で重要な情報は、使用する機械学習フレームワーク（PyTorch）、モデル（ResNet50アーキテクチャ）、データ量（20万枚の画像）と使用予定のリソース（4つのV100 GPU）であると理解することです。これらの要素に基づき、コードの実行・パッケージ化方法と適切なサービスを選択します。<br>基本的な概念や原則：<br>Setuptools：Pythonでパッケージを作成するためのライブラリです。コードの共有や再利用を容易にします。<br>コンテナ：一つ以上のアプリケーションとその依存関係をパッケージ化し、実行環境を一定に保つための技術です。<br>Vertex AI：Google Cloudのサービスで、AIモデルの作成からデプロイまでを一元化した場所です。GPUを含むカスタム階層の提供も可能です。<br>GPU：Graphics Processing Unitの略で、高速に並列処理が可能なプロセッサです。V100はNVIDIAのGPUモデルで、機械学習のトレーニングによく使用されます。<br>Google Kubernetes Engine：Google CloudのマネージドKubernetesサービスです。クラスター管理やオートスケーリングなどの機能が提供されます。<br>TFJobオペレータ：Kubernetesのカスタムリソースで、TensorFlowの分散トレーニングジョブの実行を管理します。<br>Vertex AI Workbench：Vertex AIの一部で、ノートブックインスタンスなどを使用して、データ分析や機械学習のワークロードを実行するための環境です。<br>Compute Engine VM：Google Cloudの仮想マシンサービスです。強力な計算能力と柔軟性を提供し、様々なワークロードに対応します。<br>正解についての説明：<br>（選択肢）<br>・Setuptoolsを使用してコードをパッケージ化し、ビルド済みのコンテナを使用します。必要なGPUを含むカスタム階層を使用して、Vertex AIでモデルをトレーニングします<br>この選択肢が正解の理由は以下の通りです。<br>まず、Setuptoolsを使用してコードをパッケージ化することは、Pythonプロジェクトを他の環境に移行するときに必要です。このパッケージ化により、プロジェクトとその依存関係を一元管理でき、新しい環境でも同じ動作が保証されます。<br>次に、ビルド済みのコンテナを使用するという点ですが、Google Cloudでは、各種機械学習フレームワークのビルド済みコンテナイメージが提供されています。これにより、ローカルとクラウド環境で一貫性を保った実行環境が得られ、開発効率やデバッグの容易性が向上します。<br>最後に、Vertex AIでモデルをトレーニングするという選択肢は、スケーラブルなモデルトレーニングと4つのV100 GPUを使用する要件を満たします。Vertex AIはフルマネージド型のAI Platformで、ユーザーがGPU数を指定でき、必要なリソースを確保することができます。これにより、コストを抑えながら大規模なワークロードを迅速にスケールすることができます。<br>不正解の選択肢についての説明：<br>選択肢：4つのV100 GPUを持つノードプールでGoogle Kubernetes Engineクラスターを作成します。このノードプールにTFJobオペレータを準備し、実行します<br>この選択肢が正しくない理由は以下の通りです。<br>TFJobオペレータはTensorFlowを使用したジョブ向けであり、このケースのPyTorchでは対応していません。<br>また、Google Kubernetes Engineの設定と管理はコストが掛かるため、Vertex AIでフルマネージドな環境を使用する方がコスト効率的でタスクを迅速に拡張するのに適しています。<br>選択肢：4つのV100 GPUでVertex AI Workbenchユーザ管理ノートブックインスタンスを作成し、モデルのトレーニングに使用します<br>この選択肢が正しくない理由は以下の通りです。<br>Vertex AI Workbenchユーザ管理ノートブックインスタンスは、主に対話型のデータ探索やプロトタイピングに適しています。そのため、大量のデータセットでの学習ワークロード拡張や、特にコストを最小限に抑える目的には最適ではありません。この目的には、Vertex AIでモデルをトレーニングする方が適しています。<br>選択肢：トレーニングを開始するすべての依存関係を持つCompute Engine VMを構成します。必要なGPUを含むカスタム階層を使用して、Vertex AIでモデルをトレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>Compute Engine VMを使用すると、コードのパッケージ化や依存関係の管理が手動であり、効率が悪くコストも増大する可能性があります。正解の選択肢では、Setuptoolsとコンテナを使用することで自動化と効率化を図り、コストを最小限に抑えることができます。'>
<div class='choice'> 4つのV100 GPUを持つノードプールでGoogle Kubernetes Engineクラスターを作成します。このノードプールにTFJobオペレータを準備し、実行します</div>
<div class='choice'> トレーニングを開始するすべての依存関係を持つCompute Engine VMを構成します。必要なGPUを含むカスタム階層を使用して、Vertex AIでモデルをトレーニングします</div>
<div class='choice'> Setuptoolsを使用してコードをパッケージ化し、ビルド済みのコンテナを使用します。必要なGPUを含むカスタム階層を使用して、Vertex AIでモデルをトレーニングします</div>
<div class='choice'> 4つのV100 GPUでVertex AI Workbenchユーザ管理ノートブックインスタンスを作成し、モデルのトレーニングに使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題16<br>あなたはサイバーセキュリティ組織のためにシステムログの異常検知モデルを開発しています。TensorFlowを使用してモデルを開発し、リアルタイム予測に使用する予定です。Pub/Sub経由でデータを取り込み、結果をBigQueryに書き込むDataflowパイプラインを作成する必要があります。あなたは、サービングレイテンシをできるだけ小さくしたいと考えています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「モデルをVertex AIエンドポイントにデプロイし、Dataflowジョブでこのエンドポイントを呼び出します」です。<br>この問題では、システムの異常検知モデルを開発するためのシナリオを扱っています。鍵となる要件を把握し、それを満たすための最適な手段を見つけることが求められています。具体的には、リアルタイムの予測能力、公開/購読システムを利用したデータ取得、BigQueryへの結果の書き込み必要性、そして重要な要素として、低いサービングレイテンシを維持することが求められています。これらの要件を満たすために最適なGoogle Cloudサービスとその組み合わせを考慮しなければなりません。<br>基本的な概念や原則：<br>Vertex AIエンドポイント：Google Cloud上で訓練されたMLモデルを評価、使うためのエンドポイントです。リアルタイムで予測を行うことができます。<br>Dataflow：ストリームおよびバッチデータの処理を行うフルマネージドサービスです。Apache Beam SDKを使用してジョブを作成し、大量のデータを効率的に処理することができます。<br>TensorFlow：ディープラーニングモデルの訓練とサービングを行うためのオープンソースフレームワークです。<br>Pub/Sub：リアルタイムメッセージングサービスです。データ生成システムとデータ処理システム間で情報を効率的に転送します。<br>BigQuery：Google Cloudのフルマネージド、サーバレスのビッグデータ分析サービスです。大規模な分析クエリを高速に実行することができます。<br>Cloud Run：コンテナ化したアプリケーションをサーバレスで実行するためのサービスです。<br>Google Kubernetes Engine（GKE）：Google Cloud上でKubernetesクラスターを作成、運用するためのマネージドサービスです。<br>正解についての説明：<br>（選択肢）<br>・モデルをVertex AIエンドポイントにデプロイし、Dataflowジョブでこのエンドポイントを呼び出します<br>この選択肢が正解の理由は以下の通りです。<br>まず、Vertex AIはGoogle Cloudの全面的な機械学習プラットフォームで、モデルをホストし、大量の予測リクエストをリアルタイムで高速に処理することができます。これにより、求められたサービングレイテンシの短縮を実現できます。<br>また、Dataflowのジョブを使用すれば、各ジョブがPub/Subから取り込んだデータをリアルタイムでVertex AIのエンドポイントに渡し予測を取得し、その結果をBigQueryに書き込むことができます。これにより、リアルタイムの異常検知情報の高速取得とストレージまでのパイプラインが確立されます。<br>したがって、モデルをVertex AIエンドポイントにデプロイし、Dataflowジョブでこのエンドポイントを呼び出すという選択は、高速な異常検知システムを構築する上で揃えておくべき要件を全て満たしています。<br>不正解の選択肢についての説明：<br>選択肢：Dataflowから呼び出されるCloud Runにモデル予測ロジックをコンテナ化します<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud Runではコールドスタート時のレイテンシが発生する可能性があり、常に最小のレイテンシを求める要件には合いません。<br>一方、Vertex AIエンドポイントはモデルへのリクエストに対して常に低レイテンシで応答するように設計されています。<br>選択肢：モデルを依存関係としてDataflowジョブに直接ロードし、予測に使用します<br>この選択肢が正しくない理由は以下の通りです。<br>モデルを依存関係としてDataflowジョブに直接ロードするアプローチは、スケーリングや効率的なリソース管理ができません。<br>一方、Vertex AIエンドポイントにデプロイすることで、モデルのスケーリングと効率的なリソース使用が可能となり、サービングレイテンシの最小化も達成できます。<br>選択肢：モデルをGoogle Kubernetes Engine上のTFServingコンテナにデプロイし、Dataflowジョブで呼び出します<br>この選択肢が正しくない理由は以下の通りです。<br>Google Kubernetes Engine上のTFServingコンテナを使用すると、管理コストやオーバーヘッドが増加します。その間、Vertex AIエンドポイントは、モデルの管理とデプロイを自動化する機能を提供し、サービングレイテンシを最小限にすることができます。'>
<div class='choice'> モデルをGoogle Kubernetes Engine上のTFServingコンテナにデプロイし、Dataflowジョブで呼び出します</div>
<div class='choice'> モデルをVertex AIエンドポイントにデプロイし、Dataflowジョブでこのエンドポイントを呼び出します</div>
<div class='choice'> Dataflowから呼び出されるCloud Runにモデル予測ロジックをコンテナ化します</div>
<div class='choice'> モデルを依存関係としてDataflowジョブに直接ロードし、予測に使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題17<br>特定のミッションクリティカルな機械部品が故障するかどうかを判断するために、非同期予測を提供するアーキテクチャを設計する必要があります。このシステムでは、機械の複数のセンサーからデータを収集します。あなたは、過去12時間の各センサーのデータの平均が与えられた場合に、今後N分以内に故障を予測するモデルを構築したいと考えています。<br>どのようにアーキテクチャを設計すべきですか？' data-answer='1' data-explanation='解説<br>正解は「1.イベントはセンサーからPub/Subに送られ、リアルタイムで消費され、Dataflowストリーム処理パイプラインで処理されます<br>2.パイプラインは予測のためにモデルを起動し、予測を別のPub/Subトピックに送信します<br>3.予測を含むPub/Subメッセージは、モニタリングのために下流のシステムによって消費されます」です。<br>この問題では、非同期予測を提供するアーキテクチャの設計について問われています。ミッションクリティカルな機械部品が故障するかを予測するためのモデルを構築するに当たり、かつそれを実行するアーキテクチャが必要です。センサーからのデータを収集し、過去12時間の各センサーデータの平均を用いて、今後N分以内に故障を予測することを要求しています。ここで注意すべき点は、システムのリアルタイムな反応性と、大量のセンサーデータを適切に処理する能力です。また、適切なツールやサービスを選択することでシステム設計を最適化することが求められています。<br>基本的な概念や原則：<br>Pub/Sub：Google Cloudのリアルタイムメッセージングサービスです。生産者と消費者が分離しているため、大量のデータを効率的に処理できます。<br>Dataflow：Google Cloudのストリームとバッチ処理の両方を行うデータ処理サービスです。データのフィルタリング、集約、変換などが可能です。<br>MLモデル：機械学習のアルゴリズムが適用されるデータのモデルです。MLモデルは、新しいデータが与えられると予測を行います。<br>REST API：HTTPプロトコルをベースにしたAPIの設計モデルです。データの送受信を行うために使用されます。<br>Vertex AI：Google Cloudの機械学習プラットフォームです。モデルのトレーニングからデプロイまでを一元的に管理できます。<br>Cloud Storage：Google Cloudのオブジェクトストレージサービスです。ラージデータの格納やバックアップに適しています。<br>Cloud SQL：Google Cloudのフルマネージドなリレーショナルデータベースサービスです。MySQL, PostgreSQL, SQL Serverなどのデータベースの管理をGoogle Cloudが行います。<br>正解についての説明：<br>（選択肢）<br>・1.イベントはセンサーからPub/Subに送られ、リアルタイムで消費され、Dataflowストリーム処理パイプラインで処理されます<br>2.パイプラインは予測のためにモデルを起動し、予測を別のPub/Subトピックに送信します<br>3.予測を含むPub/Subメッセージは、モニタリングのために下流のシステムによって消費されます<br>この選択肢が正解の理由は以下の通りです。<br>まず、ミッションにおける故障予測はリアルタイムで行われるべきです。そのため、Google CloudのPub/SubとDataflowの組み合わせが有効です。Pub/Subは、リアルタイムでのメッセージングシステムで、データを安全に収集し、データを分析するDataflowに送ることができます。Dataflowは、バッチデータとリアルタイムデータストリームの両方を効率的に処理することができます。<br>次に、Dataflowがモデルを起動して予測を行い、その結果をPub/Subに送ることで、リアルタイムの応答能力を保つことができます。<br>さらに、予測を含むメッセージが別のPub/Subトピックにプッシュされることで、故障予測の結果をすぐに下流のシステムで利用することができます。この流れは、予測結果の迅速なモニタリングを可能にします。これらの理由から、選択肢が故障予測のための適切なアーキテクチャと言えます。<br>不正解の選択肢についての説明：<br>選択肢：1.HTTPリクエストは、マイクロサービスとしてデプロイされ、予測用のREST APIを公開するMLモデルにセンサーから送信されます<br>2.アプリケーションは、モデルをデプロイしたVertex AIエンドポイントに問い合わせます<br>3.応答は、モデルが予測を生成するとすぐに、呼び出し元のアプリケーションによって受信されます<br>この選択肢が正しくない理由は以下の通りです。<br>本問題のシナリオでは、複数のセンサーから非同期でデータを収集し、即時に予測を行う必要があります。そのため、リアルタイムのストリーム処理が必要で、REST APIを介したHTTPリクエストではレイテンシがあり、非同期予測のニーズを満たすことが難しいです。<br>また、Vertex AIエンドポイントは大量のセンサーデータに対してリアルタイム処理を行うのに最適ではないため、この選択肢は不適切です。<br>選択肢：1.Dataflowを使ってデータをCloud Storageにエクスポートします<br>2.前処理されたデータに対してスコアリングを実行するために、Cloud Storage内の学習済みモデルを使用するVertex AIバッチ予測ジョブを送信します<br>3.バッチ予測ジョブの出力をCloud Storageからエクスポートし、Cloud SQLにインポートします<br>この選択肢が正しくない理由は以下の通りです。<br>まず、エクスポート、インポート、ストレージの操作は非同期操作であり、必要なリアルタイムの分析を行うことが困難です。つまり、この選択肢は問題の"非同期予測"の要件を満たしていません。<br>また、必要なリアルタイム性を考慮すると、Cloud StorageとCloud SQLの使用は適切ではありません。<br>正解の選択肢は、リアルタイムなデータ処理と予測を提供するGoogle Cloudの各サービスを使用することを提案しております。<br>選択肢：1. BigQueryコマンドラインツールを使用してデータをCloud Storageにエクスポートします<br>2. Cloud Storageのトレーニング済みモデルを使用して前処理されたデータのスコアリングを実行するVertex AIバッチ予測ジョブを送信します<br>3. バッチ予測ジョブの出力をCloud Storageからエクスポートし、BigQueryにインポートします<br>この選択肢が正しくない理由は以下の通りです。<br>まず、非同期予測を実現するために、リアルタイム性が求められますが、BigQueryやCloud Storageといったツールを使うとバッチ処理になり、リアルタイム性に欠けます。<br>また、Vertex AIのバッチ予測は一括で予測を行うものであり、今後N分以内の故障予測等、時間経過による変化に対する予測をコスト効率よく行うためには、Pub/SubとDataflowのストリーム処理の方がより適切です。'>
<div class='choice'><br>1. BigQueryコマンドラインツールを使用してデータをCloud Storageにエクスポートします<br>2. Cloud Storageのトレーニング済みモデルを使用して前処理されたデータのスコアリングを実行するVertex AIバッチ予測ジョブを送信します<br>3. バッチ予測ジョブの出力をCloud Storageからエクスポートし、BigQueryにインポートします</div>
<div class='choice'><br>1.イベントはセンサーからPub/Subに送られ、リアルタイムで消費され、Dataflowストリーム処理パイプラインで処理されます<br>2.パイプラインは予測のためにモデルを起動し、予測を別のPub/Subトピックに送信します<br>3.予測を含むPub/Subメッセージは、モニタリングのために下流のシステムによって消費されます</div>
<div class='choice'><br>1.HTTPリクエストは、マイクロサービスとしてデプロイされ、予測用のREST APIを公開するMLモデルにセンサーから送信されます<br>2.アプリケーションは、モデルをデプロイしたVertex AIエンドポイントに問い合わせます<br>3.応答は、モデルが予測を生成するとすぐに、呼び出し元のアプリケーションによって受信されます</div>
<div class='choice'><br>1.Dataflowを使ってデータをCloud Storageにエクスポートします<br>2.前処理されたデータに対してスコアリングを実行するために、Cloud Storage内の学習済みモデルを使用するVertex AIバッチ予測ジョブを送信します<br>3.バッチ予測ジョブの出力をCloud Storageからエクスポートし、Cloud SQLにインポートします</div>
</div>

<div class='question' data-multiple='false' data-question='問題18<br>あなたは、ある産業機器製造会社のデータサイエンテストです。あなたは、全工場から収集したセンサーデータに基づいて、同社の製造工場の電力消費量を推定する回帰モデルを開発しています。センサーは毎日数千万レコードを収集します。あなたは、現在の日付までに収集されたすべてのデータを使用するモデルのトレーニング実行を、毎日スケジュールする必要があります。モデルをスムーズに拡張し、最小限の開発作業で済むようにしたいと考えています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「BigQuery MLを使用して回帰モデルを開発します」です。<br>この問題では、大量のセンサーデータを使って工場の電力消費量を推定する回帰モデルを毎日スケジュールしてトレーニングするための最適な解決策を選ぶことが求められています。重要なのはデータ量が多いためスケーリングが必要であり、毎日のスケジューリングと最小限の開発作業が要求されている点です。そのため、回帰モデルのスケーリング、日次スケジューリング、開発作業の簡素化を効率的に実現できる選択肢を選ばなければなりません。<br>基本的な概念や原則：<br>BigQuery ML：BigQueryデータセットに対して機械学習モデルの構築と予測を行うサービスです。SQLクエリのみでモデルの訓練、評価、予測が可能です。<br>回帰モデル：出力が連続した値（例えば電力消費量）であるような問題を解決するための機械学習モデルです。<br>AutoML Table：構造化データ（テーブルデータ）のためのGoogle Cloudの自動機械学習ツールです。モデル訓練からデプロイまですべてを自動化します。<br>Vertex AI Training：機械学習モデルの訓練をGoogle Cloud上で行うためのサービスです。複数のフレームワークをサポートし、カスタムコードを使ってモデルを訓練できます。<br>TensorFlow：Googleにより開発されたオープンソースの機械学習フレームワークです。Python APIを通じて手軽に学習・推論を行える他、GPUやTPUでの高速化もサポートします。<br>scikit-learn：Pythonのオープンソース機械学習ライブラリです。多様なアルゴリズムを搭載し、簡単なAPIでデータの前処理、学習、評価が可能です。<br>正解についての説明：<br>（選択肢）<br>・BigQuery MLを使用して回帰モデルを開発します<br>この選択肢が正解の理由は以下の通りです。<br>まず、BigQuery MLは、大量のデータを迅速に分析し、そのデータから機械学習モデルを構築することができるGoogle Cloudのサービスです。回帰モデルのようなモデルを開発する作業は、データを分析し、特徴を抽出する大量のコンピューティングリソースを必要としますが、BigQuery MLはあらゆる規模のデータに対応できるため、数千万レコードのセンサーデータを毎日処理するのに適しています。<br>また、BigQuery MLはSQLの構文を使用してモデルトレーニングを行うため、データサイエンテストが従来のプログラミング言語で複雑なモデルをコーディングする必要がありません。これにより開発作業を最小限に抑えることができます。<br>さらに、BigQuery MLは既存のBigQuery ETLワークフローに統合され、パイプライン内の任意のポイントでモデルを更新できるため、毎日のスケジュールでモデルのトレーニングを実行するのに適しています。<br>不正解の選択肢についての説明：<br>選択肢：AutoML Tableを使用して回帰モデルを学習します<br>この選択肢が正しくない理由は以下の通りです。<br>AutoML Tablesは高度なモデルの訓練や最適化を行うための機能が満載のサービスであり、大量のデータを繰り返しトレーニングするとコストが高くなる可能性があります。<br>それに対して、BigQuery MLはSQLクエリを利用し、摩擦なく大きなデータセットを使った機械学習モデルの訓練と予測を行うことができ、最低限の開発作業でスムーズに拡張する要件が達成されやすいです。<br>選択肢：カスタムTensorFlow回帰モデルを開発し、Vertex AI Trainingを使用して最適化します<br>この選択肢が正しくない理由は以下の通りです。<br>カスタムTensorFlow回帰モデルを開発し、Vertex AI Trainingを使用する手法は、モデルの開発と管理に膨大な作業時間と専門知識を必要とします。<br>一方、BigQuery MLを使用すれば、既存のデータに基づきモデルの開発とスケジュール訓練が容易に実現可能で、作業の簡略化とスムーズな拡張に適しています。<br>選択肢：カスタムのscikit-learn回帰モデルを開発し、Vertex AI Trainingを使用して最適化します<br>この選択肢が正しくない理由は以下の通りです。<br>カスタムのscikit-learnモデルを開発し、Vertex AI Trainingを使用すれば確かにモデルを最適化できますが、最小限の開発作業という要件には合致しません。BigQuery MLを使用すれば、SQLクエリだけでモデル開発からトレーニングまで行えるため、より少ない開発作業で要件を満たせます。'>
<div class='choice'> AutoML Tableを使用して回帰モデルを学習します</div>
<div class='choice'> BigQuery MLを使用して回帰モデルを開発します</div>
<div class='choice'> カスタムのscikit-learn回帰モデルを開発し、Vertex AI Trainingを使用して最適化します</div>
<div class='choice'> カスタムTensorFlow回帰モデルを開発し、Vertex AI Trainingを使用して最適化します</div>
</div>

<div class='question' data-multiple='false' data-question='問題19<br>あなたは最近、組織のフレームワーク特有の重要な依存関係を使用するカスタムニューラルネットワークを設計し構築しました。Google Cloud上のマネージドトレーニングサービスを使用してモデルをトレーニングする必要があります。しかし、MLフレームワークと関連する依存関係は、AI Platform Trainingではサポートされていません。また、モデルとデータの両方が大きすぎて、1台のマシンのメモリに収まりません。選択したMLフレームワークは、スケジューラー、ワーカー、サーバーの分散構造を使用しています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='0' data-explanation='解説<br>正解は「カスタムコンテナを構築して、AI Platform Training上で分散トレーニングジョブを実行します」です。<br>この問題では、現行のMLフレームワークと依存関係がGoogle CloudのAI Platform Trainingでサポートされていない状況で、大規模なデータとモデルをどのように効率的にトレーニングするかが求められています。この問題の読解においては、MLフレームワークが分散構造を用いているという情報を踏まえ、依存関係が複数ある状況下でもGoogle Cloud上で分散トレーニングジョブを効果的に実行する方法を選択することが重要です。<br>基本的な概念や原則：<br>AI Platform Training：Google CloudのフルマネージドMLトレーニングサービスです。ユーザーが自身のMLモデルを大規模にトレーニングすることを可能にします。<br>カスタムコンテナ：特定のアプリケーションやサービスの実行環境をパッケージ化したものです。必要な依存関係や設定を含むことができます。<br>分散トレーニング：1台のマシンでは取り扱えないほど大きなデータやモデルに対して行われるトレーニングの方法です。複数台のマシン（ワーカー）が協調してトレーニングを行います。<br>MLフレームワークの依存関係：MLフレームワークが正常に機能するために必要なソフトウェアやライブラリのことです。これらが未サポートまたは非互換である場合、カスタムコンテナにパッケージ化することで問題を解決できます。<br>組み込みモデル：AI Platform Trainingで提供される、すでに構築・最適化されたMLモデルのことです。一部の一般的なタスクやアプリケーションに対して使用できますが、固有の要件や制約には対応できません。<br>スケジューラー、ワーカー、サーバーの分散構造：MLフレームワーク一部は、これらの構成要素を通じて分散計算を実行します。この構造をサポートするフレームワークでは、大規模なモデルやデータセットを扱うことができます。<br>正解についての説明：<br>（選択肢）<br>・カスタムコンテナを構築して、AI Platform Training上で分散トレーニングジョブを実行します<br>この選択肢が正解の理由は以下の通りです。<br>Google CloudではAI Platform Trainingを使ってマシン学習モデルをトレーニングすることができますが、問題の要件によれば特定のMLフレームワーク及びその依存関係がAI Platform Trainingではサポートされていないため、直接使用することはできません。しかしカスタムコンテナを構築することによって、特定のフレームワークやその依存関係を含めることができます。そのため、必要となるフレームワークや依存関係を持つカスタムコンテナを作り、それをAI Platform Training上で実行することで分散トレーニングを実現できます。<br>また、AI Platform Trainingは分散構造（スケジューラー、ワーカー、サーバー）をサポートしているため、モデルとデータが1台のマシンのメモリに収まらない問題も解決できます。以上の理由で、この選択肢が最も適切です。<br>不正解の選択肢についての説明：<br>選択肢：AI Platform Trainingで利用可能な組み込みモデルを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>問題文には、既存のMLフレームワークと依存関係がAI Platform Trainingでサポートされていないことが明記されています。そのため、AI Platform Trainingで利用可能な組み込みモデルを使用するというオプションは適しておらず、カスタムコンテナを構築し、これを用いて分散トレーニングジョブを実行するのが適切な選択です。<br>選択肢：AI Platform Trainingでジョブを実行するためのカスタムコンテナを構築します<br>この選択肢が正しくない理由は以下の通りです。<br>カスタムコンテナを構築すること自体は問題ありませんが、分散トレーニングの要件に対応するためには、"AI Platform Training上で分散トレーニングジョブを実行する"ことが必要です。<br>したがって、この選択肢は分散トレーニングを明示的に含んでいないため、不適切です。<br>選択肢：AI Platform Trainingがサポートする依存関係を持つMLフレームワークにコードを再構成します<br>この選択肢が正しくない理由は以下の通りです。<br>試験のシナリオでは、MLフレームワークとその依存関係がAI Platform Trainingではサポートされていないと指定されています。そのため、AI Platform Trainingがサポートするフレームワークにコードを再構成することは、その要件を満たす解決策ではありません。<br>また、モデルとデータが大規模なため、分散トレーニングが必要であり、カスタムコンテナを使用することでこれを可能にできます。'>
<div class='choice'> カスタムコンテナを構築して、AI Platform Training上で分散トレーニングジョブを実行します</div>
<div class='choice'> AI Platform Trainingがサポートする依存関係を持つMLフレームワークにコードを再構成します</div>
<div class='choice'> AI Platform Trainingでジョブを実行するためのカスタムコンテナを構築します</div>
<div class='choice'> AI Platform Trainingで利用可能な組み込みモデルを使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題20<br>あなたは雑誌販売会社に勤めており、どの顧客が次年度の定期購読を更新するかを予測するモデルを構築する必要があります。自社の過去のデータをトレーニングセットとして使用し、TensorFlowモデルを作成し、AI Platformにデプロイしました。モデルによって提供される各予測について、どの顧客属性が最も予測力があるかを判断する必要があります。<br>この要件を満たすために、どうすればよいですか？' data-answer='3' data-explanation='解説<br>正解は「AI PlatformのAI Explanations機能を使用します。各予測リクエストに&#39;explain&#39;キーワードを付けて送信すると、サンプリングシャープレイ法を使用した特徴属性が取得されます」です。<br>この問題では、TensorFlowモデルを使って顧客の次年度の定期購読の更新を予測し、予測力が強い顧客の属性を判断する方法について問われています。要件を満たすためには、予測に対して説明性を持たせる何らかの手段を用いる必要があります。ここで、TensorFlowモデルのそれぞれの特徴が予測にどの程度寄与しているかを解読するための適切なツールや機能を選択することが重要です。問題の文脈から、AI Platformの特定の機能やGoogle Cloudの特定のツールの利用が考慮されます。<br>基本的な概念や原則：<br>AI Platform：Google Cloudの機械学習モデルのトレーニング、デプロイ、予測を行うサービスです。TensorFlowなど様々なフレームワークに対応しています。<br>TensorFlow：Googleが開発したオープンソースの機械学習フレームワークです。ディープラーニングから古典的な機械学習まで幅広いアルゴリズムをサポートしています。<br>AI Explanations：AI Platformの機能で、機械学習モデルの予測結果に対する説明を提供します。モデルの各特徴が予測にどの程度影響を与えたかを知ることができます。<br>サンプリングシャープレイ法：ゲーム理論を基にした手法で、特徴の影響度を求めるのに使用されます。特徴の価値を公平に分配することを目指しています。<br>LASSO回帰：特徴選択のための統計的手法で、不要な特徴を削減するのに使用されます。しかし、この手法はモデルの解釈性を増す一方で、精度を低下させる可能性があります。<br>ピアソン相関係数：2つの変数間の線形相関関係の強度を測る指標です。しかし、変数間の非線形な関連性は捉えられません。<br>What-Ifツール：モデルの挙動をビジュアルで理解するためのツールです。特徴の影響度を調査する際に役立つ一方、全ての特徴の影響を正確に評価するには限界があります。<br>正解についての説明：<br>（選択肢）<br>・AI PlatformのAI Explanations機能を使用します。各予測リクエストに&#39;explain&#39;キーワードを付けて送信すると、サンプリングシャープレイ法を使用した特徴属性が取得されます<br>この選択肢が正解の理由は以下の通りです。<br>まず、AI ExplanationsはGoogle CloudのAI Platformで提供される機能であり、機械学習モデルの予測結果を理解と解釈するためのツールです。&#39;explain&#39;キーワードを付けて予測リクエストを送信すると、AI Platformは特定の予測がどの程度各特徴属性に依存しているかを示すデータを返します。これによって、各予測に最も影響を与えた顧客属性を定量的に評価することが可能になります。<br>その中でも、サンプリングシャープレイ法は注目すべき特徴を評価するための一般的な方法であり、具体的な貢献度を計算して解釈を可能にします。この方法が提供する洞察は、モデルの予測力を最大化し、予測結果に影響を与える顧客属性を特定するのに極めて役立ちます。<br>このような理由から、AI PlatformのAI Explanations機能を使用することは、それぞれの顧客属性が予測結果にどの程度影響を与えるかを判断するための最善の方法といえるでしょう。<br>不正解の選択肢についての説明：<br>選択肢：AI Platformのノートブックを使って、モデルに対してLASSO回帰分析を行い、強いシグナルを提供しない特徴を除去します<br>この選択肢が正しくない理由は以下の通りです。<br>LASSO回帰分析は特徴選択手法の一つですが、既にトレーニングとデプロイが完了したTensorFlowモデルに対して後付けで適用することはできません。<br>一方、正解のAI Explanationsは、既存のモデルに関連付けて、各特徴の説明力を直接提供する機能で、要件を満たします。<br>選択肢：予測結果をBigQueryにストリームします。BigQueryのCORR(X1, X2)関数を使用して、各特徴とターゲット変数間のピアソン相関係数を計算します<br>この選択肢が正しくない理由は以下の通りです。<br>BigQueryのCORR関数で計算するピアソン相関係数は、特徴間の線形の依存関係しか評価できません。<br>一方、AI Explanationsのサンプリングシャープレイ法は、線形非線形を問わず、各特徴が予測にどれだけ影響を与えているか評価できます。このため、より精度の高い予測結果の解釈が可能です。<br>選択肢：Google CloudのWhat-Ifツールを使って、個々の特徴を除外したときのモデルのパフォーマンスを調べます。モデルから除外したときにパフォーマンスが最も大きく低下した特徴の順に、特徴の重要度をランク付けします<br>この選択肢が正しくない理由は以下の通りです。<br>What-Ifツールは、特定の特徴がモデルのパフォーマンスにどのように影響するかを確認するためのビジュアル化を提供しますが、AI Explanationsのように特定の予測における特徴の重要性をランク付けて直接提供する機能はありません。そのため、各予測においてどの顧客属性が最も予測力があるかを判断するために、What-IfツールではなくAI Explanationsを使用する方が効率的です。'>
<div class='choice'> AI Platformのノートブックを使って、モデルに対してLASSO回帰分析を行い、強いシグナルを提供しない特徴を除去します</div>
<div class='choice'> Google CloudのWhat-Ifツールを使って、個々の特徴を除外したときのモデルのパフォーマンスを調べます。モデルから除外したときにパフォーマンスが最も大きく低下した特徴の順に、特徴の重要度をランク付けします</div>
<div class='choice'> 予測結果をBigQueryにストリームします。BigQueryのCORR(X1, X2)関数を使用して、各特徴とターゲット変数間のピアソン相関係数を計算します</div>
<div class='choice'> AI PlatformのAI Explanations機能を使用します。各予測リクエストに&#39;explain&#39;キーワードを付けて送信すると、サンプリングシャープレイ法を使用した特徴属性が取得されます</div>
</div>

<div class='question' data-multiple='false' data-question='問題21<br>あなたは、複数の地域に店舗を持つ大手食料品小売企業のMLエンジニアです。あなたは在庫予測モデルの作成を依頼されました。あなたのモデルの特徴には、地域、場所、過去の需要、季節的な人気が含まれます。あなたは、アルゴリズムに毎日の新しい在庫データから学習させたいと考えています。<br>モデルを構築するためにどのアルゴリズムを使うべきですか？' data-answer='2' data-explanation='解説<br>正解は「リカレントニューラルネットワーク（RNN）」です。<br>この問題では、在庫予測モデルの作成に最適なアルゴリズムを選択することが求められています。問題文には地域、場所、過去の需要、季節的な人気などの特徴と、毎日の新しい在庫データから学習する必要があるという情報が含まれています。つまり、時系列的なパターンやトレンドを捉えることが重要であることと、連続的な学習を可能とするアルゴリズムが求められています。これらの要点を把握して選択肢を評価することが重要です。<br>基本的な概念や原則：<br>リカレントニューラルネットワーク（RNN）：時間的な連続性を持つデータを扱うためのアルゴリズムです。過去の入力データが現在の出力に影響を与える特性があり、在庫予測などの時系列データに適しています。<br>分類：データを事前に定義された複数のカテゴリに分けるための機械学習手法です。在庫予測のような連続的な数値を予測する問題には適していません。<br>強化学習：エージェントが環境と相互作用しながら学ぶ機械学習の手法です。報酬信号に基づいて行動を学習しますが、在庫予測では適用が難しいです。<br>畳み込みニューラルネットワーク（CNN）：画像の特徴を学習するためのニューラルネットワークです。在庫予測のような時系列データには適していません。<br>特徴量：機械学習モデルの学習に使用されるデータの各属性です。この問題では地域、場所、過去の需要、季節的な人気が特徴量です。<br>正解についての説明：<br>（選択肢）<br>・リカレントニューラルネットワーク（RNN）<br>この選択肢が正解の理由は以下の通りです。<br>まず、リカレントニューラルネットワーク（RNN）は、時間的なシーケンスやパターンを認識できることを特徴としたディープラーニングの一種であり、時系列データに特に適しています。今回の問題では"地域、場所、過去の需要、季節的な人気"などの特徴を持ちつつ、"毎日の新しい在庫データから学習させたい"ことが述べられているように、これらは時間的なシーケンスやパターンを持っており、これを学習するためにはRNNが一番適しています。<br>また、在庫予測というタスク自体が過去のデータに基づいて未来の在庫を予測するという性質を持っており、その特性とRNNの持つ長期の依存関係を学ぶ能力が相まって、より精度の高い予測が可能になります。<br>このように、ユースケースとRNNの特性が一致しているため、この選択肢が正解です。<br>不正解の選択肢についての説明：<br>選択肢：分類<br>この選択肢が正しくない理由は以下の通りです。<br>分類アルゴリズムはデータを既存のカテゴリに分けるのに使用されますが、在庫予測には連続的な値（数値）を予測する必要があるので、分類アルゴリズムは適していません。<br>一方、RNNは時系列データのパターンを学習し予測するのに適しています。<br>選択肢：強化学習<br>この選択肢が正しくない理由は以下の通りです。<br>強化学習はエージェントが環境と相互作用し、各アクションに対する報酬を基にポリシーを改善していく学習方法です。しかしこのケースでは在庫予測という時系列の情報をもとに予測を行うことが重要であり、過去の情報をもとに次のステップの予測を行う、リカレントニューラルネットワーク（RNN）が適しています。<br>選択肢：畳み込みニューラルネットワーク（CNN）<br>この選択肢が正しくない理由は以下の通りです。<br>CNNは主に画像データの解析やパターン認識に使用され、時系列データの予測にはあまり適していません。<br>一方で、RNNは過去の情報を利用して未来を予測するタスクに適しているため、在庫予測のような時系列データの予測に向いています。'>
<div class='choice'> 強化学習</div>
<div class='choice'> 畳み込みニューラルネットワーク（CNN）</div>
<div class='choice'> リカレントニューラルネットワーク（RNN）</div>
<div class='choice'> 分類</div>
</div>

<div class='question' data-multiple='false' data-question='問題22<br>あなたのチームは、畳み込みニューラルネットワーク（CNN）ベースのアーキテクチャをゼロから構築しています。オンプレミスのCPUのみのインフラで実行した予備実験は有望でしたが、収束が遅いという問題がありました。市場投入までの時間を短縮するため、モデル学習をスピードアップするよう求められています。より強力なハードウェアを活用するため、Google Cloud上の仮想マシン（VM）で実験したいと考えています。あなたのコードには手動でのデバイス配置は含まれておらず、Estimatorのモデルレベルの抽象化も行われていません。<br>どの環境でモデルをトレーニングすべきですか？' data-answer='3' data-explanation='解説<br>正解は「すべてのライブラリーがプリインストールされた、n1-standard-2マシンと1GPUのディープラーニングVMを使用します」です。<br>この問題では、畳み込みニューラルネットワークベースのアーキテクチャのモデル学習のための適切な環境を探しています。オンプレミスのインフラでの学習は時間がかかるため、Google Cloud上の仮想マシンを想定しています。コードには手動でのデバイス配置は含まれておらず、Estimatorのモデルレベルの抽象化も行われていません。従って、GPUが含まれ、必要なライブラリがすべてプレインストールされている環境を選択することが求められます。不適切な環境や手動での依存関係のインストールが必要な環境は避けるべきです。<br>基本的な概念や原則：<br>ディープラーニングトレーニング：大量のデータから学習を行いモデルを作成します。GPUを使用することで高速化が可能です。<br>畳み込みニューラルネットワーク（CNN）：深層学習の一種で、特に画像認識に強いニューラルネットワークの一種です。<br>Google Cloudの仮想マシン：Google CloudのCompute Engineサービスによる仮想マシンです。様々なCPUとGPUの組み合わせが提供されます。<br>n1-standard-2マシン：Google Cloudの一般的な用途向けの仮想マシンです。中程度のパフォーマンスが特徴です。<br>GPU：グラフィックスプロセッシングユニット。並列処理が可能なため、ディープラーニングの学習に有利です。<br>ライブラリのプリインストール：必要なライブラリが事前にインストールされている状態。ユーザーが手動でライブラリをインストールする手間を省くことができます。<br>Estimatorのモデルレベルの抽象化：EstimatorはTensorFlowの高レベルAPIで、モデルのトレーニング、評価、予測等を行うことができます。これによりモデル構築を抽象化し、コードの簡略化と再利用性の向上が可能です。<br>正解についての説明：<br>（選択肢）<br>・すべてのライブラリーがプリインストールされた、n1-standard-2マシンと1GPUのディープラーニングVMを使用します<br>この選択肢が正解の理由は以下の通りです。<br>まず、ニューラルネットワークのトレーニングは大量の計算リソースを必要とするため、オンプレミスのCPUだけではなく、強力なGPUを利用することで演算の速度を大幅に向上させることができます。Google CloudのDeep Learning VMは、機械学習のフレームワークをすべてプリインストールした環境を提供するため、手間なく環境をセットアップすることができます。<br>また、選択肢の&#39;n1-standard-2&#39;マシンは一般的な用途に適した構成であり、2つのvCPUと7.5GBのメモリを備えており、畳み込みニューラルネットワークのトレーニングを十分にサポートできます。GPUと組み合わせることで、より高速なモデルの学習が可能になります。<br>さらに、手動でのデバイス配置やEstimatorのモデルレベルの抽象化が行われていないため、プリインストールされたライブラリを活用して簡単にモデルをトレーニングできます。<br>これらの理由から、n1-standard-2マシンと1GPUのディープラーニングVMを使用することが最適であり、モデル学習のスピードアップと市場投入までの時間短縮を実現できます。<br>不正解の選択肢についての説明：<br>選択肢：すべての依存関係を手動でインストールし、AVM on Compute Engineと1 TPUを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>全ての依存関係を手動でインストールするのは時間と労力が掛かる上、過去にコードはCPU環境でしか実行されていないため、直ちにTPU環境で動作するとは限らず、本筋から外れてしまう可能性があります。それに対してディープラーニングVMは、必要なライブラリがプリインストールされており、しかもGPUに対応しているため、既存のコードを修正することなく、モデル学習を素早く行うことができます。<br>選択肢：すべての依存関係が手動でインストールし、Compute Engineおよび8 GPU上のAVMを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>手動での依存関係のインストールは時間がかかり、開発速度を遅延させます。<br>また、8つのGPUはトレーニングの高速化に寄与しますが、コードが手動でデバイス配置が含まれていない場合、利用できない可能性があります。そのため、予めライブラリーがインストールされているディープラーニングVMの方が効率的です。<br>選択肢：すべてのライブラリがプリインストールされた、より強力なCPU e2-highcpu-16マシンによるディープラーニングVMを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>課題は収束の遅さであり、これはCPUのパワーよりもむしろGPUの存在が解決につながるためです。GPUは並列処理が得意で、ニューラルネットワークの訓練に適しています。<br>したがって、強力なCPUではなく、GPUを搭載したVMを使用することが適切です。'>
<div class='choice'> すべてのライブラリがプリインストールされた、より強力なCPU e2-highcpu-16マシンによるディープラーニングVMを使用します</div>
<div class='choice'> すべての依存関係を手動でインストールし、AVM on Compute Engineと1 TPUを使用します</div>
<div class='choice'> すべての依存関係が手動でインストールし、Compute Engineおよび8 GPU上のAVMを使用します</div>
<div class='choice'> すべてのライブラリーがプリインストールされた、n1-standard-2マシンと1GPUのディープラーニングVMを使用します</div>
</div>


            <!-- 他の問題も同様に追加 -->
        </div>

        <h2 id="question"></h2>
        <ul class="choices" id="choices"></ul>
        <button onclick="checkAnswer()">採点</button>
        <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
        <div class="result" id="result"></div>
    </div>

    <script>
        let currentQuestionIndex = 0;
        let correctCount = 0;
        const questions = [];

        document.addEventListener('DOMContentLoaded', () => {
            const questionElements = document.querySelectorAll('#quiz-data .question');
            questions.push(...Array.from(questionElements).map(questionElement => ({
                question: questionElement.getAttribute('data-question').replace(/\\n/g, '<br>'),
                choices: Array.from(questionElement.querySelectorAll('.choice')).map((choice, index) => ({
                    text: choice.innerHTML.replace(/\\n/g, '<br>'),  // innerHTMLに変更
                    index: index
                })),
                correctAnswer: questionElement.getAttribute('data-answer').split(',').map(Number),
                explanation: questionElement.getAttribute('data-explanation').replace(/\\n/g, '<br>'),
                multiple: questionElement.getAttribute('data-multiple') === 'true'
            })));
            showQuestion();
        });

        function shuffleArray(array) {
            for (let i = array.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [array[i], array[j]] = [array[j], array[i]];
            }
        }

        function showQuestion() {
            const questionElement = document.getElementById('question');
            const choicesContainer = document.getElementById('choices');
            const currentQuestion = questions[currentQuestionIndex];

            shuffleArray(currentQuestion.choices);

            questionElement.innerHTML = currentQuestion.question;
            choicesContainer.innerHTML = '';

            currentQuestion.choices.forEach((choice, i) => {
                const li = document.createElement('li');
                const input = document.createElement('input');
                const label = document.createElement('label');

                input.type = currentQuestion.multiple ? 'checkbox' : 'radio';
                input.name = 'choice';
                input.value = choice.index;
                input.id = 'choice' + i;

                label.htmlFor = 'choice' + i;
                label.innerHTML = choice.text;  // textContentをinnerHTMLに変更

                li.appendChild(input);
                li.appendChild(label);
                choicesContainer.appendChild(li);
            });

            document.getElementById('result').textContent = "";
            document.getElementById('nextButton').style.display = 'none';
        }

        function checkAnswer() {
            const currentQuestion = questions[currentQuestionIndex];
            const selectedChoices = Array.from(document.querySelectorAll('input[name="choice"]:checked'))
                                        .map(checkbox => parseInt(checkbox.value))
                                        .sort();
            const resultElement = document.getElementById('result');
            
            if (selectedChoices.length > 0) {
                const isCorrect = currentQuestion.multiple
                    ? selectedChoices.toString() === currentQuestion.correctAnswer.sort().toString()
                    : selectedChoices.length === 1 && selectedChoices[0] === currentQuestion.correctAnswer[0];
                
                if (isCorrect) {
                    resultElement.innerHTML = "正解です！<br>" + currentQuestion.explanation;
                    resultElement.style.color = "green";
                    correctCount++; // 正解数をカウント
                } else {
                    resultElement.innerHTML = "残念、不正解です。<br>" + currentQuestion.explanation;
                    resultElement.style.color = "red";
                }
                document.getElementById('nextButton').style.display = 'inline';
            } else {
                resultElement.textContent = "回答を選択してください。";
                resultElement.style.color = "orange";
            }
        }

        function nextQuestion() {
            currentQuestionIndex++;
            
            if (currentQuestionIndex < questions.length) {
                showQuestion();
            } else {
                showFinalResult();
            }
        }

        function showFinalResult() {
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2>問題終了！</h2>
                <p>あなたの正解数は ${correctCount} / ${questions.length} です。</p>
                <button onclick="restartQuiz()">再挑戦する</button>
            `;
        }

        function restartQuiz() {
            correctCount = 0;
            currentQuestionIndex = 0;

            // クイズのUI全体を初期化
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2 id="question"></h2>
                <ul class="choices" id="choices"></ul>
                <button onclick="checkAnswer()">採点</button>
                <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
                <div class="result" id="result"></div>
            `;

            // 初期化後に最初の問題を表示
            showQuestion();
        }        
    </script>
</body>
</html>