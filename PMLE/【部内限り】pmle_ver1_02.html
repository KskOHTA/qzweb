<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google Professional Machine Learning Engneer問題集 01</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="quiz-container">
        <div id="quiz-data" style="display: none;">


<div class='question' data-multiple='false' data-question='問題16<br>あなたは，訓練済みのEfficientNetモデルに基づいて画像分類器を訓練するために転移学習を使っています。学習データセットには20,000枚の画像があります。モデルの再学習は1日1回行う予定です。インフラストラクチャのコストを最小限に抑える必要があります。<br>どのようなプラットフォームコンポーネントと構成環境を使用すべきですか？' data-answer='1' data-explanation='解説<br>正解は「4つのV100 GPUとCloud Storageを備えたカスタムスケールティアを使用したAI Platformトレーニングジョブ」です。<br>この問題では、転移学習に基づく画像分類器の訓練に適するプラットフォームコンポーネントと構成環境を選ぶことが求められます。その際に重視すべきポイントは、インフラストラクチャのコストを最小限に抑えることです。したがって、選択肢を検討する際は、コスト効率の良い解決策を選ぶことが求められます。また、問題文が特に求めている具体的なハードウェア要件（例えばV100 GPUなど）や一日一回の再学習といった操作の頻度も考慮に入れて解答を選ぶべきです。<br>基本的な概念や原則：<br>EfficientNet：エフィシェントネットは、画像分類タスクを効率的に行うための深層学習モデルです。モデルの規模と精度の間の最適なバランスを提供します。<br>転移学習：既存の学習済みモデルを新たなタスクに適用する手法です。学習に必要な時間とリソースを大幅に削減します。<br>AI Platform Training：分散トレーニング機能を備えた、Google Cloudのフルマネージド機械学習モデルトレーニングサービスです。大量のデータに対するトレーニングとスケーリングが可能です。<br>V100 GPU：高性能なグラフィックスプロセッシングユニット（GPU）です。複雑な機械学習タスクのトレーニングや推論を高速に実行します。<br>Cloud Storage：Google Cloudのオブジェクトストレージサービスです。大容量のデータを安全に保存し、グローバルにアクセスすることができます。<br>ディープラーニングVM：GPUやTPUを活用したディープラーニング用途に最適化された仮想マシンです。しかし、フルマネージドサービスではないため、メンテナンスや管理の負担が伴います。<br>Google Kubernetes Engine：Google CloudのフルマネージドKubernetesサービスです。複数のGPUを含むクラスターを構成できますが、学習タスクの専用プラットフォームと比較してオーバーヘッドが大きいです。<br>正解についての説明：<br>（選択肢）<br>・4つのV100 GPUとCloud Storageを備えたカスタムスケールティアを使用したAI Platformトレーニングジョブ<br>この選択肢が正解の理由は以下の通りです。<br>まず、AI Platformのコンポーネントは訓練中に必要なリソースを管理する面倒を省きつつ、スケールとパフォーマンスの柔軟性を提供します。<br>さらに、トレーニングジョブが訓練の途中で停止した場合でも再開することが可能であり、利用者はトレーニングジョブの進行状況について追跡できます。<br>次に、転送学習に使用されるEfficientNetモデルは非常に計算集約的です。そのため、高性能の計算リソースが必要で、ここでは4つのV100 GPUが提案されています。V100 GPUは高レベルの計算能力を持つNVIDIAのGPUであり、深層学習タスクに非常に適しています。<br>最後に、データはCloud Storageに保存されます。Cloud Storageはスケーラビリティとデータ耐久性を提供し、20,000枚の大規模な画像データセットを保存するのに適しています。<br>したがって、このコンポーネントと構成環境の選択はインフラコストを最小限に抑えながら、適切な画像分類器の訓練を行うためのものです。<br>不正解の選択肢についての説明：<br>選択肢：4つのV100 GPUとローカルストレージを備えたディープラーニングVM<br>この選択肢が正しくない理由は以下の通りです。<br>ディープラーニングVMはGPUインスタンスが常時アクティブとなるため、1日1回の再学習だけに使うとコストが高くなります。一方のAI Platformトレーニングジョブはジョブの実行時のみ課金され、学習が終われば自動的にリソースが解放されるためコスト効率が高いです。<br>選択肢：4つのV100 GPUとCloud Storageを備えたディープラーニングVM<br>この選択肢が正しくない理由は以下の通りです。<br>ディープラーニングVMは永続的なリソースで、使用していない期間でも料金が発生します。再学習が1日1回のみであるため、リソースの稼働時間が短く、コスト効率は悪いです。<br>一方、AI Platformトレーニングジョブは使用した時間だけ課金されるため、コストを最小限に抑えられます。<br>選択肢：V100 GPUノードプールとNFSサーバーを備えたGoogle Kubernetes Engineクラスター<br>この選択肢が正しくない理由は以下の通りです。<br>Google Kubernetes Engineクラスターの維持はコストがかかり、またNFSサーバーはIOがボトルネックになる可能性があります。一方のAI Platformでは、トレーニングジョブの実行が必要なときのみリソースが確保され、Cloud Storageは大容量のデータも高速に取り扱うことができます。'>
<div class='choice'> V100 GPUノードプールとNFSサーバーを備えたGoogle Kubernetes Engineクラスター</div>
<div class='choice'> 4つのV100 GPUとCloud Storageを備えたカスタムスケールティアを使用したAI Platformトレーニングジョブ</div>
<div class='choice'> 4つのV100 GPUとCloud Storageを備えたディープラーニングVM</div>
<div class='choice'> 4つのV100 GPUとローカルストレージを備えたディープラーニングVM</div>
</div>

<div class='question' data-multiple='false' data-question='問題17<br>Cloud TPU v2を使用して物体検出モデルをトレーニングしています。トレーニング時間が予想よりも長くかかっています。<br>Cloud TPUプロファイルで得られたこの単純化されたトレースに基づいて、コスト効率の良い方法でトレーニング時間を短縮するために、どのようなアクションを取るべきですか？' data-answer='3' data-explanation='解説<br>正解は「並列読み込み、並列処理、プリフェッチを使って入力関数を書き換えます」です。<br>この問題では、Cloud TPUを用いた物体検出モデルのトレーニング時間を効率よく短縮する方法を選ぶことが求められています。ここでプロファイルの結果を用いて適切なアプローチを考えるためには、Cloud TPUの動作原理とプロファイルが示す情報を理解していることが必要です。特に並列読み込みやプリフェッチなど、データ処理とローディングの最適化について理解していることが重要です。<br>基本的な概念や原則：<br>Cloud TPU：TensorFlowの専用ハードウェアで、高速な機械学習トレーニングを支援します。<br>並列読み込み、並列処理、プリフェッチ：データの読み込みと処理の効率を上げるためのテクニックです。これらを適切に使用することで、トレーニング時間を短縮することができます。<br>Cloud TPUプロファイル：Cloud TPUのパフォーマンス分析ツールです。モデルのパフォーマンスボトルネックを識別し、最適化の方向性を見つけるのに役立ちます。<br>バッチサイズ：一度にモデルに供給するトレーニングサンプルの数です。バッチサイズを増やすと、ハードウェアリソースの使用効率が向上する可能性がありますが、必ずしもトレーニング時間の短縮にはつながりません。<br>正解についての説明：<br>（選択肢）<br>・並列読み込み、並列処理、プリフェッチを使って入力関数を書き換えます<br>この選択肢が正解の理由は以下の通りです。<br>トレーニングのパフォーマンスはデータ読み込みプロセスに大きく依存しています。入力データの読み込みと処理が効率的でないと、モデルのトレーニングが遅くなります。並列読み込み、並列処理、プリフェッチはデータの読み込みパフォーマンスを最適化するための効果的な手法です。<br>並列読み込みは、複数のデータソースから同時にデータを読み込むことで、読み込みプロセスを高速化します。<br>また、並列処理は、複数のプロセスやスレッドでデータセットの処理を行うことで、処理を高速化します。<br>そして、プリフェッチは、データの読み込みをバックグラウンドで実行し、必要なデータが使用可能になる前にそれをキャッシュします。これにより、データの読み込みがトレーニングのボトルネックになるのを防ぎ、トレーニングの全体的なパフォーマンスを改善します。以上の理由から、並列読み込み、並列処理、プリフェッチを使って入力関数を書き換えることで、トレーニング時間を効率的に短縮することが可能になります。<br>不正解の選択肢についての説明：<br>選択肢：Cloud TPU v2からCloud TPU v3に移行し、バッチサイズを増やします<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud TPUのバージョンを上げてバッチサイズを増やす方法も一見効果的に思えますが、それは計算能力を増加させるものであり、データ入力のボトルネックを解消するものではありません。<br>一方、並列読み込み、並列処理、プリフェッチを用いて入力関数を書き換えることで、データ入力の処理能力が向上します。<br>選択肢：Cloud TPU v2から8つのNVIDIA V100 GPUに移行し、バッチサイズを増やします<br>この選択肢が正しくない理由は以下の通りです。<br>NVIDIA V100 GPUに移行してバッチサイズを増やすことは、ハードウェアの変更となりコスト効率が下がります。<br>それに対して、並列読み込み、並列処理、プリフェッチを使って入力関数を書き換えることで、既存の設備を最大限に活用しトレーニング時間を短縮できます。<br>選択肢：入力関数を書き換えて、入力画像のサイズと形状を変更します<br>この選択肢が正しくない理由は以下の通りです。<br>入力画像のサイズや形状を変更することはトレーニング時間に影響を及ぼす可能性がありますが、それは主にコンピューティングリソースの使用を最適化することに依存しません。<br>一方、並列読み込み、並列処理、プリフェッチによる入力関数の書き換えはデータの読み取りと処理を効率化し、TPUの使用率を向上させることでトレーニング時間を短縮します。'>
<div class='choice'> 入力関数を書き換えて、入力画像のサイズと形状を変更します</div>
<div class='choice'> Cloud TPU v2からCloud TPU v3に移行し、バッチサイズを増やします</div>
<div class='choice'> Cloud TPU v2から8つのNVIDIA V100 GPUに移行し、バッチサイズを増やします</div>
<div class='choice'> 並列読み込み、並列処理、プリフェッチを使って入力関数を書き換えます</div>
</div>

<div class='question' data-multiple='false' data-question='問題18<br>あなたは、複数のCSVファイルに格納された1,000億レコードの構造化データセットでTensorFlowモデルをトレーニングしています。入出力の実行パフォーマンスを改善する必要があります。<br>この要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「CSVファイルをTFRecordsのシャードに変換し、データをCloud Storageに保存します」です。<br>この問題では、大量のCSVデータからTensorFlowモデルのトレーニングを行う際の入出力パフォーマンス改善について問われています。ここでは、CSVデータをどのように取り扱い、どのプラットフォームやサービスを用いるかが重要です。選択肢にあるBigQuery、Cloud Bigtable、Cloud Storage、Hadoop分散ファイルシステムについて、データ操作のパフォーマンス観点から適切性を評価してみてください。また、TFRecordsというTensorFlow独自のデータフォーマットに言及する選択肢がある点も注目です。<br>基本的な概念や原則：<br>TFRecords：TensorFlowのバイナリフォーマットで、構造化データを効率的にストレージに保存し、機械学習トレーニングに使用するためのフォーマットです。複数のTFRecordsファイルをシャードとして保存することで、読み込みパフォーマンスが向上します。<br>Cloud Storage：Google Cloudの耐久性と高可用性を備えたオブジェクトストレージサービスです。マルチリージョナル、リージョナル、Nearline、Coldlineなど、様々なストレージクラスを提供しています。<br>BigQuery：Google Cloudのフルマネージドなビッグデータ分析サービスです。SQLクエリによるデータ探索と分析が可能です。<br>Cloud Bigtable：Google CloudのNoSQLビッグデータデータベースサービスです。低レイテンシの読み書きとデータのスケーリングに優れています。<br>Hadoop Distributed File System（HDFS）：大規模データセットを分散ストレージするためのファイルシステムです。しかし、TensorFlow用のデータ読み込みを最適化するためにTFRecordsとCloud Storageの組み合わせが推奨されます。<br>正解についての説明：<br>（選択肢）<br>・CSVファイルをTFRecordsのシャードに変換し、データをCloud Storageに保存します<br>この選択肢が正解の理由は以下の通りです。<br>まず、TFRecordsはTensorFlowによる大量のデータを高速に処理するためのバイナリフォーマットで、学習パフォーマンスを向上させるために設計されています。大規模なデータを学習する際、たくさんの小さいファイルを扱うよりも一つまたは数つの大きなファイルを扱った方が入出力パフォーマンスが改善されるため、CSVファイルをTFRecordsに変換することが有効です。<br>また、シャーディングは大規模なデータをより小さな部分（シャード）に分割することで、データをより効率的に管理し、並列処理が可能になるためパフォーマンスが向上します。<br>さらに、Cloud Storageは大規模データを安全に保管し、効率的にアクセスすることができます。TFRecordsに変換したシャードをCloud Storageに保存することで、学習データの読み込み速度が向上し、全体のパフォーマンスが改善します。<br>不正解の選択肢についての説明：<br>選択肢：データをBigQueryに読み込み、BigQueryからデータを読み込みます<br>この選択肢が正しくない理由は以下の通りです。<br>BigQueryでデータを管理することは、分析クエリの最適化には適していますが、TensorFlowモデルのトレーニングを効率化することはありません。<br>一方、TFRecordsは機械学習のために設計されたフォーマットで、大量のデータを効率的に読み込んでTensorFlowでのトレーニングパフォーマンスを改善します。<br>選択肢：Cloud Bigtableにデータをロードし、Bigtableからデータを読み込みます<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud Bigtableは大規模なイベントデータや時系列データのリアルタイム分析等に適したスカラブルなデータベースサービスであり、このケースのような大量の構造化データセットに対する機械学習モデルのトレーニングには適していません。<br>一方、TFRecordsはTensorFlowに最適化されたフォーマットであり、大量のデータセットの読み書き性能を向上させることができます。<br>選択肢：CSVファイルをTFRecordsのシャードに変換し、Hadoop分散ファイルシステム（HDFS）にデータを格納します<br>この選択肢が正しくない理由は以下の通りです。<br>Hadoop分散ファイルシステム（HDFS）は、Google Cloud環境でのモデルトレーニングにおける最適なデータストレージソリューションではありません。<br>それに対して、Cloud Storageは大規模なデータセットを扱うための高スケーラビリティと高パフォーマンスを提供し、TFRecordsファイルと併用することでTensorFlowモデルのトレーニング効率を向上させることができます。'>
<div class='choice'> CSVファイルをTFRecordsのシャードに変換し、Hadoop分散ファイルシステム（HDFS）にデータを格納します</div>
<div class='choice'> CSVファイルをTFRecordsのシャードに変換し、データをCloud Storageに保存します</div>
<div class='choice'> Cloud Bigtableにデータをロードし、Bigtableからデータを読み込みます</div>
<div class='choice'> データをBigQueryに読み込み、BigQueryからデータを読み込みます</div>
</div>

<div class='question' data-multiple='false' data-question='問題19<br>あなたは世界中に何百万人もの顧客を持つゲーム会社に勤めています。すべてのゲームには、プレイヤー同士がリアルタイムでコミュニケーションできるチャット機能があります。メッセージは20以上の言語で入力でき、Cloud Translation APIを使ってリアルタイムで翻訳されます。あなたは、さまざまな言語間でパフォーマンスが均一であることを保証しながら、サービスインフラを変更することなく、リアルタイムでチャットをモデレートするMLシステムを構築するよう依頼されました。<br>あなたは、Cloud Translation APIによって翻訳されたチャットメッセージを埋め込むために、社内のword2vecモデルを使用して最初のモデルをトレーニングしました。しかし、このモデルは異なる言語間でパフォーマンスに大きな違いがあります。<br>どのように改善すべきですか？' data-answer='3' data-explanation='解説<br>正解は「元の言語のチャットメッセージを使用して分類器を訓練します」です。<br>この問題では、ゲーム会社のリアルタイムチャットのモデレーションシステムを改善する手段を探しています。問題では、Cloud Translation APIと社内のword2vecモデルを使用してシステムを構築し、異なる言語間でパフォーマンスに差が出てしまっていることが述べられています。試験の解答者はこの状況を考慮に入れ、機械学習モデルの改善法を提案する必要があります。その際、全ての言語に対して均一なパフォーマンスが求められている点に留意することが重要です。翻訳ではなく元の言語で訓練を行うことや、正規化項の導入、または異なるモデルへの置換などのアプローチが考えられます。各選択肢の中から最も効果的な改善策を選びます。<br>基本的な概念や原則：<br>Cloud Translation API：Google Cloudのサービスで、テキストを複数の言語に自動的に翻訳します。リアルタイムまたはバッチ処理の両方の翻訳が可能です。<br>MLシステム（機械学習システム）：データを利用して自動的に学習し、予測や分類を行うシステムです。ユーザーのチャットなどのテキストデータを分析できます。<br>モデレーション：適切でない投稿や迷惑行為を制御する過程です。MLシステムを用いて自動的にモデレーションを行います。<br>word2vec：テキストデータをベクトル空間へマッピングする機械学習アルゴリズムです。似たような単語は似たようなベクトルになるようにトレーニングされます。<br>分類器のトレーニング：MLモデルにデータを供給し、特定のタスク（この場合、チャットメッセージのモデレーション）を達成するために学習するプロセスです。<br>正則化項：モデルが過学習（トレーニングデータに過度に適合）するのを防ぐための手法です。Min-Diffアルゴリズムでは、正則化項を利用して一部のデータポイントの損失を増加させ、体系的なバイアスを低減します。<br>GPT-3、T5：高度な自然言語処理モデルで、テキスト生成やテキスト理解のタスクで広く使用されます。しかし、これらのモデルを導入するだけでパフォーマンスの問題が解決するわけではありません。<br>正解についての説明：<br>（選択肢）<br>・元の言語のチャットメッセージを使用して分類器を訓練します<br>この選択肢が正解の理由は以下の通りです。<br>元の言語のチャットメッセージを使用して分類器を訓練すれば、翻訳の品質やその他の要素から生じるパフォーマンスのばらつきを避けることができます。Cloud Translation APIを通過したテキストの翻訳の精度が同じであるとは限らないため、それを入力として使用すると、モデルのパフォーマンスに格差が生じる原因です。対象とする全ての言語において初めからモデルを訓練すれば、そのような影響を最小限に抑えることができます。<br>また、各言語に対して特化したモデリングが可能になるため、更なる精度向上を期待できます。このような理由から、元の言語のチャットメッセージを使用して分類器を訓練することが含まれているこの答えが適切です。<br>不正解の選択肢についての説明：<br>選択肢：Min-Diffアルゴリズムのような正則化項を損失関数に加えます<br>この選択肢が正しくない理由は以下の通りです。<br>Min-Diffアルゴリズムのような正則化項を損失関数に加えると、特定のデータセットに対する過学習を防ぐロールがありますが、ここでの問題点は異なる言語間でのパフォーマンスの違いなので、正則化項を加える戦略は解決策となりません。この問題を解決するためには、各言語の特性に合わせて学習を行うべきであり、元の言語のチャットメッセージを使用して分類器を訓練することが適切な策です。<br>選択肢：社内のword2vecをGPT-3またはT5に置き換えます<br>この選択肢が正しくない理由は以下の通りです。<br>単にword2vecモデルをGPT-3やT5に置き換えるだけでは、言語間でのパフォーマンス差を解決する保証はありません。これらのモデルも全ての言語に対して均一なパフォーマンスを得るわけではないため、正解の選択肢が提案するように元の言語のメッセージを使用して学習する方が良い結果が期待できます。<br>選択肢：誤検出率が高すぎる言語のモデレーションを削除します<br>この選択肢が正しくない理由は以下の通りです。<br>誤検出率が高すぎる言語のモデレーションを削除すると、その言語を使用するプレイヤーのチャットコミュニケーションは未管理になり、問題行動が起きる可能性があります。問題は言語間のパフォーマンス差を解消することであり、これを達成するための効果的な方法は元の言語のチャットメッセージを用いて分類器を訓練することです。'>
<div class='choice'> 誤検出率が高すぎる言語のモデレーションを削除します</div>
<div class='choice'> Min-Diffアルゴリズムのような正則化項を損失関数に加えます</div>
<div class='choice'> 社内のword2vecをGPT-3またはT5に置き換えます</div>
<div class='choice'> 元の言語のチャットメッセージを使用して分類器を訓練します</div>
</div>

<div class='question' data-multiple='false' data-question='問題20<br>あなたは、X線画像が骨折のリスクを示しているかどうかを分類することを目的としたMLモデルを開発しています。TPUをアクセラレーターとして使用し、Vertex AI上でResNetアーキテクチャをトレーニングしましたが、トレーニング時間とメモリー使用量に改善の余地があります。学習コードを素早く反復したいですが、コードの変更は最小限にしたいと考えています。また、モデルの精度への影響も最小限に抑えたいと考えています。<br>この要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「float32の代わりにbfloat16を使うようにモデルを設定します」です。<br>この問題では、MLモデルのトレーニングに関する最適化のための手段を求められています。ここで重要な要件は、パフォーマンス向上が求められている一方で、プログラムの変更は最小限にとどめ、モデルの精度への影響も最小限にしたいという点です。"float32の代わりにbfloat16を使うようにモデルを設定します"などの選択肢がこれらの要件を満たすかどうか、という視点で答えを選ぶと良いでしょう。<br>基本的な概念や原則：<br>bfloat16：半精度浮動小数点数の形式で、16ビットの浮動小数点数を表します。計算効率とメモリ使用量の改善を達成しながら、float32とほぼ同様のモデル精度を維持できます。<br>TPU：Tensor Processing Unitの略で、機械学習用にGoogleが開発した専用ハードウェアアクセラレータです。高度な計算能力とメモリ容量を提供し、MLモデルのトレーニングと推論を高速化します。<br>Vertex AI：Google Cloud上で機械学習モデルの作成、トレーニング、デプロイを一元管理するためのフルマネージドツールです。JupyterノートブックやTensorBoardといった周辺ツールとの統合もサポートしています。<br>ResNet：残差ネットワークの略で、深層学習の分野における一般的なモデル構造です。特徴層を特別な接続方法でリンクし、ディープラーニングの学習を改善します。<br>MLモデルの精度：モデルがどの程度の予測性能を持つかを評価する指標で、適切な精度が求められる状況で改善が必要とされます。<br>正解についての説明：<br>（選択肢）<br>・float32の代わりにbfloat16を使うようにモデルを設定します<br>この選択肢が正解の理由は以下の通りです。<br>まず、float32とbfloat16の違いから始めます。両者はともに浮動小数点数を表現する形式ですが、bfloat16はメモリ要件が半分の16ビットで、そのためGPUやTPUでの計算が高速化されます。特に、TensorflowにはTPUでbfloat16を最大限に活用するためのサポートが用意されています。<br>また、bfloat16に変更することでメモリ使用量が大幅に削減され、それによりトレーニング時間も短縮される可能性があります。<br>さらに、モデルの精度への影響は極めて小さいと考えられます。これはbfloat16が、float32と比較して動的範囲は同じでありながら精度は低いためです。これにより、ResNetのような深いネットワークで精度を維持しながらも計算効率を改善できます。<br>したがって、最小のコード変更でトレーニング時間とメモリ使用量を改善したいのであれば、モデルの設定をbfloat16にするのが最適と言えます。<br>不正解の選択肢についての説明：<br>選択肢：モデルアーキテクチャのレイヤー数を減らします<br>この選択肢が正しくない理由は以下の通りです。<br>モデルアーキテクチャのレイヤー数を減らすと、モデルの表現力または学習能力が大幅に低下し、精度が落ちる恐れがあります。<br>一方、float32をbfloat16に置き換えると、メモリ使用量と計算時間が減少し、影響は最小限になります。<br>選択肢：グローバルバッチサイズを1024から256に減らします<br>この選択肢が正しくない理由は以下の通りです。<br>グローバルバッチサイズを1024から256に減らすと、モデルの精度が下がる可能性があり、また、トレーニング時間も増える可能性があります。<br>一方、データタイプをbfloat16に変更すると、TPUの計算速度とメモリ効率が向上し、精度への影響も最小限に抑えられます。<br>選択肢：モデルで使用する画像の寸法を小さくします<br>この選択肢が正しくない理由は以下の通りです。<br>モデルで使用する画像の寸法を小さくすると、モデルが学習できる情報量が減少し、結果的にモデルの精度に大きな影響を与えます。<br>一方、float32の代わりにbfloat16を使う変更は、トレーニング時間とメモリ使用量を効果的に削減しつつ、精度への影響を最小限に抑えます。'>
<div class='choice'> モデルアーキテクチャのレイヤー数を減らします</div>
<div class='choice'> float32の代わりにbfloat16を使うようにモデルを設定します</div>
<div class='choice'> グローバルバッチサイズを1024から256に減らします</div>
<div class='choice'> モデルで使用する画像の寸法を小さくします</div>
</div>

<div class='question' data-multiple='false' data-question='問題21<br>あなたは、異なるソースからの画像を低レイテンシで処理するML学習モデルの入力パイプラインの開発を依頼されました。あなたは、入力データがメモリに収まらないことに気づきました。<br>Googleが推奨するベストプラクティスに従って、どのようにデータセットを作成すべきですか？' data-answer='1' data-explanation='解説<br>正解は「画像をTFRecordsに変換してCloud Storageに保存し、tf.data APIを使ってトレーニング用の画像を読み込みます」です。<br>この問題では、メモリに収まらない大量の画像データをMLモデルの入力パイプラインとして効率的に処理するための方法を問います。Googleのベストプラクティスに基づくソリューションを適用することが重要です。そのために、異なるデータ保存と読み込み方法を比較検討し、低レイテンシで処理できるのかを考えてみましょう。また、tf.data APIをどのように使うか、特に大量のデータを効率的に読み取る方法が求められています。<br>基本的な概念や原則：<br>TFRecords：TensorFlowのバイナリ形式で、トレーニングデータをシリアル化して保存します。大量のデータを効率的に読み込むのに適しています。<br>Cloud Storage：Google Cloudのオブジェクトストレージサービスです。大量のデータを信頼性高く、安全に保存し、利用することができます。<br>tf.data API：TensorFlowのデータ入力パイプラインを構築する強力なツールです。大量のデータセットを効率的に読み込むための関数とヘルパーを提供します。<br>メモリの制限：大量のデータを扱う際には、全てのデータを同時にメモリにロードできない場合があります。その場合、一部のデータを読み込むだけでなく、需要に応じてデータを効率的にストリーミングする方法が必要です。<br>tf.data.Dataset.prefetch：TensorFlowの機能で、データの送信とその処理をオーバーラップさせることができます。これにより、GPUが次のデータバッチを待つ時間なく処理を進めることができます。<br>tf.data.Dataset.from_tensors：TensorFlowの機能で、すでにメモリにロードされたテンソルからデータセットを作成します。大量のデータを扱う際にはメモリの制限が生じる可能性があります。<br>正解についての説明：<br>（選択肢）<br>・画像をTFRecordsに変換してCloud Storageに保存し、tf.data APIを使ってトレーニング用の画像を読み込みます<br>この選択肢が正解の理由は以下の通りです。<br>まず、TFRecords形式はTensorFlowが推奨するフォーマットであり、大規模なデータの効率的な読み取りを可能にします。故に、メモリに収まらない大量の画像データを扱う際には、TFRecords形式を活用するのが適切です。<br>さらに、TFRecordsはCloud Storageと組み合わせて利用することが可能で、Cloud Storageの大規模なデータ保存能力と低レイテンシの特性を活かせます。<br>また、tf.data APIはTensorFlowで提供されているAPIで、テンソルの入力パイプラインを構築・最適化するための強力なツールです。TFRecords形式のデータをtf.data APIを通じて読み込むことで、データのシャッフルやバッチ作成、リピートなどの操作を効率よく実行することができます。これにより、メモリの制約に影響されることなく、大量の画像データを扱う機械学習モデルの入力パイプラインを効率的に実行することができます。<br>不正解の選択肢についての説明：<br>選択肢：tf.data.Dataset.prefetch変換を作成します<br>この選択肢が正しくない理由は以下の通りです。<br>tf.data.Dataset.prefetch変換は、データセットのパイプライン化に役立ち、データ処理とモデル訓練のオーバーラップを可能にします。<br>しかし、元の画像データがメモリに収まらない問題を解決するための手段ではありません。<br>それに対して、正解の選択肢はTFRecordsに画像を変換しCloud Storageに保存することで、大量のデータを実効的に管理し、リソースの利用率を最適化する解決策を提供します。<br>選択肢：画像をtf.Tensorオブジェクトに変換し、Dataset.from_tensor_slices()を実行します<br>この選択肢が正しくない理由は以下の通りです。<br>tf.Tensorオブジェクトに画像を変換し、Dataset.from_tensor_slices()を使用しても、記述の通り全てのデータがメモリに収まる訳ではないため、処理が不可になります。"画像をTFRecordsに変換してCloud Storageに保存し、tf.data APIを使う"方法は、大量のデータを効率的に扱うことができます。<br>選択肢：画像をtf.Tensorオブジェクトに変換し、tf.data.Dataset.from_tensors()を実行します<br>この選択肢が正しくない理由は以下の通りです。<br>tf.Tensorオブジェクトに全画像を変換し、tf.data.Dataset.from_tensors()を実行すると、全データを一度にメモリ上にロードしようとします。<br>しかし、問題文から入力データはメモリに収まらないことがわかっているため、この方法ではメモリオーバーフローが発生する可能性があります。対して正答のTFRecordsとtf.data APIのうまく組み合わせは、大規模データの効率的な読み込みと並列処理を実現します。'>
<div class='choice'> tf.data.Dataset.prefetch変換を作成します</div>
<div class='choice'> 画像をTFRecordsに変換してCloud Storageに保存し、tf.data APIを使ってトレーニング用の画像を読み込みます</div>
<div class='choice'> 画像をtf.Tensorオブジェクトに変換し、Dataset.from_tensor_slices()を実行します</div>
<div class='choice'> 画像をtf.Tensorオブジェクトに変換し、tf.data.Dataset.from_tensors()を実行します</div>
</div>

<div class='question' data-multiple='false' data-question='問題22<br>あなたは大手ホテルチェーンに勤務しており、マーケティングチームを支援して、ターゲットを絞ったマーケティング戦略のための予測を集めるよう依頼されています。マーケティングを適宜調整できるように、今後20日間のユーザー生涯価値（LTV）を予測する必要があります。顧客データセットはBigQueryにあり、AutoML Tablesを使用してトレーニング用の表形式データを準備しています。このデータには、複数の列にまたがるタイムシグナルがあります。<br>AutoMLがデータに最適なモデルを適合させるには、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「手動での変換を行わずに、トレーニング用にデータを送信します。タイムシグナルがある列を使用して、データを手動で分割します。検証セットのデータがトレーニングセットのデータの30日後のものであること、テストセットのデータが検証セットの30日後のものであることを確認します」です。<br>この問題では、BigQueryに格納されたデータを使用して、ユーザーの生涯価値を予測するための最適なモデルの適合方法をAutoML Tablesを使用して求めることが求められています。データにはタイムシグナルが含まれており、その特性を考慮に入れる必要があります。AutoMLが適切に予測モデルを訓練するためには、データの整形や分割方法が適切であることが重要であり、それぞれの選択肢がこれをどのように処理するかを理解することが必要です。また、トレーニングセット、検証セット、テストセットの時系列的な関係性も問題解決の一部です。<br>基本的な概念や原則：<br>BigQuery：Google Cloudのフルマネージドなデータウェアハウスサービスです。SQLを用いたデータ解析が可能で、高度にスケーラブルです。<br>AutoML Tables：機械学習モデル構築を自動化するGoogle Cloudのサービスです。表形式のデータを容易に扱うことができます。<br>ユーザー生涯価値（LTV）：顧客がビジネスにもたらす収益の見積もりで、マーケティング戦略の重要な指標です。<br>トレーニングセット、検証セット、テストセット：機械学習モデルのトレーニングと評価を行うために使用するデータの分割方法です。モデルの性能の汎化性を評価するために、データを分けて使用します。<br>タイムシグナル：時系列データの特徴の一つで、時間的なパターンまたはシーケンスを捉える情報です。予測モデルの精度を向上させるために使用されます。<br>データ変換：機械学習モデルのパフォーマンスを向上させるため、または特定の要件を満たすために行われるデータの前処理の手法です。手動で行うことも、自動的に行うツールも存在します。<br>正解についての説明：<br>（選択肢）<br>・手動での変換を行わずに、トレーニング用にデータを送信します。タイムシグナルがある列を使用して、データを手動で分割します。検証セットのデータがトレーニングセットのデータの30日後のものであること、テストセットのデータが検証セットの30日後のものであることを確認します<br>この選択肢が正解の理由は以下の通りです。<br>まず、AutoML TablesはGoogle Cloudの機械学習サービスで、表形式データの機械学習モデル作成を自動化します。AutoML Tablesは、データの前処理、特徴量エンジニアリング、モデルの適合度などの一部を自動化することができるため、手動での変換を行わずにデータを送信することができます。<br>次に、時系列データのある列を手動で分割すると、元の時系列のパターンが保持されます。モデルは過去のデータを使って未来を予測するため、検証セットはトレーニングセットの30日後、テストセットは検証セットの30日後とすることで、時系列の流れを正確に反映します。これにより、モデルの将来のパフォーマンスをより正確に評価でき、マーケティング戦略の調整に役立てられます。<br>不正解の選択肢についての説明：<br>選択肢：タイムシグナルを含むすべての列を手動で配列にまとめます。AutoMLにこの配列を適切に解釈させます。トレーニング、検証、テストセットにまたがる自動データ分割を選択します<br>この選択肢が正しくない理由は以下の通りです。<br>タイムシグナルを含むすべての列を手動で配列にまとめると、データの時間依存性が失われ、正確なモデルの学習が難しくなります。<br>また、自動データ分割を選択すると時間軸にそったアプローチが不可能になり、正確な予測が期待できません。正解では時間依存性を保つことで正確な予測が可能です。<br>選択肢：手動で変換を行うことなく、トレーニング用にデータを送信します。AutoMLが適切な変換を行うようにします。トレーニング、検証、テストセット間の自動データ分割を選択します<br>この選択肢が正しくない理由は以下の通りです。<br>自動データ分割を選択すると、モデルの予測能力が弱まる可能性があります。時間に感応する予測モデルの場合、特定の時間順序での手動分割が適切な戦略です。これはテストデータが検証データの未来のデータであることを保証するからです。<br>選択肢：データを手動で変換せずにトレーニング用に送信し、適切な列をTime列として指定します。AIlowAutoMLは、提供されたタイムシグナルに基づいてデータを分割し、より新しいデータを検証およびテストセット用に確保します<br>この選択肢が正しくない理由は以下の通りです。<br>AutoML Tablesでは、データの分割を自動的に行う機能は提供されていません。そのため、時間シグナルに基づいてデータを自動的に分割し、新しいデータを検証およびテストセットに用いるというオプションは存在しません。'>
<div class='choice'> タイムシグナルを含むすべての列を手動で配列にまとめます。AutoMLにこの配列を適切に解釈させます。トレーニング、検証、テストセットにまたがる自動データ分割を選択します</div>
<div class='choice'> 手動での変換を行わずに、トレーニング用にデータを送信します。タイムシグナルがある列を使用して、データを手動で分割します。検証セットのデータがトレーニングセットのデータの30日後のものであること、テストセットのデータが検証セットの30日後のものであることを確認します</div>
<div class='choice'> データを手動で変換せずにトレーニング用に送信し、適切な列をTime列として指定します。AIlowAutoMLは、提供されたタイムシグナルに基づいてデータを分割し、より新しいデータを検証およびテストセット用に確保します</div>
<div class='choice'> 手動で変換を行うことなく、トレーニング用にデータを送信します。AutoMLが適切な変換を行うようにします。トレーニング、検証、テストセット間の自動データ分割を選択します</div>
</div>

<div class='question' data-multiple='false' data-question='問題23<br>データセットの探索的分析を行っているとき、あなたはカテゴリー特徴Aがかなりの予測力を持つが、時々欠落していることを発見しました。<br>あなたはこのデータセットをどう扱うべきですか？' data-answer='1' data-explanation='解説<br>正解は「カテゴリ特徴Aに欠損値用のクラスを追加します。特徴Aが欠損しているかどうかを示す新しいバイナリ特徴を作成します」です。<br>この問題では、欠落データの扱い方について問われています。カテゴリ特徴Aが高い予測力を持つものの、時折欠落する状況に対処するための最適な方法を選択することが求められています。選択肢を見るとき、予測に有用な情報を保持しつつ、欠落データが存在する場合の影響を最小限に抑える方法を選ばなければなりません。完全性と予測力のバランスを保つことが鍵です。<br>基本的な概念や原則：<br>カテゴリ特徴：非数値データを含む特徴で、通常はテキストラベル（例えば赤、青、緑など）を使用します。これらはしばしばワンホットエンコーディングまたは整数エンコーディングなどの手法を用いて数値データに変換されます。<br>欠損値：データセット内で値が存在しない、または観測されていない項目を指します。欠損値の取り扱いは重要な前処理ステップであり、異なる取り扱い方が精度に影響を与えます。<br>ダミー変数：カテゴリ特徴の一種で、特定の条件が成立している（たとえば特徴が欠損している）かどうかを示すバイナリ値（0または1）を持つ変数です。特徴の欠損を示す新しいバイナリ特徴を追加することは、情報を維持しながら欠損データを処理する一つの方法です。<br>前処理：データ分析を行う前に、データをクリーニングし、フォーマットを変更し、必要な変換を行うプロセスです。前処理には欠損データの処理やエンコーディングの適用などが含まれます。<br>正解についての説明：<br>（選択肢）<br>・カテゴリ特徴Aに欠損値用のクラスを追加します。特徴Aが欠損しているかどうかを示す新しいバイナリ特徴を作成します<br>この選択肢が正解の理由は以下の通りです。<br>欠損データの問題を扱う一つの有効な方法は、その欠損そのものを新たな情報として扱うことです。特徴Aが時折欠損しており、その上その特徴が予測に大きな影響を及ぼすということならば、特徴Aが欠損していること自体が重要な情報を含んでいる可能性があります。<br>すなわち、特徴Aの欠損値を表す新たなクラスを追加することで、予測モデルは特徴Aが欠損しているインスタンスを別のグループ化に学習し、予測に役立てることができます。<br>さらに、特徴Aが欠損しているかどうかを示す新しいバイナリ特徴を作成することにより、モデルは特徴Aの存在と欠損の両方を考慮した予測ができるようになります。<br>したがって、このアプローチは欠損値を包括的に扱うことができ、予測性能の向上に寄与します。<br>不正解の選択肢についての説明：<br>選択肢：15%以上の値が欠落している場合は、特徴Aを削除します。そうでない場合は、特徴Aをそのまま使用します<br>この選択肢が正しくない理由は以下の通りです。<br>特徴Aが予測力を持つ重要なカテゴリであるにもかかわらず、一定の割合以上が欠落しているという単純な理由で特徴を削除するのは不適切です。特徴Aが欠落している場合でもその情報自体が重要な意味を持つ可能性があります。正解の選択肢では、欠損しているかどうかを示す新しい特徴を作成することで、その情報を補完しています。<br>選択肢：特徴Aの最頻値を計算し、それを使って特徴Aの欠損値を置き換えます<br>この選択肢が正しくない理由は以下の通りです。<br>特徴Aの最頻値を計算し、それを使用して欠損値を置き換えると、特徴Aが欠損していることの情報が失われてしまいます。その結果、その特徴の欠損がデータの他の部分やターゲット変数と何らかの関連性がある場合には、その関連性を検出できなくなる可能性があります。適切な欠損値の対応が重要となるため、欠損値用のクラスを特徴Aに追加し、バイナリ特徴を作成することで欠損情報自体を特徴として利用します。<br>選択肢：欠損値を特徴Aとピアソン相関が最も高い特徴の値で置き換えます<br>この選択肢が正しくない理由は以下の通りです。<br>特徴Aとピアソン相関が最も高い特徴の値で置き換えると、欠損値がランダムに発生する際の原因を無視し、かつ特徴Aの真の重要性を過大評価する可能性があります。対して正解選択肢は欠損値を新しいカテゴリとして扱うことで、データの本来の意味を歪めることなくその特徴を活用します。'>
<div class='choice'> 欠損値を特徴Aとピアソン相関が最も高い特徴の値で置き換えます</div>
<div class='choice'> カテゴリ特徴Aに欠損値用のクラスを追加します。特徴Aが欠損しているかどうかを示す新しいバイナリ特徴を作成します</div>
<div class='choice'> 15%以上の値が欠落している場合は、特徴Aを削除します。そうでない場合は、特徴Aをそのまま使用します</div>
<div class='choice'> 特徴Aの最頻値を計算し、それを使って特徴Aの欠損値を置き換えます</div>
</div>

<div class='question' data-multiple='false' data-question='問題24<br>データサイエンスチームは、事前に訓練されたResNetモデルに基づいて、画像分類用のPyTorchモデルを訓練しています。いくつかのパラメータを最適化するためにハイパーパラメータチューニングを行う必要があります。<br>この要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「カスタムコンテナを使用してAI Platform上でハイパーパラメータチューニングジョブを実行します」です。<br>この問題では、画像分類用のPyTorchモデルのハイパーパラメータチューニングが要求されています。ここで重要な点は、使用するツールがPyTorchの場合に対応可能なものであること、そしてモデルが事前訓練済みのResNetモデルをベースにしているという情報です。したがって、選択肢を考慮する際には、これらの情報が必要条件を満たす解決策を探すことが求められます。誘導的にPyTorchをTensorFlowやKerasに変換する選択肢は避けるべきです。<br>基本的な概念や原則：<br>ResNetモデル：深層学習で用いられる、深さが非常に大きいニューラルネットワークの一種です。層の深さによる学習の困難さを解決するために、ショートカット接続を導入しています。<br>PyTorch：Pythonベースのオープンソースの機械学習ライブラリで、ディープラーニングのための計算グラフ概念を提供します。<br>ハイパーパラメータチューニング：機械学習モデルの学習プロセスに影響を与えるパラメータの調整を指します。これを最適化することで、モデルの性能が改善します。<br>カスタムコンテナ：ユーザが定義した独自のソフトウェアと設定を含むDockerコンテナです。特定の要件に合わせた環境を作成するのに使用されます。<br>AI Platform：機械学習モデルの開発、訓練、予測を行うためのGoogle Cloudのフルマネージドサービスです。ハイパーパラメータチューニングをサポートしています。<br>Keras Tuner、Katib：それぞれKerasとKubeflowに対応したハイパーパラメータ最適化ライブラリですが、PyTorchモデルに直接使用できません。<br>正解についての説明：<br>（選択肢）<br>・カスタムコンテナを使用してAI Platform上でハイパーパラメータチューニングジョブを実行します<br>この選択肢が正解の理由は以下の通りです。<br>まず、Google CloudのAI Platformはモデル訓練と予測のためのクラウドベースの一般的なマシン学習プラットフォームで、ハイパーパラメータチューニングを行う機能が用意されています。ハイパーパラメータチューニングは計算機が行うためモデル開発者が手作業で行う必要が無く、効率的にパラメータの最適化が可能です。<br>また、AI PlatformはPyTorchやTensorFlowなどの様々なフレームワークに対応していますが、内蔵のランタイムではサポートされていないパッケージやツールを使用する必要がある場合、カスタムコンテナを使用することで対応が可能です。<br>カスタムコンテナを使用すれば特定のバージョンのPyTorchや事前に訓練されたResNetモデルなど、必要なものをコンテナ内部にパッケージングすることができます。<br>したがって、カスタムコンテナを使用してAI Platform上でハイパーパラメータチューニングを行うことで、データサイエンスチームが効率的に最適なパラメータを調整できるというわけです。<br>不正解の選択肢についての説明：<br>選択肢：モデルをKerasモデルに変換し、Keras Tunerジョブを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>モデルをKerasモデルに変換すると、チームが訓練したPyTorchモデルの利点が活かせません。ハイパーパラメータチューニングは、モデルのフレームワークを変えずに行いたいニーズがあります。対して正しい選択肢は、元のフレームワークのままハイパーパラメータチューニングをすることを可能にします。<br>選択肢：Kuberflow Pipelinesインスタンスを作成し、Katib上でハイパーパラメータチューニングジョブを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>Kubeflow PipelinesとKatibもハイパーパラメータチューニングのために利用できますが、導入と管理が必要で運用が複雑になります。<br>一方、AI Platformではカスタムコンテナを使用してハイパーパラメータチューニングジョブを実行でき、管理の手間を省けます。<br>選択肢：モデルをTensorFlowモデルに変換し、AI Platform上でハイパーパラメータチューニングジョブを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>モデルをTensorFlowに変換することは不要で、むしろ時間と労力を無駄にする可能性があります。カスタムコンテナを使用すれば、PyTorchモデルに対して直接AI Platformでハイパーパラメータチューニングが可能なため、より効率的です。'>
<div class='choice'> モデルをTensorFlowモデルに変換し、AI Platform上でハイパーパラメータチューニングジョブを実行します</div>
<div class='choice'> カスタムコンテナを使用してAI Platform上でハイパーパラメータチューニングジョブを実行します</div>
<div class='choice'> Kuberflow Pipelinesインスタンスを作成し、Katib上でハイパーパラメータチューニングジョブを実行します</div>
<div class='choice'> モデルをKerasモデルに変換し、Keras Tunerジョブを実行します</div>
</div>

<div class='question' data-multiple='false' data-question='問題25<br>あなたはあるeコマース企業のMLエンジニアで、物流チームが毎月注文すべき在庫量を予測するモデルの構築を任されています。<br>あなたはどのアプローチを取るべきですか？' data-answer='3' data-explanation='解説<br>正解は「時系列予測モデルを使用して、各商品の月間売上を予測します。その結果をロジスティクスチームに渡し、彼らがモデルによって予測された量を在庫の基礎とできるようにします」です。<br>この問題では、具体的なビジネスシナリオで機械学習（ML）モデルを適切に適用するための理解が求められています。具体的には、在庫量を予測するために適切なMLアプローチを理解する必要があります。設問で提供された業務内容と正解、不正解の選択肢を見て、各MLアプローチがどのような目的に適しているか、また特定の業務目的に対してどのアプローチが最も効果的かを評価することがキーとなります。<br>基本的な概念や原則：<br>時系列予測モデル：過去のデータに基づいて未来の値を予測するための統計学的手法です。時間的なパターンとトレンドを捉えるため、在庫管理や販売予測などによく使用されます。<br>クラスタリングアルゴリズム：似たような特徴を持つデータを同じグループに分けるアルゴリズムです。人間の介入なく自動的にグループ分けを行いますが、予測には直接用いられません。<br>回帰モデル：数値データと一つ以上の説明変数の関係性を表すモデルです。数値予測に使用されますが、特定の時間パターンを捉えるためには工夫が必要です。<br>分類モデル：データを特定のカテゴリに分類するためのモデルです。直接的な数値予測には向きませんが、カテゴリ分けによる見通しを立てるために利用されます。<br>正解についての説明：<br>（選択肢）<br>・時系列予測モデルを使用して、各商品の月間売上を予測します。その結果をロジスティクスチームに渡し、彼らがモデルによって予測された量を在庫の基礎とできるようにします<br>この選択肢が正解の理由は以下の通りです。<br>時系列予測モデルは、過去のデータに基づいて未来の値を予測するのに最適です。この場合、それはeコマース企業の過去の売上データを使用して、各商品の未来の売上を予測します。このような予測は、物流チームが必要な在庫量を決定するのに役立ちます。これは在庫管理と需要予測の一般的なアプローチで、多くのビジネスで従来から採用されています。MLエンジニアとして、時系列予測モデルを使用することで、企業全体の効率を向上させ、在庫管理におけるヒューマンエラーや過剰在庫、欠品リスクを軽減します。<br>そして、その予測の結果を物流チームが実際の在庫管理の基礎とすることで、事業の運営をスムーズに進め、顧客の満足度を向上させることが期待できます。<br>したがって、この選択肢は適切なアプローチです。<br>不正解の選択肢についての説明：<br>選択肢：クラスタリングアルゴリズムを使って人気商品をグループ化します。そのリストを物流チームに渡し、人気商品の在庫を増やします<br>この選択肢が正しくない理由は以下の通りです。<br>クラスタリングアルゴリズムは商品をグループ化することはできますが、商品の月間売上を予測する目的には不適切です。具体的な数値予測には時系列予測などの回帰モデルが適しています。<br>選択肢：回帰モデルを使用して、毎月どれくらいの追加在庫を購入すべきかを予測します。その結果を月初にロジスティクスチームに渡し、モデルで予測された量だけ在庫を増やすようにします<br>この選択肢が正しくない理由は以下の通りです。<br>回帰モデルは単純に数値間の関係を求めるモデルで、時系列の変動を考慮した予測は採用できません。正解の選択肢のように時系列予測モデルを使うことで、過去データの時系列パターンを利用して予測ができ、高精度な在庫予測が可能です。<br>選択肢：分類モデルを使用して、在庫レベルをUNDER_STOCKED、OVER_STOCKED、CORRECTLY_STOCKEDに分類する毎月、ロジスティクスチームにレポートを渡し、在庫レベルを微調整できるようにします<br>この選択肢が正しくない理由は以下の通りです。<br>分類モデルを使用して在庫レベルをUNDER_STOCKED、OVER_STOCKED、CORRECTLY_STOCKEDに分類するアプローチは具体的な在庫量を予測しないため、ロジスティクスチームが必要な在庫量を正確に調整するのが難しいです。<br>一方、時系列予測モデルを使用すれば各商品について具体的な月間売上を予測することができ、その結果を在庫の基礎とすることができます。'>
<div class='choice'> クラスタリングアルゴリズムを使って人気商品をグループ化します。そのリストを物流チームに渡し、人気商品の在庫を増やします</div>
<div class='choice'> 回帰モデルを使用して、毎月どれくらいの追加在庫を購入すべきかを予測します。その結果を月初にロジスティクスチームに渡し、モデルで予測された量だけ在庫を増やすようにします</div>
<div class='choice'> 分類モデルを使用して、在庫レベルをUNDER_STOCKED、OVER_STOCKED、CORRECTLY_STOCKEDに分類する毎月、ロジスティクスチームにレポートを渡し、在庫レベルを微調整できるようにします</div>
<div class='choice'> 時系列予測モデルを使用して、各商品の月間売上を予測します。その結果をロジスティクスチームに渡し、彼らがモデルによって予測された量を在庫の基礎とできるようにします</div>
</div>

<div class='question' data-multiple='false' data-question='問題26<br>あなたは、300万枚のX線画像からなるデータセットで物体検出機械学習モデルをトレーニングしています。Vertex AI Trainingを使用して、32コア、128GBのRAM、および1つのNVIDIA P100 GPUを搭載したCompute Engineインスタンス上でカスタムトレーニングアプリケーションを実行しています。モデルのトレーニングに非常に時間がかかっていることに気づきました。モデルのパフォーマンスを犠牲にすることなく、トレーニング時間を短縮したいと考えています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='0' data-explanation='解説<br>正解は「トレーニングジョブのNVIDIA P100 GPUをv3-32 TPUに置き換えます」です。<br>この問題では、大量のデータセットで機械学習モデルのトレーニングを行っていて、その過程でトレーニング時間が長くなっている状況に対処方法を尋ねています。あくまでもモデルのパフォーマンスを犠牲にしないまま、トレーニング時間を短縮する方法を選ばなければなりません。各選択肢がそれぞれどのようにトレーニング速度に影響を与え、結果的にパフォーマンスにどう影響するのかを見極めることが必要です。<br>基本的な概念や原則：<br>Tensor Processing Unit（TPU）：Googleが開発した専用の機械学習アクセラレータで、大規模なニューラルネットワークタスクを高速に実行します。GPUよりも高速で電力効率も優れています。<br>NVIDIA P100 GPU：高性能のグラフィックスプロセッサユニットで、機械学習や高性能計算に広く使用されます。ただし、TPUと比較すると一般的に性能は劣ります。<br>バッチサイズ：トレーニング中に一度に処理するデータの数です。バッチサイズを大きくすると、メモリ使用量が増えますが、トレーニング速度が向上することもあります。<br>早期停止：ニューラルネットワークのトレーニングを効率的に行うための手法です。検証誤差が改善しなくなった場合にトレーニングを停止します。<br>tf.distribute.StrategyAPI：TensorFlowのAPIで、複数のマシンやデバイス上でのモデルの分散トレーニングを可能にします。分散型トレーニングはモデルのトレーニングを高速化する一方、一部のモデルでは性能が低下することもあります。<br>Vertex AI Training：Google Cloudのフルマネージド機械学習モデルトレーニングサービスです。様々なハードウェアオプション（CPU、GPU、TPU）を用いてカスタムトレーニングジョブを実行できます。<br>正解についての説明：<br>（選択肢）<br>・トレーニングジョブのNVIDIA P100 GPUをv3-32 TPUに置き換えます<br>この選択肢が正解の理由は以下の通りです。<br>まず、TPU（Tensor Processing Unit）はGoogleが開発したカスタムチップで、大量のデータを扱うニューラルネットワークの学習と推論の高速化に特化しています。そのため、コンピューティングリソースが集中して必要となるモデルのトレーニングを高速かつ効率的に行うことができます。したがってトレーニング時間の短縮に寄与します。<br>この場合、高パフォーマンスなNVIDIAのP100 GPUをv3-32 TPUに置き換えることで、計算能力を大幅に高めることができます。TPUの使用は、同じコンピューティングリソースでより多くのトレーニングを行い、モデルのトレーニング時間を大幅に短縮させる場合に特に有効です。<br>また、モデルのパフォーマンスを犠牲にすることなく、より多くのデータを並行して処理する能力は、大規模なデータセットでのトレーニングに求められる主要な要件です。このため、特に高性能なTPUを活用することで、この要件を満たすことができます。<br>不正解の選択肢についての説明：<br>選択肢：インスタンスメモリを512GBに増やし、バッチサイズを大きくします<br>この選択肢が正しくない理由は以下の通りです。<br>バッチサイズを増やすことでトレーニングのスピードは上がりますが、それによりメモリの使用量が増えるため、メモリを増やす措置も必要となってきます。<br>しかし、これではトレーニングにかかる時間の削減は期待できても、モデルのパフォーマンスには影響がないため、要件を満たすことはできません。<br>一方、TPUに置き換えると、GPUよりも高速な計算が可能となり、トレーニング時間を短縮できます。<br>選択肢：Vertex AI Trainingジョブの早期停止を有効にします<br>この選択肢が正しくない理由は以下の通りです。<br>早期停止はトレーニング時間を短縮する方法の一つですが、モデルのパフォーマンスを犠牲にします。<br>一方、トレーニングジョブのNVIDIA P100 GPUをv3-32 TPUに置き換えると、より高速な計算性能を得られ、トレーニング時間を短縮できるため正解の選択肢です。<br>選択肢：tf.distribute.StrategyAPIを使用し、分散トレーニングジョブを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>tf.distribute.StrategyAPIを使用して分散トレーニングジョブを行うという選択肢は、理論的にはトレーニング時間を短縮できますが、既存のモデルやトレーニングスクリプトの大幅な改訂が必要になる可能性があります。<br>一方、v3-32 TPUを使用すれば、強力な計算能力を簡単に追加してトレーニング時間を短縮することができます。'>
<div class='choice'> トレーニングジョブのNVIDIA P100 GPUをv3-32 TPUに置き換えます</div>
<div class='choice'> tf.distribute.StrategyAPIを使用し、分散トレーニングジョブを実行します</div>
<div class='choice'> インスタンスメモリを512GBに増やし、バッチサイズを大きくします</div>
<div class='choice'> Vertex AI Trainingジョブの早期停止を有効にします</div>
</div>

<div class='question' data-multiple='false' data-question='問題27<br>あなたは最近、ディープラーニングモデルを開発しました。新しいモデルをテストするために、大規模なデータセットで数エポック学習させました。トレーニング実行中、トレーニング損失と検証損失がほとんど変化しないことを確認しました。あなたはモデルを素早くデバッグしたいと考えています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='0' data-explanation='解説<br>正解は「モデルがデータセットの小さなサブセットで損失値は低いを得られることを検証します」です。<br>この問題では、大規模なデータセットで学習したディープラーニングモデルが問題を抱えており、そのデバッグ方法を求めています。ここで特に注意すべきは、トレーニング損失と検証損失がほとんど変化しないという状況です。これは一般的に、モデルが学習に失敗しているか、適切なパラメータが設定されていないことを示しています。選択肢を選ぶ際には、この点を踏まえ、モデルのデバッグに適した手法をよく考える必要があります。<br>基本的な概念や原則：<br>小さなサブセットでのモデル学習：初期段階のモデルを検証するための手法です。時間とリソースの消費を抑えながら、モデルの学習が適切に進行しているかを素早く確認することができます。良好な学習結果を得られない場合は、モデルやデータセットに問題がある可能性が高いです。<br>特徴エンジニアリング：手作りの特徴を追加することで、モデルがデータから抽出する情報を補完や改善する手法です。ただし、過度な特徴エンジニアリングはモデルの一般化能力を低下させる可能性もあるため、注意が必要です。<br>ハイパーパラメータチューニング：モデルの学習率などのハイパーパラメータを自動的に最適化する手法です。Vertex AIハイパーパラメータチューニングサービスはその一例です。ただし、初期段階のモデル検証には適さない場合もあります。<br>ハードウェアアクセラレータ：GPUやTPU等、ディープラーニングの計算負荷を軽減するための専用ハードウェアです。クラウド環境で利用可能で、大量のデータを素早く学習させることができます。ただし、モデル検証の初期段階で必要なエポック数を増やすことは、問題の特定や解決に直接は寄与しません。<br>正解についての説明：<br>（選択肢）<br>・モデルがデータセットの小さなサブセットで損失値は低いを得られることを検証します<br>この選択肢が正解の理由は以下の通りです。<br>トレーニング損失と検証損失がほとんど変化しないという現象は、モデルが学習に失敗している可能性を示しています。このような状況では、まずモデルが適切に機能しているか確認する必要があります。モデルがデータセットの小さなサブセットで損失値が低いことを確認することで、モデルが少なくとも小さなデータセットでは予想どおりの動作をすることを確認できます。これにより、モデルの基本的な機能確認を素早く実施することができ、モデルのデバッグが行いやすくなります。大規模なデータセットで完全なエポック学習を行うよりも、小規模なデータセットで素早くフィードバックを得ることが可能であり、より効率的なデバッグが可能になります。<br>不正解の選択肢についての説明：<br>選択肢：ドメイン知識をモデルに注入するために、手作りの特徴を追加します<br>この選択肢が正しくない理由は以下の通りです。<br>手作りの特徴を追加することでモデルの改善に役立つこともありますが、その対応だけでは素早くデバッグする要件を満たすわけではありません。そもそもの問題点は、損失がほとんど減少していない、つまりモデルが学習していないことなので、まずはモデルが学習できることを確認するのが正しい戦略です。<br>選択肢：Vertex AIハイパーパラメータチューニングサービスを使用して、より良い学習率を特定します<br>この選択肢が正しくない理由は以下の通りです。<br>トレーニング損失と検証損失がほとんど変化しない状況はモデルの学習が進行していないことを示します。これは、学習率だけが問題であるとは限らず、より根本的な問題が存在する可能性があります。そのため、まずはモデルがデータセットの小さなサブセットで正常に学習できるかを確認することが重要です。これによりモデルの基本的な機能を素早くデバッグできます。<br>選択肢：ハードウェアアクセラレータを使用し、より多くのエポック数でモデルをトレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>ハードウェアアクセラレータを使用してより多くのエポック数でモデルをトレーニングする選択肢は、既にトレーニング及び検証の損失が変動しない問題を解決するものではなく、時間を削減するための手段です。<br>一方、正解選択肢のデータセットの小さなサブセットで検証は、問題特定とデバッグを迅速に行うための有効なアプローチです。'>
<div class='choice'> モデルがデータセットの小さなサブセットで損失値は低いを得られることを検証します</div>
<div class='choice'> Vertex AIハイパーパラメータチューニングサービスを使用して、より良い学習率を特定します</div>
<div class='choice'> ドメイン知識をモデルに注入するために、手作りの特徴を追加します</div>
<div class='choice'> ハードウェアアクセラレータを使用し、より多くのエポック数でモデルをトレーニングします</div>
</div>

<div class='question' data-multiple='false' data-question='問題28<br>あなたは、患者のX線画像から特定の疾患の有無を検出するための二値分類MLアルゴリズムに取り組んでいます。データセットでは、95%の画像に疾患が見られないため、データは大きく不均衡です。<br>この場合、あなたのモデルのパフォーマンスを最も適切に評価する指標はどれですか？' data-answer='1' data-explanation='解説<br>正解は「F1値」です。<br>この問題では、不均衡なデータセットを使用している状況下でモデルのパフォーマンスを適切に評価できる指標を問われています。ここでは、モデルの精度だけでなく、疾患が見られない画像も適切に識別できるかどうかが重要です。そのため、明らかに不均衡なデータセットに対して効果的な評価指標を選択することがポイントとなります。<br>基本的な概念や原則：<br>F1値：精度とリコールの調和平均です。データが不均衡な場合や正と負の予測が同等に重要な場合に適した評価指標とされています。<br>精度（Precision）：予測された正のサンプルのうち実際に正であった割合です。全体のうちで正確に予測できた割合を示します。<br>リコール（Recall）：実際に正であったサンプルのうち、予測で正と判定されたものの割合です。全体の正のうちどのくらいを見つけ出せたかを示します。<br>RMSE（Root Mean Square Error）：予測値と真の値の差分の二乗平均の平方根です。連続量の予測誤差を測定するために使用されます。<br>不均衡なデータ：一部のクラスに大量のインスタンスが存在し、他のクラスにはほとんど存在しないデータセットのことです。この種のデータは、分類モデルにとって特に困難です。<br>正解についての説明：<br>（選択肢）<br>・F1値<br>この選択肢が正解の理由は以下の通りです。<br>まず、不均衡なデータセットに対するモデルのパフォーマンスを評価する際には、正確性（Accuracy）だけではなく、精度（Precision）、再現率（Recall）など他の指標を考慮することが重要です。具体的には、モデルが疾患ありと予測したX線画像が実際にどれだけ疾患であるかを示す精度と、実際の疾患全体の中でどれだけの割合を検出できたかを示す再現率を考慮します。<br>そして、F1値は精度と再現率の調和平均であり、これら二つのバランスを示す値としてよく利用されます。偽陽性と偽陰性の両方を考慮に入れ、モデルが正しく疾患を検出できるかどうかを評価するには非常に適切な指標です。<br>したがって、この場合ではF1値が最も適切な評価指標です。<br>不正解の選択肢についての説明：<br>選択肢：精密度<br>この選択肢が正しくない理由は以下の通りです。<br>精密度は偽陽性を適切に取り扱うが、一方で不均衡データでは多くの偽陰性が存在する可能性があり、それが考慮されません。対してF1値は精密度と再現率の調和平均であり、偽陽性と偽陰性の両方を考慮した評価が可能です。<br>選択肢：リコール<br>この選択肢が正しくない理由は以下の通りです。<br>リコールは偽陰性（疾患があるのに検出できなかったケース）を避けることに重きを置く指標ですが、データが不均衡な場合、単体での評価には限界があります。<br>一方、F1値は適合率とリコールの調和平均で、両者のバランスをとることができます。よってF1値が適切です。<br>選択肢：RMSE<br>この選択肢が正しくない理由は以下の通りです。<br>RMSE（平均二乗誤差の平方根）は平均的な予測誤差の大きさを評価する指標で、連続値を予測する回帰問題に使います。<br>しかし、今回のケースは二値分類問題であり、しかも不均衡なデータセットなので、真陽性と偽陽性、真陰性、偽陰性を考慮するF1値の方が適しています。'>
<div class='choice'> リコール</div>
<div class='choice'> F1値</div>
<div class='choice'> RMSE</div>
<div class='choice'> 精密度</div>
</div>

<div class='question' data-multiple='false' data-question='問題29<br>あなたは100以上の入力特徴量を持つ線形モデルを構築しています。あなたは、情報量の多い特徴量はそのままに、情報量の少ない特徴量をモデルから除去したいと考えています。<br>どの手法を使うべきですか？' data-answer='2' data-explanation='解説<br>正解は「L1正則化を使って、情報量の少ない特徴の係数を0にします」です。<br>この問題では、あなたが大量の入力特徴量を持つ線形モデルの構築を担当しており、情報量の少ない特徴量をモデルから除去する方法を求められています。ここで心掛けるべきは、各手法がどのようなシチュエーションや目的に使用されるのかを正確に理解することです。そして、それぞれの手法が問題のシナリオ、つまり情報量の少ない特徴量をモデルから除去するという特定の目標を達成するのにどれだけ適しているかを評価することが重要です。<br>基本的な概念や原則：<br>L1正則化：機械学習モデルの過適合を防ぐための手法です。情報量の少ない特徴の係数を0にすることで、不要な特徴量を自動的に除去します。<br>主成分分析（PCA）：多次元データの次元削減を行う統計的手法です。情報量の多い特徴量（主成分）を抽出し、データを新たな座標軸に変換します。<br>Shapley値：ゲーム理論に由来する、特徴量の影響を評価するための指標です。各特徴が目的変数に与える寄与度を評価します。<br>反復ドロップアウト手法：機械学習モデルのロバスト性を向上させるための手法です。ランダムに一部のニューロンを"消去"し、過学習を防止します。<br>正解についての説明：<br>（選択肢）<br>・L1正則化を使って、情報量の少ない特徴の係数を0にします<br>この選択肢が正解の理由は以下の通りです。<br>まず、L1正則化は特徴量の選択方法の一つであり、特徴量の多さがモデルの複雑さを増加させることを防ぎます。L1正則化は、回帰分析における係数の絶対値を最小化する尺度を付加することで、情報量の少ない、つまりモデルへの寄与が少ない特徴量の係数を0にします。このため、L1正則化はスパースな特徴量セットを生み出し、最終的に特徴量の数を削減します。<br>また、線形モデルにおいては、情報量の多い特徴量と情報量の少ない特徴量のバランスをとることが重要で、これは過学習を防ぎモデルの予測精度を改善します。これらの理由から、情報量の少ない特徴量をモデルから除去するためにL1正則化を使用することは適切です。<br>不正解の選択肢についての説明：<br>選択肢：主成分分析（PCA）を使用して、最も情報量の少ない特徴を除去します<br>この選択肢が正しくない理由は以下の通りです。<br>主成分分析（PCA）は特徴量の数を減らすための方法ではありますが、PCAは新たな特徴空間を作成し、既存の特徴量のどれが情報量が少ないかを明示的に識別するわけではありません。<br>一方、L1正則化は特徴量の係数を0にして特定の特徴量を無視することが可能で、情報量の少ない特徴量を直接的に特定、除去することができます。<br>選択肢：モデルを構築したら、Shapley値を使って、どの特徴が最も情報量が多いかを判断します<br>この選択肢が正しくない理由は以下の通りです。<br>Shapley値は、特徴の重要性を定量化するための手法であり、モデルの解釈性を改善するものですが、特徴量をモデルから除去する 効果はありません。<br>それに対して、L1正則化は係数を0にすることで、情報量の少ない特徴量を事実上モデルから除去します。<br>選択肢：反復ドロップアウト手法を使用して、どの特徴を削除してもモデルが劣化しないかを特定します<br>この選択肢が正しくない理由は以下の通りです。<br>反復ドロップアウト手法はニューラルネットワークの訓練における過学習を防ぐ手法であり、情報量の少ない特徴を特定し除去する目的には適していません。<br>一方、L1正則化は特徴選択を行い、情報量の少ない特徴を自動的にモデルから除去する効果があります。'>
<div class='choice'> 主成分分析（PCA）を使用して、最も情報量の少ない特徴を除去します</div>
<div class='choice'> モデルを構築したら、Shapley値を使って、どの特徴が最も情報量が多いかを判断します</div>
<div class='choice'> L1正則化を使って、情報量の少ない特徴の係数を0にします</div>
<div class='choice'> 反復ドロップアウト手法を使用して、どの特徴を削除してもモデルが劣化しないかを特定します</div>
</div>


            <!-- 他の問題も同様に追加 -->
        </div>

        <h2 id="question"></h2>
        <ul class="choices" id="choices"></ul>
        <button onclick="checkAnswer()">採点</button>
        <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
        <div class="result" id="result"></div>
    </div>

    <script>
        let currentQuestionIndex = 0;
        let correctCount = 0;
        const questions = [];

        document.addEventListener('DOMContentLoaded', () => {
            const questionElements = document.querySelectorAll('#quiz-data .question');
            questions.push(...Array.from(questionElements).map(questionElement => ({
                question: questionElement.getAttribute('data-question').replace(/\\n/g, '<br>'),
                choices: Array.from(questionElement.querySelectorAll('.choice')).map((choice, index) => ({
                    text: choice.innerHTML.replace(/\\n/g, '<br>'),  // innerHTMLに変更
                    index: index
                })),
                correctAnswer: questionElement.getAttribute('data-answer').split(',').map(Number),
                explanation: questionElement.getAttribute('data-explanation').replace(/\\n/g, '<br>'),
                multiple: questionElement.getAttribute('data-multiple') === 'true'
            })));
            showQuestion();
        });

        function shuffleArray(array) {
            for (let i = array.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [array[i], array[j]] = [array[j], array[i]];
            }
        }

        function showQuestion() {
            const questionElement = document.getElementById('question');
            const choicesContainer = document.getElementById('choices');
            const currentQuestion = questions[currentQuestionIndex];

            shuffleArray(currentQuestion.choices);

            questionElement.innerHTML = currentQuestion.question;
            choicesContainer.innerHTML = '';

            currentQuestion.choices.forEach((choice, i) => {
                const li = document.createElement('li');
                const input = document.createElement('input');
                const label = document.createElement('label');

                input.type = currentQuestion.multiple ? 'checkbox' : 'radio';
                input.name = 'choice';
                input.value = choice.index;
                input.id = 'choice' + i;

                label.htmlFor = 'choice' + i;
                label.innerHTML = choice.text;  // textContentをinnerHTMLに変更

                li.appendChild(input);
                li.appendChild(label);
                choicesContainer.appendChild(li);
            });

            document.getElementById('result').textContent = "";
            document.getElementById('nextButton').style.display = 'none';
        }

        function checkAnswer() {
            const currentQuestion = questions[currentQuestionIndex];
            const selectedChoices = Array.from(document.querySelectorAll('input[name="choice"]:checked'))
                                        .map(checkbox => parseInt(checkbox.value))
                                        .sort();
            const resultElement = document.getElementById('result');
            
            if (selectedChoices.length > 0) {
                const isCorrect = currentQuestion.multiple
                    ? selectedChoices.toString() === currentQuestion.correctAnswer.sort().toString()
                    : selectedChoices.length === 1 && selectedChoices[0] === currentQuestion.correctAnswer[0];
                
                if (isCorrect) {
                    resultElement.innerHTML = "正解です！<br>" + currentQuestion.explanation;
                    resultElement.style.color = "green";
                    correctCount++; // 正解数をカウント
                } else {
                    resultElement.innerHTML = "残念、不正解です。<br>" + currentQuestion.explanation;
                    resultElement.style.color = "red";
                }
                document.getElementById('nextButton').style.display = 'inline';
            } else {
                resultElement.textContent = "回答を選択してください。";
                resultElement.style.color = "orange";
            }
        }

        function nextQuestion() {
            currentQuestionIndex++;
            
            if (currentQuestionIndex < questions.length) {
                showQuestion();
            } else {
                showFinalResult();
            }
        }

        function showFinalResult() {
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2>問題終了！</h2>
                <p>あなたの正解数は ${correctCount} / ${questions.length} です。</p>
                <button onclick="restartQuiz()">再挑戦する</button>
            `;
        }

        function restartQuiz() {
            correctCount = 0;
            currentQuestionIndex = 0;

            // クイズのUI全体を初期化
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2 id="question"></h2>
                <ul class="choices" id="choices"></ul>
                <button onclick="checkAnswer()">採点</button>
                <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
                <div class="result" id="result"></div>
            `;

            // 初期化後に最初の問題を表示
            showQuestion();
        }        
    </script>
</body>
</html>