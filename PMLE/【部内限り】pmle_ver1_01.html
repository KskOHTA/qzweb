<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google Professional Machine Learning Engneer問題集 01</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="quiz-container">
        <div id="quiz-data" style="display: none;">


<div class='question' data-multiple='false' data-question='問題1<br>あるデータセットで探索的データ分析を行っているとき、重要なカテゴリ特徴に5％の欠損値があることに気づきます。あなたは、欠損値から生じる可能性のあるバイアスを最小化したいと思います。<br>欠損値をどのように扱うべきですか？' data-answer='0' data-explanation='解説<br>正解は「欠損値を、欠損値を示すプレースホルダカテゴリで置き換えます」です。<br>この問題では、探索的データ分析において欠損値の扱い方を問われています。"重要なカテゴリ特徴"が存在し、その中に"5％の欠損値"があるという状況と、欲しいアウトプットは"欠損値から生じるバイアスを最小化"したいという点に注意が必要です。選択肢を見ると、様々な欠損値の扱い方が提案されていますが、それらがどのような結果や影響を及ぼすかを正確に把握することが重要です。<br>基本的な概念や原則：<br>欠損値の扱い：データの欠損値はデータ分析や機械学習モデル作成に影響を与えます。適切な欠損値の取り扱い方法は、分析の目的やデータの性質によります。<br>欠損値を示すプレースホルダカテゴリ：欠損値がランダムに発生している場合や、欠損自体が何らかの情報を持つ場合には、新たなカテゴリとして欠損値を扱うことが有効です。これにより、欠損の情報も活用するとともにバイアスの影響を抑制できます。<br>行の削除：欠損値のある行を削除すると、欠損値がないデータのみで分析を進めることになり、バイアスが生じやすいです。また、重要な情報を失う可能性もあります。<br>特徴の平均値での埋め合わせ：数値データにおいては一般的な手法ですが、カテゴリデータの場合、適切な値を見つけるのが困難であり、誤解を招く可能性があります。<br>データのアップサンプリング：一部の行を複製してデータセットを人工的に増やす手法です。ただし、この手法も過学習やバイアスを引き起こす可能性があります。<br>正解についての説明：<br>（選択肢）<br>・欠損値を、欠損値を示すプレースホルダカテゴリで置き換えます<br>この選択肢が正解の理由は以下の通りです。<br>欠損値を置き換えることによって、データの分析や予測モデリングが可能になります。特にカテゴリーデータの場合、単に欠損値を削除すると、データセット内の他の特徴との関係性が歪んでしまい、その結果としてバイアスが増大する可能性があります。そのため、欠損値を示すプレースホルダカテゴリで置き換えることにより、欠損がランダムに発生した事実を保持し、その存在を認識しながらデータの処理を進めることができます。これによって欠損値から生じる可能性のあるバイアスを最小限に抑えることができます。<br>したがって、欠損値を示すプレースホルダカテゴリで置き換えるアプローチが適切です。<br>不正解の選択肢についての説明：<br>選択肢：欠損値のある行を削除し、データセットを5%アップサンプルします<br>この選択肢が正しくない理由は以下の通りです。<br>欠損値のある行を削除しデータセットをアップサンプルする方法は、欠損値から生じるバイアスを最小化するとは限りません。<br>一方、欠損値をプレースホルダカテゴリで置き換えると、元の行が保持されバイアスが最小化されます。<br>選択肢：欠損値を特徴の平均値で置き換えます<br>この選択肢が正しくない理由は以下の通りです。<br>特徴がカテゴリ型の場合、平均値で置き換えることは適切ではありません。平均値は連続特徴量での欠損値補填に使うべきです。対して正解の選択肢は、欠損値を特別なカテゴリとして扱うため、欠損が有意である情報も考慮しているため適切な選択肢です。<br>選択肢：欠損値のある行を検証データセットに移します<br>この選択肢が正しくない理由は以下の通りです。<br>欠損値のある行を検証データセットに移す方法は、欠損値から生じるバイアス対策という目的に対して誤ったアプローチです。これは訓練データと検証データ間の分布の一貫性を損なう可能性があります。<br>それに対し、欠損値をプレースホルダカテゴリで置き換える方法は欠損値を明示的に扱い、バイアスを最小化します。'>
<div class='choice'> 欠損値を、欠損値を示すプレースホルダカテゴリで置き換えます</div>
<div class='choice'> 欠損値のある行を削除し、データセットを5%アップサンプルします</div>
<div class='choice'> 欠損値を特徴の平均値で置き換えます</div>
<div class='choice'> 欠損値のある行を検証データセットに移します</div>
</div>

<div class='question' data-multiple='false' data-question='問題2<br>あなたのチームは、画像に運転免許証、パスポート、クレジットカードが含まれているかどうかを予測するモデルを構築する必要があります。データエンジニアリングチームはすでにパイプラインを構築し、運転免許証が写っている10,000枚の画像、パスポートが写っている1,000枚の画像、クレジットカードが写っている1,000枚の画像からなるデータセットを生成しました。次のようなラベルマップでモデルをトレーニングする必要があります：[drivers_license&#39;, &#39;passport&#39;, &#39;credit_card&#39;].<br>どの損失関数を使うべきですか？' data-answer='0' data-explanation='解説<br>正解は「カテゴリカルクロスエントロピー」です。<br>この問題では、適切な損失関数を選択するために、問題の性質と入力データの特性を理解することが重要です。具体的には、入力となる画像が運転免許証、パスポート、クレジットカードのいずれかを示すものであり、それぞれのクラスが排他的である点を理解する必要があります。また、トレーニングにはそれぞれのクラスに属する画像が含まれていて、その数がやや不均衡である点も考慮する必要があります。これらの情報を基に、選択肢の中から最も適切な損失関数を選ぶことが求められます。<br>基本的な概念や原則：<br>カテゴリカルクロスエントロピー：多クラス分類問題に適した損失関数で、正解ラベルと予測された確率分布の差を計算します。特に、ハードラベル（one-hotベクトル形式）に適しています。<br>カテゴリカルヒンジ：多クラス分類問題に適した損失関数の一つですが、マルチクラスSVMと同様の目的関数を持ちます。最大の誤ったクラスの予測と正しいクラスの予測間のマージンを最大化しようとします。<br>バイナリクロスエントロピー：２つのクラスだけが存在する二項分類問題に適用される損失関数です。正解ラベルと予測された確率の差を計測します。<br>スパースカテゴリカルクロスエントロピー：多クラス分類問題に使用される損失関数で、one-hotエンコーディングではなく整数値をラベルに使用します。ラベルのエンコーディングをスパース（非密集）にしたい場合に適しています。<br>正解についての説明：<br>（選択肢）<br>・カテゴリカルクロスエントロピー<br>この選択肢が正解の理由は以下の通りです。<br>まず、カテゴリカルクロスエントロピーは、マルチクラス分類問題における損失関数として最も一般的な選択肢です。今回の問題は、運転免許証、パスポート、クレジットカードのいずれが画像に含まれているかを予測するという3クラス分類問題であるため、カテゴリカルクロスエントロピーは適切な選択肢です。<br>カテゴリカルクロスエントロピーは、モデルの予測する確率分布と、真のデータの確率分布との間の距離を測定します。つまり、モデルの予測が正しいラベルの確率を高く、誤ったラベルの確率を低くするようにモデルをトレーニングします。この性質により、チームが目指すタスク、すなわち特定のクラス（運転免許証、パスポート、クレジットカード）が画像に含まれているかどうかを予測するモデルのトレーニングを適切に行うことが可能になります。<br>不正解の選択肢についての説明：<br>選択肢：カテゴリカルヒンジ<br>この選択肢が正しくない理由は以下の通りです。<br>カテゴリカルヒンジ損失はマルチラベル分類問題に対して使われますが、ここでの問題はマルチクラス（ただしクラス間は相互排他）な問題です。そのため、適切な損失関数はカテゴリカルクロスエントロピーです。<br>選択肢：バイナリクロスエントロピー<br>この選択肢が正しくない理由は以下の通りです。<br>バイナリクロスエントロピーは二値分類問題に使用され、出力ラベルが2つだけの場合に適しています。<br>それに対して、この問題では&#39;運転免許証&#39;、&#39;パスポート&#39;、&#39;クレジットカード&#39;の3つのラベルを予測する複数クラス分類問題であるため、3つ以上のクラス分類に適したカテゴリカルクロスエントロピーを使用するべきです。<br>選択肢：スパースカテゴリカルクロスエントロピー<br>この選択肢が正しくない理由は以下の通りです。<br>スパースカテゴリカルクロスエントロピーは目標変数が整数でラベリングされている場合に適していますが、ここではラベルマップが文字列のリストで提供されています。<br>それに対して、カテゴリカルクロスエントロピーは文字列のようなラベルマップに対して適した損失関数です。'>
<div class='choice'> カテゴリカルクロスエントロピー</div>
<div class='choice'> カテゴリカルヒンジ</div>
<div class='choice'> スパースカテゴリカルクロスエントロピー</div>
<div class='choice'> バイナリクロスエントロピー</div>
</div>

<div class='question' data-multiple='false' data-question='問題3<br>あなたはカテゴリ入力変数を持つデータセットを使ってMLモデルを開発しています。あなたはデータの半分を訓練セットとテストセットにランダムに分割しました。トレーニングセットのカテゴリ変数にワンホットエンコーディングを適用した後、テストセットで1つのカテゴリ変数が欠落していることに気づきます。<br>この要件を満たすために、どうすればよいですか？' data-answer='3' data-explanation='解説<br>正解は「テストデータのカテゴリー変数にワンホットエンコーディングを適用します」です。<br>この問題では、あなたが機械学習モデルを訓練するためのデータセットを扱っている状況を考える必要があります。訓練とテストのデータセットはランダムに分けられ、カテゴリー変数に対してワンホットエンコーディングが訓練データに適用されました。しかし、テストデータには一つのカテゴリー変数が欠けているという問題点があります。選択肢を見る際には、この問題を解決する最も効果的な方法を選ぶ必要があります。<br>基本的な概念や原則：<br>ワンホットエンコーディング：カテゴリ変数をバイナリベクトルとして表現する方法です。各要素は特定のカテゴリに対応し、そのカテゴリがデータセット内の特定のポイントに出現するかどうかを示します。<br>訓練セットとテストセット：機械学習モデルの訓練と評価に使用されるデータの分割です。訓練データはモデルの学習に、テストデータはモデルの未知のデータへの汎化能力の評価に使用されます。<br>スパース表現：多くの要素がゼロで、ほんの一部だけが非ゼロ値を持つデータ表現方法です。計算効率を高めることができますが、すべての機械学習アルゴリズムがスパース表現をうまく取り扱うわけではありません。<br>データの分割比率：訓練データとテストデータをどの程度の割合で分割するかを示すパラメータです。この比率は問題の性質と利用可能なデータの量によって決まります。<br>正解についての説明：<br>（選択肢）<br>・テストデータのカテゴリー変数にワンホットエンコーディングを適用します<br>この選択肢が正解の理由は以下の通りです。<br>ワンホットエンコーディングは、カテゴリ変数を扱うための一般的な手法であり、各カテゴリに対して一意のバイナリベクトルを生成します。学習セットでワンホットエンコーディングを適用した場合、テストセットでも同様のエンコーディングを適用する必要があります。これにより、トレーニングセットとテストセットのデータが同じ形式で表現され、モデルが適切に機能することが保証されます。<br>もしテストセットで新たなカテゴリが見つかれば、そのカテゴリに対応する新しいバイナリベクトルを作り出す必要がございます。<br>しかし、これが問題なのは、モデルが新しいバイナリベクトルに対応する特徴を学習していない場合です。そのため、よくある対策は未知のカテゴリをまとめて"その他"のようなカテゴリとして定義しておくことです。<br>以上より、テストデータのカテゴリー変数にワンホットエンコーディングを適用することが適切な対応です。<br>不正解の選択肢についての説明：<br>選択肢：テストセットではスパース表現を使用します<br>この選択肢が正しくない理由は以下の通りです。<br>スパース表現をテストセットで使用するとは、欠落しているカテゴリ変数の問題を解決するものではなく、ワンホットエンコーディング後の0の多いデータを効率的に格納する手法です。<br>一方、ワンホットエンコーディングをテストデータに適用することで、欠落しているカテゴリ変数を補うことができます。<br>選択肢：70％をトレーニングセット、30％をテストセットとして、データをランダムに再分配します<br>この選択肢が正しくない理由は以下の通りです。<br>データの70%を訓練セットに、30%をテストセットにランダムに再分配するとはいえ、これがカテゴリ変数の欠落問題を解決する保証はありません。<br>逆に、テストデータのカテゴリ変数にワンホットエンコーディングを適用することで、すべてのカテゴリ変数を正確に表現できます。<br>選択肢：すべてのカテゴリーを代表するデータをより多く集めます<br>この選択肢が正しくない理由は以下の通りです。<br>トレーニングとテストデータの分割後にさらにデータを集めることは、テストデータを与えられた状態から変えてしまうため、実際のパフォーマンス評価に影響を及ぼします。対して正解の選択肢であるテストデータにワンホットエンコーディングを適用することで、欠落しているカテゴリ変数を適切に扱うことができます。'>
<div class='choice'> テストセットではスパース表現を使用します</div>
<div class='choice'> 70％をトレーニングセット、30％をテストセットとして、データをランダムに再分配します</div>
<div class='choice'> すべてのカテゴリーを代表するデータをより多く集めます</div>
<div class='choice'> テストデータのカテゴリー変数にワンホットエンコーディングを適用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題4<br>あなたは、センサーの読み取り値に基づいて生産ラインの部品の故障を調査するよう依頼されました。データセットを受け取った後、あなたは、読み取り値の1%未満が故障インシデントを表す正のサンプルであることに気づきました。あなたはいくつかの分類モデルの学習を試みましたが、どれも収束しませんでした。<br>クラスの不均衡問題をどのように解決すべきですか？' data-answer='2' data-explanation='解説<br>正解は「アップウェイトを使用してデータをダウンサンプリングし、10％が正のサンプルであるサンプルを作成します」です。<br>この問題では、機械学習におけるクラスの不均衡問題を解決するための戦略について問われています。明らかにデータセットには差があり、それが分類モデルの学習の収束を妨げています。問題文からは、どのモデルも収束しないと述べているため、正確な分類を達成するために、その問題を対処するための適切な戦略を選択する必要があります。不均衡なデータの扱いと、それがモデルのパフォーマンスにどのように影響するかを理解することが重要です。<br>基本的な概念や原則：<br>クラスの不均衡：クラス分布が均等でないデータセットにおける挑戦です。これは、一部のクラスに過剰に集中したデータによって、モデルが他のクラスを適切に学習できない結果です。<br>ダウンサンプリング：過剰なクラスのサンプルをランダムに削除する方法です。これにより、クラス間のバランスを改善します。<br>アップサンプリング：負のクラスの数を、興味のあるクラスの数（正のサンプル）と同等に増やす方法です。再サンプリングやブートストラップとも呼ばれます。<br>アップウェイト：正のサンプルに対して高い重みを与え、学習におけるその影響力を増加させる手法です。これはデータの不均衡を補正し、モデルがたくさんあるクラス（負のクラス）にバイアスされることを防ぎます。<br>畳み込みニューラルネットワーク：深層学習の一種で、主に画像認識タスクに使用されます。この問題文では、分類問題の解決には適していないため、これは不適切な手法です。<br>ソフトマックス活性化：複数の分類ラベルがある場合の出力層でよく使用される活性化関数です。クラス間の確率分布を予測します。この問題文では、クラスの不均衡問題を解決するための手法ではありません。<br>正解についての説明：<br>（選択肢）<br>・アップウェイトを使用してデータをダウンサンプリングし、10％が正のサンプルであるサンプルを作成します<br>この選択肢が正解の理由は以下の通りです。<br>まず、問題は故障インシデントのデータが不足していること、つまりクラスの不均衡問題を示しています。このような状況では一般的に、アップサンプリング（少数派クラスのデータを増やす）またはダウンサンプリング（多数派クラスのデータを減らす）といった手法が用いられます。<br>そして、この問題における正解の選択肢はそれらを組み合わせた方法を指しています。具体的には"アップウェイトを使用してデータをダウンサンプリングし、10％が正のサンプルであるサンプルを作成します"すなわち、あまり頻繁に現れない故障インシデントを表すクラス（正のサンプル）を増加させ（アップウェイト）、一方で全体としてはデータ量を減らす（ダウンサンプル）ことで、クラス間の不均衡を是正しようという方針です。これにより学習が収束しにくい問題が解消され、更には適切に学習モデルが機能しやすい状態を作り出すことが期待できます。<br>不正解の選択肢についての説明：<br>選択肢：クラス分布を使って10％の正のサンプルを生成します<br>この選択肢が正しくない理由は以下の通りです。<br>単純にクラス分布を使用して10%の正のサンプルを生成するだけでは、モデルが正のサンプルを適切に認識できず、結果的に収束困難な状況を解消できない可能性があります。<br>一方、アップウェイトを使用してデータをダウンサンプリングすると、正例と負例の比率を適切に調整することができ、適切な収束を促すことができます。<br>選択肢：最大プーリングとソフトマックス活性化の畳み込みニューラルネットワークを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>最大プーリングとソフトマックス活性化の畳み込みニューラルネットワークの使用は、非均衡データ問題に明確な解決策を提供せず、主に画像認識などの問題に使用されます。<br>一方、アップウェイトを使用してデータをダウンサンプリングすると、クラスが均等になり、分類モデルの学習が容易になります。<br>選択肢：正のサンプルと負のサンプルの数が等しくなるまで、負のサンプルを取り除きます<br>この選択肢が正しくない理由は以下の通りです。<br>負のサンプルを取り除くことでデータセットが失われ、重要な情報が失われてしまいます。<br>一方で、アップウェイトを使用してデータをダウンサンプリングすると、クラスの不均衡を補正することができ、欠落する情報も少ないです。'>
<div class='choice'> クラス分布を使って10％の正のサンプルを生成します</div>
<div class='choice'> 正のサンプルと負のサンプルの数が等しくなるまで、負のサンプルを取り除きます</div>
<div class='choice'> アップウェイトを使用してデータをダウンサンプリングし、10％が正のサンプルであるサンプルを作成します</div>
<div class='choice'> 最大プーリングとソフトマックス活性化の畳み込みニューラルネットワークを使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題5<br>scikit-learnを使用してカスタムMLモデルを構築しました。しかし、トレーニングに予想以上の時間がかかっています。モデルをVertex AI Trainingに移行し、モデルのトレーニング時間を改善することにしました。<br>あなたは、まず何を試すべきですか？' data-answer='2' data-explanation='解説<br>正解は「Vertex AI上のDLVM画像でモデルをトレーニングし、可能な限りNumPyとSciPyの内部メソッドを利用するようにコードを記述します」です。<br>この問題では、既存のMLモデルのトレーニング時間の短縮が目標とされています。このモデルはscikit-learnで開発されており、その移行先としてVertex AI Trainingが指定されています。問題を解く際は、選択肢の中からscikit-learnモデルを直接サポートするGoogle Cloudサービスを見つけることが重要です。また、指示された目標を達成するためには、単にサービスを変更するだけでなく、固有の機能や特性を利用して効率的にトレーニング時間を削減する解決策を選びましょう。<br>基本的な概念や原則：<br>Vertex AI Training：Google CloudのAIと機械学習のプラットフォームで、モデルのトレーニングと展開が可能です。インフラストラクチャの管理をせずにトレーニングを実行できます。<br>DLVM（Deep Learning Virtual Machine）：Google Cloudの仮想マシンイメージで、一連のディープラーニングと機械学習のライブラリをプレインストールしています。<br>NumPy：Pythonで数値計算を行うためのライブラリです。高性能な多次元配列オブジェクトとこれを操作するツールを提供します。<br>SciPy：Pythonで科学技術計算を行うためのライブラリです。NumPyに基づいており、より高度な機能を提供します。<br>Vertex AI上でのモデルトレーニング：Google CloudのVertex AIトレーニングを使用して、既存のモデルをCloud上で効率的にトレーニングする手法です。<br>Compute Engine：Google CloudのIaaS（Infrastructure as a Service）であり、仮想マシンを提供します。分散トレーニングなどに使用可能ですが、ハンドリングが必要です。<br>TensorFlow：オープンソースの機械学習ライブラリです。多岐にわたるタスクで使用できますが、モデルの移行は工数がかかる場合があります。<br>正解についての説明：<br>（選択肢）<br>・Vertex AI上のDLVM画像でモデルをトレーニングし、可能な限りNumPyとSciPyの内部メソッドを利用するようにコードを記述します<br>この選択肢が正解の理由は以下の通りです。<br>Google CloudのVertex AI Trainingは大規模なデータセットでMLモデルのトレーニングを行う際に役立つプラットフォームです。ここでは、深層学習仮想マシン（DLVM）という特殊なVMイメージを利用でき、その中にはMLやデータサイエンス作業に必要な一連のツールとライブラリがプリインストールされています。そのため、scikit-learnを使用したモデルトレーニングを行うのに適しています。<br>また、実際の実装においては、NumPyやSciPyの内部メソッドをフル活用することが推奨されます。これらは低レベルの数値演算を処理する能力があり、計算コストが高い操作を高度に最適化して実行します。これにより、モデルのトレーニング時間が大幅に短縮される可能性があります。<br>したがって、トレーニング時間を改善するためには、Vertex AIのDLVMイメージを使用し、可能な限りNumPyとSciPyの内部メソッドを活用することが推奨されます。<br>不正解の選択肢についての説明：<br>選択肢：モデルをTensorFlowに移行し、Vertex AI Trainingを使用してトレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>モデルをTensorFlowに移行するという選択肢は、モデル自体を再記述する大きな労力が必要です。<br>一方、DLVM画像でトレーニングすることで、既存のscikit-learnモデルをそのまま使用しながら、高速な計算資源を活用でき、トレーニング時間を改善することができます。<br>選択肢：複数のCompute Engine VMを使用して分散モードでモデルをトレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>Compute Engine VMでの分散トレーニングは確かにトレーニング時間を改善するかもしれませんが、その設定や管理は複雑で時間がかかります。<br>一方、Vertex AIは分散トレーニングを簡単に実施でき、またDLVM画像の使用はNumPyやSciPyのメソッドのパフォーマンスを向上させるため、より効率的です。<br>選択肢：GPUでVertex AI Trainingを使用してモデルをトレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>GPUでVertex AI Trainingを使用すると確かに計算速度が向上する可能性がありますが、これがベストな解答ではありません。これは、scikit-learnは基本的にGPUを使用しないライブラリであり、そのままGPUを利用してもトレーニング時間の改善は見込めなりません。逆にDLVM画像を使用し、NumPyやSciPyの内部メソッドを利用すれば、計算処理が高速化され、トレーニング時間が短縮する可能性があります。'>
<div class='choice'> GPUでVertex AI Trainingを使用してモデルをトレーニングします</div>
<div class='choice'> 複数のCompute Engine VMを使用して分散モードでモデルをトレーニングします</div>
<div class='choice'> Vertex AI上のDLVM画像でモデルをトレーニングし、可能な限りNumPyとSciPyの内部メソッドを利用するようにコードを記述します</div>
<div class='choice'> モデルをTensorFlowに移行し、Vertex AI Trainingを使用してトレーニングします</div>
</div>

<div class='question' data-multiple='false' data-question='問題6<br>あなたは世界的な自動車メーカーのMLエンジニアです。あなたは、世界中の様々な都市における自動車の販売台数を予測するMLモデルを構築する必要があります。<br>車種と販売台数の間の都市固有の関係を学習するために、どの特徴や特徴クロスを使うべきですか？' data-answer='1' data-explanation='解説<br>正解は「ビニングされた緯度、ビニングされた経度、およびワンホットエンコードされた車種の要素ごとの積として得られる1つの特徴」です。<br>この問題では、特徴エンジニアリングにおける適切なアプローチを選択する能力が試されています。特に注意しなければならないのは、予測対象が特定の都市における自動車の販売台数であり、そのために特定の特徴量組み合わせ（車種、緯度、経度）が適切に組み込まれた特徴を作成しなければならないことです。選択肢を評価する際には、これらの特徴が都市固有の結果を理解し反映させるためにどのように組み合わせられ、表現されているかを見極めることが重要です。<br>基本的な概念や原則：<br>ビニング：連続値を一定の幅（ビン）を持つ離散的なカテゴリに変換する処理です。地理的位置などの広範な値を扱いやすい形に変換します。<br>ワンホットエンコーディング：カテゴリ変数をバイナリベクトルに変換する手法です。特定のカテゴリは1、その他は0の値を持つことになります。MLモデルがカテゴリデータを効果的に利用できるようになります。<br>特徴クロス：複数の特徴間の交互作用を捉えるために使用される特徴工程手法です。特徴間の複雑な関係をモデリングするために使用されます。<br>MLモデル：機械学習アルゴリズムから構築され、特定のタスクに対する予測を提供する数学的表現です。教師あり学習、教師なし学習、強化学習など、さまざまなタイプの学習を用いて構築できます。<br>正解についての説明：<br>（選択肢）<br>・ビニングされた緯度、ビニングされた経度、およびワンホットエンコードされた車種の要素ごとの積として得られる1つの特徴<br>この選択肢が正解の理由は以下の通りです。<br>まず、都市固有の関係を捉えるためには、緯度と経度の組み合わせ（ビニングされた緯度と経度）が有効です。緯度と経度は地理的位置を特定しますが、それぞれ単独で使用した場合には、情報が粗すぎて価値が低い可能性があります。<br>しかし、これら二つを組み合わせる（ビニングまたはクロス）ことで、具体的な位置情報を捉えることができ、都市固有の情報を表します。<br>次に、ワンホットエンコードされた車種を特徴として使用することで、各都市での異なる車種の販売パフォーマンスを学習することができます。これにより、特定の都市で特定の車種が売れる可能性を予測する精度が向上します。<br>また、これらすべての特徴の要素ごとの積（特徴クロス）を使用することで、複数の特徴の相互作用（緯度と経度に基づく地理的位置と車種の相互作用など）を捉えられます。これは、特定の地域で特定の車種が売れるという複雑な関係を捉えるために重要です。<br>不正解の選択肢についての説明：<br>選択肢：次の3つの個別の特徴：ビニングされた緯度、ビニングされた経度、およびワンホットエンコードされた車種<br>この選択肢が正しくない理由は以下の通りです。<br>次の3つの個別の特徴：ビニングされた緯度、ビニングされた経度、およびワンホットエンコードされた車種を使用すると、これらの特徴間の関連性を捉えることができません。<br>一方、特徴の積を使用すると、各都市での車種と販売台数の間の都市固有の関係を学習することが可能になります。<br>選択肢：緯度、経度、車種間の要素ごとの積として取得される1つの特徴<br>この選択肢が正しくない理由は以下の通りです。<br>緯度と経度をそれぞれそのまま特徴として使用すると、モデルは細かい位置情報を捉えることが難しくなります。そのため、特定の地域やエリアに対するパターンを理解するのに十分な情報が得られません。<br>それに対して、ビニングされた緯度と経度を用いると、より大きな地域やセグメントに基づいた関連性を学習できます。<br>選択肢：2つの特徴量を要素ごとの積としてクロスさせる次の特徴：1つ目は、ビニングされた緯度とワンホットエンコードされた車種の積、2つ目は、ビニングされた経度とワンホットエンコードされた車種の積<br>この選択肢が正しくない理由は以下の通りです。<br>この選択肢では、緯度と車種、経度と車種が別々にクロスされています。これでは各都市（緯度と経度の組み合わせ）での車種の販売台数を適切に予測することが難しくなります。正解の選択肢では、緯度、経度、車種が一緒にクロスされているため、特定の都市での車種の販売台数を効果的に予測できます。'>
<div class='choice'> 次の3つの個別の特徴：ビニングされた緯度、ビニングされた経度、およびワンホットエンコードされた車種</div>
<div class='choice'> ビニングされた緯度、ビニングされた経度、およびワンホットエンコードされた車種の要素ごとの積として得られる1つの特徴</div>
<div class='choice'> 2つの特徴量を要素ごとの積としてクロスさせる次の特徴：1つ目は、ビニングされた緯度とワンホットエンコードされた車種の積、2つ目は、ビニングされた経度とワンホットエンコードされた車種の積</div>
<div class='choice'> 緯度、経度、車種間の要素ごとの積として取得される1つの特徴</div>
</div>

<div class='question' data-multiple='false' data-question='問題7<br>あなたは銀行に勤めており、不正検知のためのランダムフォレストモデルを構築しています。あなたはトランザクションを含むデータセットを持っています。<br>あなたの分類器の性能を向上させる前処理戦略はどれですか？' data-answer='2' data-explanation='解説<br>正解は「不正取引を10倍オーバーサンプルします」です。<br>この問題では、あなたが銀行で働いているデータサイエンテストであり、主に不正トランザクションを検出するためのランダムフォレストモデルの構築を担当しているという状況設定から始まっています。あなたの目指すべきゴールは、"分類器の性能を向上させる"とされています。したがって、選択肢を評価するときは、それが分類器の性能、特に不正トランザクションの検出能力を明確に向上させるかを重視すべきです。加えて、ランダムフォレストモデルにとって最適な前処理手法なのかも考慮する必要があります。<br>基本的な概念や原則：<br>不正取引のオーバーサンプリング：データセット内でマイナーなクラス（このケースでは不正取引）のデータを増加させる方法です。これにより、不均衡なデータセットのクラス間のバランスを改善し、マイナーなクラスのパターンを識別するモデルの能力を向上させます。<br>ランダムフォレストモデル：複数の決定ツリーからなる分類器です。各ツリーはランダムなサブセットの特徴を使用してトレーニングされ、最終的な予測はすべてのツリーの結果を集約して行われます。<br>データ不均衡：あるクラスのデータが他のクラスのデータに比べて大幅に少ない状態です。これは、特に不正取引のようなレアなイベントを予測するモデルの性能に影響を与える可能性があります。<br>TFRecords：データをバイナリ形式で保存するためのGoogleのフォーマットです。これはデータの読み書きを高速化し、大規模なデータセットの管理を簡易化しますが、モデルの性能向上には直接影響を与えません。<br>Zスコア正規化：数値データを標準化する一般的な手法です。各値から平均を引き、標準偏差で割ることで、データは平均0、標準偏差1の分布に変換されます。これはモデルの学習を助けますが、性能を直接向上させるものではありません。<br>ワンホットエンコーディング：カテゴリ変数をバイナリベクトル形式に変換する方法です。これはカテゴリ変数を扱うモデルの学習を助けますが、性能を直接向上させるものではありません。<br>正解についての説明：<br>（選択肢）<br>・不正取引を10倍オーバーサンプルします<br>この選択肢が正解の理由は以下の通りです。<br>不正取引のデータは、通常の取引のデータに比べて圧倒的に少ないケースが多いです。モデルが適切に学習するためには各クラスのデータがバランス良く存在することが望ましいため、不均衡なデータセットを扱う際にはオーバーサンプリングという手法が用いられます。この手法を用いると、少数派クラスのデータを増やすことでデータのバランスを調整します。ここでの"不正取引を10倍オーバーサンプルする"は、不正取引のデータ数を人工的に増やすことです。これにより、モデルは不正取引についてより多くの情報を学習することができ、分類器の性能を向上させることができます。<br>不正解の選択肢についての説明：<br>選択肢：TFRecordsにデータを書き込みます<br>この選択肢が正しくない理由は以下の通りです。<br>TFRecordsにデータを書き込むのは、データのストレージや入力の最適化手法ですが、分類器の性能を直接向上させる戦略ではありません。<br>一方、不正取引をオーバーサンプルすることは、不均衡なクラスを持つ問題に対する一般的な戦略で、分類器の性能を向上させます。<br>選択肢：すべての数値的特徴をZスコア正規化します<br>この選択肢が正しくない理由は以下の通りです。<br>すべての数値的特徴をZスコア正規化することは、特徴のスケールを揃える効果はありますが、不正取引を特定しやすくするための分類器の性能向上には直接寄与しません。<br>それに対して、不正取引をオーバーサンプルすることで、データの偏りを緩和し、不正取引を見つけやすくするため分類器の性能向上に直接寄与します。<br>選択肢：すべてのカテゴリー特徴にワンホットエンコーディングを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>ワンホットエンコーディングは、カテゴリー特徴を数値に変換する手法であるため、分類器の性能自体を向上させるものではありません。<br>一方、不正取引をオーバーサンプルするという前処理は、不正取引が少ないデータセットにおいてその特徴を強調し、分類器の性能を直接向上させる効果があるため、相応しい策とされます。'>
<div class='choice'> すべての数値的特徴をZスコア正規化します</div>
<div class='choice'> TFRecordsにデータを書き込みます</div>
<div class='choice'> 不正取引を10倍オーバーサンプルします</div>
<div class='choice'> すべてのカテゴリー特徴にワンホットエンコーディングを使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題8<br>Compute Engine上のGPUを搭載した仮想マシンを使用して、特定の画像に存在する政府IDのタイプを予測するコンピュータービジョンモデルをトレーニングする必要があります。トレーニングには次のパラメータを使用します：<br>- オプティマイザー: SGD<br>- 画像サイズ = 224 x 224<br>- バッチサイズ = 64<br>- エポック = 10<br>- Verbose = 2<br>トレーニング中に次のエラーが発生します。<br>ResourceExhaustedError: テンソルを割り当てるときにメモリ不足（OOM）が発生しました。<br>あなたはトラブルシューティングとして何をするべきですか？' data-answer='0' data-explanation='解説<br>正解は「バッチサイズを小さくします」です。<br>この問題では、GPUを用いたコンピュータービジョンモデルのトレーニング中に出たエラーに対するトラブルシューティングのアプローチを決定する必要があります。ここでのエラーは"ResourceExhaustedError: テンソルを割り当てるときにメモリ不足（OOM）が発生しました。"となっており、システムがデータを処理するためのリソースが不足していることが示されています。選択肢を考える際に、このエラーを解決するためにはシステムのメモリ負荷を軽減することが求められると理解することが重要です。<br>基本的な概念や原則：<br>バッチサイズ：一度にモデルに供給されるデータの量です。大きすぎると、メモリ不足（OOM）エラーを引き起こす可能性があります。<br>ResourceExhaustedError：これは通常、使用可能な物理メモリーが求められるメモリーよりも少ないときに発生するエラーです。<br>SGD（確率的勾配降下法）：オプティマイザの一つで、各更新でランダムに選択される1つのデータ点で勾配を計算します。<br>エポック：トレーニング中にデータセットがモデルを通過する回数です。<br>Verbose：トレーニング時のログの詳細レベルを制御するフラグです。ここでのエラーとは直接関連はありません。<br>GPUメモリ：GPU上でモデルのトレーニングを行うときに使用されるメモリです。このメモリの量が不足してくると、ResourceExhaustedErrorが発生します。<br>正解についての説明：<br>（選択肢）<br>・バッチサイズを小さくします<br>この選択肢が正解の理由は以下の通りです。<br>まず、ResourceExhaustedErrorは、メモリ上にデータを割り当てる際に必要な空間が不足していることを示しています。具体的にはコンピュータービジョンモデルのトレーニング中に、GPUメモリが不足していることを示唆しています。<br>ここでのキーとなるパラメータは"バッチサイズ"です。バッチサイズは、一度にGPUにロードされる画像の数を示します。つまり、バッチサイズが大きいと、一度に処理される画像の数が多くなり、それに伴い必要となるGPUメモリも増えます。<br>したがって、あるバッチサイズでメモリ不足のエラーが発生した場合、バッチサイズを小さくすることで一度に必要となるメモリ容量を減らし、エラーを解決することができます。<br>不正解の選択肢についての説明：<br>選択肢：オプティマイザを変更します<br>この選択肢が正しくない理由は以下の通りです。<br>オプティマイザを変更すると、モデルのトレーニング方式が変わりますが、メモリに対する負荷を直接的に減少させるわけではありません。<br>一方、バッチサイズを小さくすることで、一度に処理するデータの量が少なくなり、メモリ不足の問題を解消できる可能性があります。<br>選択肢：学習率を変更します<br>この選択肢が正しくない理由は以下の通りです。<br>学習率を変更するという選択肢はトレーニングの収束速度や精度に影響を与えるかもしれませんが、メモリ不足のエラー（ResourceExhaustedError）の解決には寄与しません。<br>一方で、バッチサイズを小さくすると同時に処理するデータの量が減り、メモリ使用量が減るためこの問題が解消されます。<br>選択肢：画像サイズを縮小します<br>この選択肢が正しくない理由は以下の通りです。<br>画像サイズを縮小すると、モデルのパフォーマンスに影響を与える可能性があります。一方でバッチサイズを小さくすることは、トレーニングのメモリ負荷の軽減に直接貢献し、リソース不足の問題に対処できます。このため画像サイズを縮小する選択肢は不適切です。'>
<div class='choice'> バッチサイズを小さくします</div>
<div class='choice'> オプティマイザを変更します</div>
<div class='choice'> 学習率を変更します</div>
<div class='choice'> 画像サイズを縮小します</div>
</div>

<div class='question' data-multiple='false' data-question='問題9<br>あなたは最近、概念実証（PoC）深層学習モデルを作成しました。全体的なアーキテクチャには満足していますが、いくつかのハイパーパラメータの値を決定する必要があります。Vertex AIでハイパーパラメータのチューニングを行い、モデルで使用されるカテゴリ特徴の適切な埋め込み次元と、最適な学習レートの両方を決定したいと考えています。以下の設定を行います：<br>- 埋め込み次元については、タイプをINTEGERに設定し、最小値を16、最大値を64とします。<br>- 学習率は、タイプをDOUBLEに設定し、最小値を10e-05、最大値を10e-02に設定します。<br>デフォルトのベイズ最適化チューニングアルゴリズムを使用しており、モデルの精度を最大化したいと考えています。トレーニング時間は気にしていません。<br>各ハイパーパラメータのスケーリングとmaxParallelTrialsはどのように設定しますか？' data-answer='1' data-explanation='解説<br>正解は「埋め込み次元にはUNIT_LINEAR_SCALE、学習率にはUNIT_LOG_SCALEを使用し、並列試行回数は少なめにします」です。<br>この問題では、Vertex AIのハイパーパラメータチューニングについて質問されており、特に埋め込み次元と学習率のスケーリング方法とそれに伴う並列試行回数の設定方法が求められています。ここで重要なのは、正しいスケーリングタイプを選択し、その上で効率的にチューニングを行うための並列試行数を設定することです。ハイパーパラメータの範囲とタイプ、チューニング目標（精度の最大化）とトレーニング時間についての配慮から、最適なスケーリングと並列試行数の設定を選択することが重要です。<br>基本的な概念や原則：<br>Vertex AI：Google Cloudの統合型MLプラットフォームで、データの準備、モデルのトレーニングとデプロイ、機械学習の予測を簡素化します。<br>深層学習モデル：大量のデータからパターンを学習し、予測を行う人工知能の一種です。多層のニューラルネットワークを使用します。<br>ハイパーパラメータチューニング：機械学習モデルの学習プロセスを制御するパラメータの調整を指します。これはモデルの性能に大きな影響を与えます。<br>埋め込み次元：カテゴリ変数を離散的なベクトル表現に変換するために使用される、埋め込みベクトルの次元数です。<br>学習率：ニューラルネットワークの学習速度を制御するハイパーパラメータです。大きすぎると学習が不安定になり、小さすぎると学習が遅くなります。<br>ベイズ最適化：ハイパーパラメータの最適な組み合わせを見つけるのに役立つアルゴリズムです。これは、従来のグリッドサーチやランダムサーチとは異なり、以前の試行から学習します。<br>UNIT_LINEAR_SCALE、UNIT_LOG_SCALE：ハイパーパラメータの値をどのようにスケール化するかを定義するVertex AIで使用されるスケーリングタイプです。UNIT_LINEAR_SCALEは値の範囲が線形であることを示し、UNIT_LOG_SCALEは対数的なスケーリングを示します。<br>maxParallelTrials：同時に行われるハイパーパラメータチューニングの試行の最大数を指定します。プロジェクト全体で使えるリソースに制限があるため、適切な数を設定することが重要です。<br>正解についての説明：<br>（選択肢）<br>・埋め込み次元にはUNIT_LINEAR_SCALE、学習率にはUNIT_LOG_SCALEを使用し、並列試行回数は少なめにします<br>この選択肢が正解の理由は以下の通りです。<br>まず、埋め込み次元はカテゴリ特徴の表現力を決定します。ベイズ最適化では、ある範囲の値を均等にサンプリングするためにユニットリニアスケールが適しています。<br>一方、学習率は非線形の影響を持つパラメータで、その値の小さな変化がモデルのパフォーマンスに大きな影響を与えます。従ってユニットログスケールは、小さい値と大きい値の間で均等にサンプリングを行うために適しています。<br>また、最適化の並列実行数は少なめに設定するのが良いとされています。理由は、ベイズ最適化が探索と活用のトレードオフを扱うため、多くの試行が同時に行われると、リソースが浪費される可能性があります。最適なパラメータを見つけるためには、適度な探索と活用が必要です。<br>不正解の選択肢についての説明：<br>選択肢：埋め込み次元にはUNIT_LINEAR_SCALEを、学習率にはUNIT_LOG_SCALEを使用し、多数の並列試行を使用します<br>この選択肢が正しくない理由は以下の通りです。<br>多数の並列試行を使用すると、ベイズ最適化チューニングアルゴリズムの特性上、試行間で情報を共有する機会が失われてしまいます。これは、個々の試行の結果を次の試行のスケジューリングに反映するアルゴリズムの性質に反しています。重要なのは、各試行の結果から学習し、次第に最適なハイパーパラメータを見つけていくことです。<br>選択肢：埋め込み次元にはUNIT_LOG_SCALEを、学習率にはUNIT_LINEAR_SCALEを使用し、多数の並列試行を使用します<br>この選択肢が正しくない理由は以下の通りです。<br>一般的にハイパーパラメータチューニングでは、学習率は対数スケール（UNIT_LOG_SCALE）、埋め込み次元はリニアスケール（UNIT_LINEAR_SCALE）で設定します。独立した結果を得るために、大量の並列試行を行うと結果が偏る可能性がありますから少なめの並列試行回数を設定することが推奨されます。<br>選択肢：埋め込み次元にはUNIT_LOG_SCALE、学習率にはUNIT_LINEAR_SCALEを使用し、並列試行回数は少なめにします<br>この選択肢が正しくない理由は以下の通りです。<br>埋め込み次元には範囲内で均等な値の探索が適しており、UNIT_LINEAR_SCALEが望ましいです。<br>一方、学習率の最適な値は通常対数的に分布することが多く、UNIT_LOG_SCALEの方が適しています。従ってこの設定では、適切な値の探索が難しくなります。'>
<div class='choice'> 埋め込み次元にはUNIT_LINEAR_SCALEを、学習率にはUNIT_LOG_SCALEを使用し、多数の並列試行を使用します</div>
<div class='choice'> 埋め込み次元にはUNIT_LINEAR_SCALE、学習率にはUNIT_LOG_SCALEを使用し、並列試行回数は少なめにします</div>
<div class='choice'> 埋め込み次元にはUNIT_LOG_SCALEを、学習率にはUNIT_LINEAR_SCALEを使用し、多数の並列試行を使用します</div>
<div class='choice'> 埋め込み次元にはUNIT_LOG_SCALE、学習率にはUNIT_LINEAR_SCALEを使用し、並列試行回数は少なめにします</div>
</div>

<div class='question' data-multiple='false' data-question='問題10<br>あなたは、生物の特性に基づいたディープラーニングMLモデルの実験を行っているバイオテクノロジーのスタートアップに勤めています。あなたのチームは、MLモデルの新しいアーキテクチャの初期段階の実験に頻繁に取り組んでおり、C++でカスタムTensorFlow演算を書いています。大規模なデータセットと大規模なバッチサイズでモデルをトレーニングします。典型的なバッチサイズは1024で、各例のサイズは約1MBです。すべての重みと埋め込みを含むネットワークの平均サイズは20GBです。<br>モデルにどのようなハードウェアを選択すべきですか？' data-answer='1' data-explanation='解説<br>正解は「それぞれ16個のNVIDIA Tesla A100 GPU（合計640 GB GPUメモリ）、96個のvCPU、および1.4 TB RAMが搭載されている、2台のa2-megagpu-16gマシンで構成されたクラスター」です。<br>この問題では、大規模なデータセットと大規模なバッチサイズでMLモデルのトレーニングをする際に、どのようなハードウェアを選択すべきかという観点から考える必要があります。問題文からは、ネットワークの平均サイズが20GBであることを勘案し、GPUメモリ、vCPU、RAMがそれぞれどの程度必要なのかを吟味しなければなりません。また、正解となる選択肢は、ハードウェアのスペックだけでなく、そのスペックを活用できるクラスター構成が適切なものである必要もあります。したがって、この問題を解く際はそれぞれの要素を適切に検討し、データセットとバッチサイズの要件を満たすハードウェアを選択することが求められます。<br>基本的な概念や原則：<br>ディープラーニングMLモデル：大量のデータを扱い、人間の脳の神経細胞のネットワークを模倣した深層学習の手法を用いた機械学習モデルです。<br>TensorFlow：Googleが開発したオープンソースの機械学習ライブラリで、ディープラーニングの演算を効率的に行うことができます。<br>GPU：Graphics Processing Unitの略称で、画像処理を高速に行うために設計されたプロセッサです。大量の並列処理能力を持つため、ディープラーニングの計算にも有効に利用されます。<br>vCPU：仮想CPUのことで、物理的なCPUコアを複数の仮想CPUに分割して利用する技術です。<br>RAM：Random Access Memoryの略称で、コンピュータの主記憶装置であり、一時的なデータの保存に使用されます。機械学習など、大量のデータを扱う処理ではRAMの容量が重要です。<br>NVIDIA Tesla A100：高速な演算能力を持つGPU製品で、機械学習やディープラーニングの計算に適しています。<br>n1-highcpu：Google Cloudで提供されるインスタンスタイプで、vCPUの数が多く設定されており、CPU密度の高い処理に適しています。<br>正解についての説明：<br>（選択肢）<br>・それぞれ16個のNVIDIA Tesla A100 GPU（合計640 GB GPUメモリ）、96個のvCPU、および1.4 TB RAMが搭載されている、2台のa2-megagpu-16gマシンで構成されたクラスター<br>この選択肢が正解の理由は以下の通りです。<br>モデルのサイズが20GBと大規模であるため、それを取り扱うには高いGPUメモリが必要です。<br>また、大量のデータを処理するための大規模なバッチサイズが必要であり、それには高性能のGPUが求められます。NVIDIA Tesla A100 GPUは高性能で大容量のメモリを持っています。2台のa2-megagpu-16gマシンで構成されたクラスターはそれぞれ16個のNVIDIA Tesla A100 GPUを搭載しており、合計して640GBのGPUメモリが得られます。これにより、大規模なデータセットと大規模なバッチサイズを処理するのに十分な能力を持ちます。<br>また、これらのマシンは96個のvCPUと1.4TBのRAMを持っていて、大規模な計算作業に適しています。<br>したがって、この選択肢は大規模なモデルとデータを処理するのに最適なハードウェアを提供します。<br>不正解の選択肢についての説明：<br>選択肢：それぞれ8個のNVIDIA Tesla V100 GPU（合計128GB GPUメモリ）を搭載した2台のn1-highcpu-64マシンと、64個のvCPUと58GB RAMを搭載したn1-highcpu-64マシンで構成されるクラスター<br>この選択肢が正しくない理由は以下の通りです。<br>提供されているハードウェアスペックは、モデルのネットワークサイズが20GBという要件を満たすにはGPUメモリが不十分であり、大規模なバッチサイズとデータセットに対してもリソースが不足しています。<br>一方、正解の選択肢は膨大なGPUメモリとRAMを提供し、これらの要件を十分に満たすことができます。<br>選択肢：v2-8 TPUと64 GB RAMを搭載したn1-highcpu-64マシンを備えたクラスター<br>この選択肢が正しくない理由は以下の通りです。<br>モデルの平均サイズが20GBであり、大規模なバッチサイズを利用するためには、64GBのRAMとv2-8 TPUではメモリ容量が十分でないという問題があります。<br>一方、選択肢1のa2-megagpu-16gマシンは合計640GBのGPUメモリを提供し、処理能力及びメモリ容量において適切です。<br>選択肢：それぞれ96個のvCPUと86 GB RAMを搭載した4台のn1-highcpu-96マシンで構成されるクラスター<br>この選択肢が正しくない理由は以下の通りです。<br>n1-highcpu-96マシンは、高CPU性能を提供しますが、この問題は大量のGPUメモリを必要とするディープラーニングタスクであり、対応するGPUなどのハードウェアリソースが不足しています。そのため、テンソル演算に最適化されたNVIDIA Tesla A100 GPUが搭載されたマシンがより適切です。'>
<div class='choice'> v2-8 TPUと64 GB RAMを搭載したn1-highcpu-64マシンを備えたクラスター</div>
<div class='choice'> それぞれ16個のNVIDIA Tesla A100 GPU（合計640 GB GPUメモリ）、96個のvCPU、および1.4 TB RAMが搭載されている、2台のa2-megagpu-16gマシンで構成されたクラスター</div>
<div class='choice'> それぞれ8個のNVIDIA Tesla V100 GPU（合計128GB GPUメモリ）を搭載した2台のn1-highcpu-64マシンと、64個のvCPUと58GB RAMを搭載したn1-highcpu-64マシンで構成されるクラスター</div>
<div class='choice'> それぞれ96個のvCPUと86 GB RAMを搭載した4台のn1-highcpu-96マシンで構成されるクラスター</div>
</div>

<div class='question' data-multiple='false' data-question='問題11<br>あなたは多国籍飲料会社のデータサイエンスチームに所属しています。あなたは、自然な風味のボトル入り飲料水の新商品について、各地域での収益性を予測するMLモデルを開発する必要があります。あなたは、全地域の製品タイプ、製品販売量、経費、利益を含む履歴データを提供されます。<br>あなたは、モデルの入力と出力として何を使うべきですか？' data-answer='2' data-explanation='解説<br>正解は「製品タイプと、緯度と経度の特徴の交差、その後のビニングを特徴として使用します。利益をモデルの出力として使用します」です。<br>この問題では、MLモデルの設計を考える上での入力特徴の選択と出力ターゲットの選択について求められています。出力として予測が必要なのは"各地域での収益性"であり、重要な入力特徴となる可能性があるのは"製品タイプ、製品販売量、経費、利益"です。これらの情報を基に、あなたが選択するモデルの入力特徴および出力ターゲットが視覚的・数学的に理にかなっているか、効率的であるかを判断します。また、地域性を考慮した特徴量の選択も念頭に置いておくことが重要です。<br>基本的な概念や原則：<br>特徴量：機械学習モデルの学習や予測に使用される入力データの属性や特性です。この問題では、製品タイプや地理的位置情報（緯度と経度）が特徴量です。<br>特徴の交差（特徴クロッシング）：２つ以上の特徴を組み合わせて新たな特徴を生成することです。この問題では、緯度と経度を交差させて地理的な特徴を作ることが求められています。<br>ビニング（Binning）：連続値を区間ごとに分けてカテゴリカルな値に変換する処理です。この問題では、交差された特徴をビニングすることで更に分析可能な新たな特徴を作ります。<br>機械学習モデルの出力：機械学習モデルが予測するターゲット変数です。この問題では、利益が出力です。出力はモデルの訓練目標となり、モデルの性能を評価する基準ともなります。<br>正解についての説明：<br>（選択肢）<br>・製品タイプと、緯度と経度の特徴の交差、その後のビニングを特徴として使用します。利益をモデルの出力として使用します<br>この選択肢が正解の理由は以下の通りです。<br>まず、製品タイプは収益性を予測する際の重要な因子となるため、これを特徴として採用することは合理的です。製品の種類によって、その収益性は大きく異なる可能性がありますからです。<br>また、地域性を示すために緯度と経度の交差特徴を使用することも推奨されます。これにより、地理的なパターンをモデルが学習できるようになります。それぞれの地域での収益性を予測するためには、地域情報を正確にモデルに反映させることが重要です。ビニングはこれらの広範な地理情報をカテゴリ化する手法であり、モデルの学習をさらに向上させます。<br>そして、利益をモデルの出力として使用する理由は、利益こそが収益性を直接示す指標であり、最終的な目標変数となるためです。<br>不正解の選択肢についての説明：<br>選択肢：緯度、経度、製品タイプを特徴として使用します。利益をモデルの出力として使用します<br>この選択肢が正しくない理由は以下の通りです。<br>単純に緯度、経度、製品タイプを特徴として使用するだけでは、地理的位置と製品タイプの関連性を学習することが難しいでしょう。製品タイプと地域の交差特徴がなければ、地域性を反映した製品収益性の予測は難しくなります。そのため、交差特徴とビニングは必要となるため、この選択肢は誤りです。<br>選択肢：緯度、経度、製品タイプを特徴として使用します。収益と支出をモデルの出力として使用します<br>この選択肢が正しくない理由は以下の通りです。<br>まず、この選択肢では、モデルの出力として収益と支出を使用する提案がされていますが、これら二つの値は別々の予測が必要になり、一つのモデルで同時に予測することは困難です。<br>一方、正解の選択肢は利益だけを出力としています。これは一つの出力を予測する方が現実的であるためです。<br>選択肢：製品タイプと、緯度と経度の特徴の交差、その後のビニングを特徴として使用します。収益と支出をモデルの出力として使用します<br>この選択肢が正しくない理由は以下の通りです。<br>各地域での収益性を予測する目的から、支出をモデルの出力として使用することは必要ないです。その代わり、収益性を直接表す&#39;利益&#39;を出力とするべきです。果たして、収益と支出両方を出力にすると、関連性の度合いや各地域ごとの影響差が捉えにくくなります。'>
<div class='choice'> 緯度、経度、製品タイプを特徴として使用します。収益と支出をモデルの出力として使用します</div>
<div class='choice'> 緯度、経度、製品タイプを特徴として使用します。利益をモデルの出力として使用します</div>
<div class='choice'> 製品タイプと、緯度と経度の特徴の交差、その後のビニングを特徴として使用します。利益をモデルの出力として使用します</div>
<div class='choice'> 製品タイプと、緯度と経度の特徴の交差、その後のビニングを特徴として使用します。収益と支出をモデルの出力として使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題12<br>あなたは、分類されたスキャン文書の画像に会社のロゴが含まれているかどうかを検出する二値分類MLアルゴリズムに取り組んでいます。データセットでは、96%の例にロゴがないので、データセットは非常に歪んでいます。<br>どのメトリクスがあなたのモデルに最も信頼性を与えますか？' data-answer='2' data-explanation='解説<br>正解は「リコールが精度よりも重視される重み付きF値」です。<br>この問題では、不均衡なデータセットに基づく機械学習モデルの性能評価メトリクスを選ぶことが重要です。ロゴの存在が少数派である不均衡なデータセットを扱っているため、モデルが大多数の"ロゴなし"の例を正しく分類するだけで高い精度が得られてしまう、という問題が生じる可能性があります。問題文で重視されることが示されたメトリクスに焦点を当て、そのメトリクスが不均衡なデータに対するモデルの性能を適切に反映できるかどうかを考えることが必要です。<br>基本的な概念や原則：<br>F値：精度と再現率の調和平均を求めるメトリクスです。分類問題においてFPとFNのバランスを評価します。<br>精度（Precision）：真陽性（真に正で予測が正しい）の比率です。偽陽性（実際は負で予測が正）を小さくすることを重視します。<br>再現率（Recall; リコール）：正であるすべてのケースの中で正しく予測できた比率です。偽陰性（実際は正で予測が負）を小さくすることを重視します。<br>F1値：精度と再現率が等しく重視される場合のF値です。<br>RMSE（Root Mean Squared Error）：実際の値と予測値の差の二乗平均の平方根。連続的な予測値が適用される回帰問題でよく使われます。<br>不均衡なデータセット：一部のクラスの出現率が他のクラスよりも高いデータセット。適切な評価指標を選択しないと、一部のクラスが過度に影響を与える可能性があります。<br>正解についての説明：<br>（選択肢）<br>・リコールが精度よりも重視される重み付きF値<br>この選択肢が正解の理由は以下の通りです。<br>まず、問題のシナリオである二値分類において、データセットが大きく偏っている（このケースではロゴがない方が圧倒的に多い）場合、精度（正確性）だけを指標とすると、全ての入力に対してロゴがないと予測しても非常に高い精度が得られてしまいます。<br>しかし、それではロゴがある場合を見逃してしまう可能性があるため、このような偏ったデータに対しては精度だけでなく、リコール（再現率）も考慮することが重要です。リコールは、実際にロゴがあるものの中で、どれだけをロゴがあると正確に予測できたかを示す指標です。そこでF値が重要になります。F値は精度とリコールの調和平均を取って一つの指標にしたもので、バランス良く精度とリコールを評価できます。以上から、非常に偏ったデータセットに対するモデルの評価には、リコールが精度よりも重視される重み付きF値が最も信頼性を与えるメトリクスとなります。<br>不正解の選択肢についての説明：<br>選択肢：RMSE<br>この選択肢が正しくない理由は以下の通りです。<br>RMSE（Root Mean Square Error）は回帰分析に用いられる誤差の二乗の平均の平方根を示す指標で、二値分類においては用いられません。プロジェクトが歪んでいるバイナリ分類問題のため、精度よりもリコールを重視するF値が適しています。<br>選択肢：F1値<br>この選択肢が正しくない理由は以下の通りです。<br>F1値は精度とリコールの調和平均を求めるメトリクスであり、両方のバランスをとるものです。<br>しかし、問題ではロゴの存在を見逃さないことが重要なため、リコールがより重視される重み付きF値、つまりRecallを重視したF-betaスコアを用いるべきです。<br>選択肢：精度が再現率よりも重視される重み付きF値<br>この選択肢が正しくない理由は以下の通りです。<br>分類問題において、データに偏りがある場合、精度だけでモデルの性能を評価すると誤った結果を得る可能性があります。再現率は、実際の正例のうちどれだけ検出できたかを示すため、偏ったデータセットでロゴの存在を確認する場合、再現率が精度より重視されるべきです。'>
<div class='choice'> RMSE</div>
<div class='choice'> 精度が再現率よりも重視される重み付きF値</div>
<div class='choice'> リコールが精度よりも重視される重み付きF値</div>
<div class='choice'> F1値</div>
</div>

<div class='question' data-multiple='false' data-question='問題13<br>あなたの会社は、多くの異なるオンラインソースからニュース記事を集約し、ユーザーに送信するアプリケーションを管理しています。あなたは、読者が現在読んでいる記事に似た記事を読者に提案する推薦モデルを構築する必要があります。<br>どのアプローチを使うべきですか？' data-answer='1' data-explanation='解説<br>正解は「すべての記事をword2vecを使ってベクトルにエンコードし、ベクトルの類似性に基づいて記事を返すモデルを構築します」です。<br>この問題では、異なるオンラインソースから集約したニュース記事を基に推薦モデルを構築するという要件が提示されています。読者が現在読んでいる記事に似た記事を提案するというニーズを満足するために、テキストデータの表現やマッチング方法について深く理解することが求められます。また、提案された選択肢が具体的な技術やアプローチを示しているため、それぞれの技術の特性や適用範囲を理解することで、もっとも適切な選択肢を選ぶことができます。<br>基本的な概念や原則：<br>word2vec：自然言語処理（NLP）で使用されるアルゴリズムで、テキストの単語をベクトル表現に変換します。これにより、単語間の意味上の関係性を数学的に表現できます。<br>ベクトルの類似性：ベクトル空間内の2つのベクトルがどれだけ似ているかを測る方法です。NLPでは、単語ベクトル間の類似性を測るためによく使用されます。<br>協調フィルタリング：レコメンダシステムでよく使用されるアプローチで、ユーザーの過去の行動や嗜好に基づいてアイテムを推薦します。<br>ロジスティック回帰：確率を予測するための統計的モデルで、二項分類問題によく使用されます。<br>SVM（サポートベクターマシン）：データを二分する境界線（または超平面）を見つけ出すための教師あり学習モデルです。高次元のデータに対しても分類が可能で、ラベル付きデータを学習します。<br>正解についての説明：<br>（選択肢）<br>・すべての記事をword2vecを使ってベクトルにエンコードし、ベクトルの類似性に基づいて記事を返すモデルを構築します<br>この選択肢が正解の理由は以下の通りです。<br>まず、word2vecは、テキストデータを高次元のベクトル表現に変換する機械学習モデルです。この表現により、テキスト中の単語や文章の意味的な関係性を捉えることが可能になります。具体的には、似たような意味を持つ単語や文章は、ベクトル空間上で互いに近い位置にマップされます。<br>したがって、word2vecを用いてニュース記事をベクトルにエンコードすれば、それぞれの記事の間の類似性を数値化できます。<br>その上で、この数値化された類似性を基に記事を推薦するモデルを構築することで、ユーザーが現在読んでいる記事に近い意味合いを持つ記事を効率良く提案することができます。この手法は、大量のテキストデータから複雑なパターンを発見し、ユーザー体験を個別に最適化するために、有用です。<br>したがって、与えられたシナリオに対して、word2vecを使いベクトルの類似性に基づく推薦モデルを構築するというアプローチは適切であると言えます。<br>不正解の選択肢についての説明：<br>選択肢：ユーザーの過去の行動に基づいて、ユーザーに記事を推薦する協調フィルタリングシステムを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>協調フィルタリングシステムはユーザーの過去の行動に基づいて記事を推薦しますが、問題の要件は"読者が現在読んでいる記事に似た記事を推薦する"ことを求めており、ユーザーの過去の行動に基づくという考え方は要件と異なるため不適切です。<br>それに対し、word2vecを使えば記事そのものの類似性に基づく推薦が可能です。<br>選択肢：ユーザーに記事を推薦すべきかどうかを予測するロジスティック回帰モデルを各ユーザーについて構築します<br>この選択肢が正しくない理由は以下の通りです。<br>ロジスティック回帰モデルを各ユーザーについて構築するアプローチは、ユーザーが特定の記事を読む確率を予測しますが、ある記事がユーザーにとって魅力的であるという情報だけを利用しています。<br>しかし、この問題では、"似た"記事を提案する推薦モデルが必要であり、記事間の類似性を評価するためのツールが求められています。そのため、word2vecを使用して記事をエンコードし、ベクトルの類似性に基づくモデルの構築が適切です。<br>選択肢：数百の記事を手動でラベル付けし、手動で分類された記事に基づいてSVM分類器を訓練し、追加の記事をそれぞれのカテゴリに分類します<br>この選択肢が正しくない理由は以下の通りです。<br>数百の記事を手動でラベル付けしてSVM分類器を訓練するアプローチは、記事の大量のデータを処理するのに時間と労力がかかるため効率的ではありません。<br>また、このアプローチでは読者が現在読んでいる記事に"似た"記事を提案するという要件を満たしません。<br>一方、word2vecを使って記事をベクトルにエンコードし、ベクトルの類似性に基づいて提案するアプローチは、大量の記事を効率的に扱い、読者の興味に基づいた推薦を可能にします。'>
<div class='choice'> ユーザーの過去の行動に基づいて、ユーザーに記事を推薦する協調フィルタリングシステムを作成します</div>
<div class='choice'> すべての記事をword2vecを使ってベクトルにエンコードし、ベクトルの類似性に基づいて記事を返すモデルを構築します</div>
<div class='choice'> ユーザーに記事を推薦すべきかどうかを予測するロジスティック回帰モデルを各ユーザーについて構築します</div>
<div class='choice'> 数百の記事を手動でラベル付けし、手動で分類された記事に基づいてSVM分類器を訓練し、追加の記事をそれぞれのカテゴリに分類します</div>
</div>

<div class='question' data-multiple='false' data-question='問題14<br>あなたは、時系列データを使った分類問題に取り組み始め、ほんの数回の実験で、訓練データに対する受信者動作特性曲線下面積（AUC ROC）値99％を達成しました。あなたは、洗練されたアルゴリズムを使用したり、ハイパーパラメータのチューニングに時間をかけたりしていません。<br>問題を特定し、解決するために、次のステップは何をすべきですか？' data-answer='3' data-explanation='解説<br>正解は「モデルのトレーニング中にネストされた交差検証を適用することで、データリークに対処します」です。<br>この問題では、時系列データを扱う分類問題におけるモデルの性能とその結果についての評価を理解することが求められています。数回の試みでモデルのAUC ROC値が99%を達成しているのに、洗練されたアルゴリズムを使用したりハイパーパラメータのチューニングを行なっていない点に注目し、そこに何か問題が無いかを考える必要があります。高いパフォーマンスが出ているにも関わらず改善の余地を模索するため、なぜそのような結果が出ているのか、何か問題が隠れていないかを見極めることが大切です。<br>基本的な概念や原則：<br>受信者動作特性曲線下面積（AUC ROC）：機械学習モデルのパフォーマンスを評価する一般的な指標です。1に近いほど予測性能が高く、0.5はランダムな予測と同等です。<br>データリーク：モデルの訓練データにテストデータからの情報が含まれてしまう現象です。これにより、モデルがテストデータに対して過度に好成績を収める可能性があります。<br>ネストされた交差検証：モデルの選択とハイパーパラメータの調整を別々に行うための手法です。これにより、データリークを防ぎ、モデルの一般化性能を改善します。<br>オーバーフィッティング：モデルが訓練データに過度に適合し、新しいデータに対する予測性能が低下する現象です。<br>ハイパーパラメータチューニング：モデルの学習プロセスを制御するパラメータの調整です。適切なチューニングはモデルの性能を向上させます。<br>正解についての説明：<br>（選択肢）<br>・モデルのトレーニング中にネストされた交差検証を適用することで、データリークに対処します<br>この選択肢が正解の理由は以下の通りです。<br>まず、非常に高いAUC ROCを得る事は、実際のモデル性能を反映していない可能性があります。短期間で極めて高い性能が達成された場合、それは通常、データリークと呼ばれる問題が存在する兆候です。データリークとは、訓練データがテストデータやバリデーションデータと何らかの形で重複していたり、未来の情報が訓練データに含まれていたりすることで、モデルが訓練データに過度に適用する傾向に陥ることを指します。<br>この問題を解決するために、データリークに対処する手法が必要です。ネストされた交差検証はその一つで、交差検証を二重に行うことで、モデルがハイパーパラメータを過度に学習してしまうのを防ぎます。つまり、ネストされた交差検証は、外側のループでデータセットを訓練セットとテストセットに分割し、内側のループで訓練セットをさらに訓練セットとバリデーションセットに分割します。これによりモデルの汎化性能をより正確に評価することができます。<br>したがって、データリークの問題を解決するためには、ネストされた交差検証を適用するのが効果的です。<br>不正解の選択肢についての説明：<br>選択肢：より複雑でないアルゴリズムを使用することで、モデルのオーバーフィッティングに対処します<br>この選択肢が正しくない理由は以下の通りです。<br>アルゴリズムの複雑さを下げることでオーバーフィッティングに対処するという選択肢は不適切です。これは、問題はオーバーフィッティングではなく、訓練データに対する高いパフォーマンスはデータリークを示唆しています。ネストされた交差検証はデータリークに対処でき、より一般的なモデルのパフォーマンスを算出できます。<br>選択肢：目標値と相関の高い特徴を除去することで、データリークに対処します<br>この選択肢が正しくない理由は以下の通りです。<br>目標値との相関が高い特徴を除去するという方法は、問題の特定や解決に直結しません。<br>逆に、これらの特徴は予測に有用な場合もあります。<br>それに対して、ネストされた交差検証を適用することでデータリークを特定し対処できます。<br>選択肢：AUC ROC値を下げるようにハイパーパラメータをチューニングすることで、モデルのオーバーフィッティングに対処します<br>この選択肢が正しくない理由は以下の通りです。<br>ハイパーパラメータをチューニングしてAUC ROC値を下げることは、モデルの性能を人為的に下げることであり、オーバーフィッティング問題の本質的な解決には繋がりません。<br>それに対して、ネストされた交差検証は、データリークを防ぎ、過学習のリスクを減らす効果的な手法です。'>
<div class='choice'> 目標値と相関の高い特徴を除去することで、データリークに対処します</div>
<div class='choice'> より複雑でないアルゴリズムを使用することで、モデルのオーバーフィッティングに対処します</div>
<div class='choice'> AUC ROC値を下げるようにハイパーパラメータをチューニングすることで、モデルのオーバーフィッティングに対処します</div>
<div class='choice'> モデルのトレーニング中にネストされた交差検証を適用することで、データリークに対処します</div>
</div>

<div class='question' data-multiple='false' data-question='問題15<br>あなたはあるオンライン旅行代理店に勤めています。あなたは、ユーザーが次に見るべき最も関連性の高いウェブバナーを予測するよう依頼されています。あなたの会社にとってセキュリティは重要です。モデルのレイテンシ要件は99パーセンタイルにおいて300msで、インベントリは数千のウェブバナーです。あなたは、最もシンプルなソリューションを導入したいと考えています。<br>予測パイプラインはどのように構成すべきですか？' data-answer='3' data-explanation='解説<br>正解は「ウェブサイトにクライアントを組み込み、App Engineにゲートウェイをデプロイし、Cloud Bigtableにデータベースをデプロイして書き込みとユーザーのナビゲーションコンテキストの読み込みを行い、AI Platform Predictionにモデルをデプロイします」です。<br>この問題では、オンライン旅行代理店のウェブサイトで最も関連性の高いウェブバナーを予測するためのシンプルなソリューションを設計し、実装するための適切なアプローチが要求されています。特にモデルのレイテンシ要件とセキュリティの重要性に焦点を当てて、予測パイプラインの設計を行う必要があります。また、問題文からシステムが数千のウェブバナーから選択することと、これに対応するための適切なデータストレージの選択が重要であることが理解できます。これらの要求事項に基づいて、Google Cloudのサービスを最適でシンプルな方法で組み合わせて予測パイプラインを構成する解答を選択することが求められます。<br>基本的な概念や原則：<br>App Engine：Google CloudのフルマネージドなPaaSサービスです。アプリケーションをデプロイし、自動的にスケーリングしてリクエストを処理します。<br>Cloud Bigtable：大規模で高速なNoSQLデータベースサービスです。大量のデータをリアルタイムで処理する能力を持ち、イベント駆動型のアプリケーションや分析に適しています。<br>AI Platform Prediction：Google CloudのMLモデルをホスティングし予測を提供するサービスです。任意の量のデータに対してスケーラブルな予測を提供し、ユーザーからのリクエストに応じて自動スケーリングします。<br>クライアント組み込み：ウェブサイトやアプリケーションに、ユーザーの行動や選択を追跡するためのコードを埋め込むことです。ここでは、ユーザーのウェブサイト内のナビゲーション状況を追跡しています。<br>ゲートウェイのデプロイ：APIリクエストを受け取り、適切なバックエンドサービスにルーティングするための部分を指します。ここでは、App Engineがこの役割を果たしています。<br>レイテンシ要件：システムがユーザーの要求に応答するまでに許容できる最大時間を指します。ここでは99パーセンタイルで300msという厳しい要件が設定されています。<br>セキュリティ：仕組みやデバイス、ユーザー情報などが不正アクセスや漏洩、改ざんから保護されている状態を指します。ここでは、企業としてセキュリティが重要とされています。<br>正解についての説明：<br>（選択肢）<br>・ウェブサイトにクライアントを組み込み、App Engineにゲートウェイをデプロイし、Cloud Bigtableにデータベースをデプロイして書き込みとユーザーのナビゲーションコンテキストの読み込みを行い、AI Platform Predictionにモデルをデプロイします<br>この選択肢が正解の理由は以下の通りです。<br>まず、ウェブサイトにクライアントを組み込むことで、ユーザーのナビゲーションコンテキストをリアルタイムで捉えます。これにより、可能な限り関連性の高いウェブバナーを提案できます。<br>次に、App Engineにゲートウェイをデプロイすることで、セキュリティやスケーラビリティなどの面で優れた自動管理型のプラットフォームを活用できます。<br>また、Cloud Bigtableをデータベースとして利用することで、大量のバナーデータを高速に書き込み、読み取ることができます。大規模なリアルタイム分析を行う必要があるシナリオにおいて、Bigtableはそのパフォーマンスとスケーラビリティで一際目立ちます。<br>そして、AI Platform Predictionにモデルをデプロイすることで、予測パイプラインをシンプルに保つとともに、マシンラーニングモデルを安定して高速に実行することができます。AI Platform PredictionはGoogle Cloudが提供する予測メカニズムで、高いレイテンシ要件を満たす能力を持っています。<br>これらの組み合わせにより、シンプルな予測パイプラインを導入しつつ、ユーザーに関連性の高いバナーを素早く推薦することができます。<br>不正解の選択肢についての説明：<br>選択肢：ウェブサイトにクライアントを組み込み、AI Platform Predictionにモデルをデプロイします<br>この選択肢が正しくない理由は以下の通りです。<br>モデルをAI Platform Predictionに直接デプロイするだけでは、ユーザーのナビゲーションコンテキストの読み込みや書き込み等が考慮されていません。これらの操作はインベントリデータやユーザーの行動を追跡し、次に表示するウェブバナーを適切に予測するために重要であり、データベースと連携して行う必要があります。<br>選択肢：ウェブサイトにクライアントを組み込み、App Engineにゲートウェイをデプロイし、AI Platform Predictionにモデルをデプロイします<br>この選択肢が正しくない理由は以下の通りです。<br>不正解の選択肢では、ユーザーのナビゲーションコンテキストの書き込みと読み込みを行うためのデータベースシステムが提供されていません。<br>一方、正確な選択肢ではCloud Bigtableがその役割を果たしています。これにより、適切なウェブバナーの予測を行うための必要なコンテキスト情報が利用できます。<br>選択肢：ウェブサイトにクライアントを組み込み、App Engineにゲートウェイをデプロイし、書き込み用とユーザーのナビゲーションコンテキスト読み取り用のデータベースをMemorystoreにデプロイし、Google Kubernetes Engineにモデルをデプロイします<br>この選択肢が正しくない理由は以下の通りです。<br>Memorystoreは主にキャッシュやセッションデータの保存に適したインメモリデータストアであり、数千のウェブバナーという大量のデータを安全に保存するには適していません。<br>また、モデルをGoogle Kubernetes Engineにデプロイすると、管理と運用の複雑さが増すため、"最もシンプルなソリューション"ではありません。'>
<div class='choice'> ウェブサイトにクライアントを組み込み、App Engineにゲートウェイをデプロイし、AI Platform Predictionにモデルをデプロイします</div>
<div class='choice'> ウェブサイトにクライアントを組み込み、AI Platform Predictionにモデルをデプロイします</div>
<div class='choice'> ウェブサイトにクライアントを組み込み、App Engineにゲートウェイをデプロイし、書き込み用とユーザーのナビゲーションコンテキスト読み取り用のデータベースをMemorystoreにデプロイし、Google Kubernetes Engineにモデルをデプロイします</div>
<div class='choice'> ウェブサイトにクライアントを組み込み、App Engineにゲートウェイをデプロイし、Cloud Bigtableにデータベースをデプロイして書き込みとユーザーのナビゲーションコンテキストの読み込みを行い、AI Platform Predictionにモデルをデプロイします</div>
</div>


            <!-- 他の問題も同様に追加 -->
        </div>

        <h2 id="question"></h2>
        <ul class="choices" id="choices"></ul>
        <button onclick="checkAnswer()">採点</button>
        <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
        <div class="result" id="result"></div>
    </div>

    <script>
        let currentQuestionIndex = 0;
        let correctCount = 0;
        const questions = [];

        document.addEventListener('DOMContentLoaded', () => {
            const questionElements = document.querySelectorAll('#quiz-data .question');
            questions.push(...Array.from(questionElements).map(questionElement => ({
                question: questionElement.getAttribute('data-question').replace(/\\n/g, '<br>'),
                choices: Array.from(questionElement.querySelectorAll('.choice')).map((choice, index) => ({
                    text: choice.innerHTML.replace(/\\n/g, '<br>'),  // innerHTMLに変更
                    index: index
                })),
                correctAnswer: questionElement.getAttribute('data-answer').split(',').map(Number),
                explanation: questionElement.getAttribute('data-explanation').replace(/\\n/g, '<br>'),
                multiple: questionElement.getAttribute('data-multiple') === 'true'
            })));
            showQuestion();
        });

        function shuffleArray(array) {
            for (let i = array.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [array[i], array[j]] = [array[j], array[i]];
            }
        }

        function showQuestion() {
            const questionElement = document.getElementById('question');
            const choicesContainer = document.getElementById('choices');
            const currentQuestion = questions[currentQuestionIndex];

            shuffleArray(currentQuestion.choices);

            questionElement.innerHTML = currentQuestion.question;
            choicesContainer.innerHTML = '';

            currentQuestion.choices.forEach((choice, i) => {
                const li = document.createElement('li');
                const input = document.createElement('input');
                const label = document.createElement('label');

                input.type = currentQuestion.multiple ? 'checkbox' : 'radio';
                input.name = 'choice';
                input.value = choice.index;
                input.id = 'choice' + i;

                label.htmlFor = 'choice' + i;
                label.innerHTML = choice.text;  // textContentをinnerHTMLに変更

                li.appendChild(input);
                li.appendChild(label);
                choicesContainer.appendChild(li);
            });

            document.getElementById('result').textContent = "";
            document.getElementById('nextButton').style.display = 'none';
        }

        function checkAnswer() {
            const currentQuestion = questions[currentQuestionIndex];
            const selectedChoices = Array.from(document.querySelectorAll('input[name="choice"]:checked'))
                                        .map(checkbox => parseInt(checkbox.value))
                                        .sort();
            const resultElement = document.getElementById('result');
            
            if (selectedChoices.length > 0) {
                const isCorrect = currentQuestion.multiple
                    ? selectedChoices.toString() === currentQuestion.correctAnswer.sort().toString()
                    : selectedChoices.length === 1 && selectedChoices[0] === currentQuestion.correctAnswer[0];
                
                if (isCorrect) {
                    resultElement.innerHTML = "正解です！<br>" + currentQuestion.explanation;
                    resultElement.style.color = "green";
                    correctCount++; // 正解数をカウント
                } else {
                    resultElement.innerHTML = "残念、不正解です。<br>" + currentQuestion.explanation;
                    resultElement.style.color = "red";
                }
                document.getElementById('nextButton').style.display = 'inline';
            } else {
                resultElement.textContent = "回答を選択してください。";
                resultElement.style.color = "orange";
            }
        }

        function nextQuestion() {
            currentQuestionIndex++;
            
            if (currentQuestionIndex < questions.length) {
                showQuestion();
            } else {
                showFinalResult();
            }
        }

        function showFinalResult() {
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2>問題終了！</h2>
                <p>あなたの正解数は ${correctCount} / ${questions.length} です。</p>
                <button onclick="restartQuiz()">再挑戦する</button>
            `;
        }

        function restartQuiz() {
            correctCount = 0;
            currentQuestionIndex = 0;

            // クイズのUI全体を初期化
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2 id="question"></h2>
                <ul class="choices" id="choices"></ul>
                <button onclick="checkAnswer()">採点</button>
                <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
                <div class="result" id="result"></div>
            `;

            // 初期化後に最初の問題を表示
            showQuestion();
        }        
    </script>
</body>
</html>