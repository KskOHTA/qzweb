<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google Professional Machine Learning Engneer問題集 01</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="quiz-container">
        <div id="quiz-data" style="display: none;">


<div class='question' data-multiple='false' data-question='問題1<br>あなたは、さまざまなオンプレミスのデータマートを横断する統合アナリティクス環境の構築を担当しています。あなたの会社では、サーバー間でデータを統合する際に、データ品質とセキュリティの問題が発生しています。その原因となっているのは、さまざまな分離されたツールや一時的なソリューションの使用です。総作業コストを削減し、繰り返し作業を削減する、フルマネージドのクラウドネイティブなデータ統合サービスが必要です。チームの中には、抽出、変換、ロード（ETL）プロセスを構築するためのコードレスインターフェースを好むメンバーもいます。<br>どのサービスを利用すべきですか？' data-answer='0' data-explanation='解説<br>正解は「Cloud Data Fusion」です。<br>この問題では、オンプレミスのデータマートを統合して、データ品質とセキュリティの問題を解消し、作業コストと繰り返し作業の削減を図るクラウドネイティブなデータ統合サービスを見つけることが求められています。さらに、コードレスインターフェースを利用することを求めているメンバーがいるため、それも考慮に入れる必要があります。これらの要件を満たす適切なサービスを選択肢から選ぶことが求められています。<br>基本的な概念や原則：<br>Cloud Data Fusion：フルマネージド、クラウドネイティブのサービスで、データインテグレーションの作業を簡素化します。データの抽出、変換、ロード（ETL）をビジュアルインターフェースから行うことができます。データ統合や品質管理、セキュリティなどの問題を解消します。<br>データインテグレーション：異なるデータソースからのデータを統合し、統一的なビューを提供するプロセスです。データ品質やデータセキュリティを確保するために重要な作業です。<br>ETL（Extract, Transform, Load）：データウェアハウスにデータをロードするための一般的なプロセスです。データを源システムから抽出（E）、必要に応じて変換（T）、そして対象のデータウェアハウスにロード（L）します。<br>Dataflow：Google Cloudのストリームとバッチデータ処理サービスです。コードを書くことでETLタスクを実施します。<br>Dataprep：Google Cloudのデータサービスで、データを理解し、クリーンアップし、変換するためのツールです。ビジュアルインターフェースを持ちますが、ETLプロセスの全体的な管理は行いません。<br>Apache Flink：ストリーム処理とバッチ処理をサポートするオープンソースのフレームワークです。ETL作業を支援しますが、自己管理が必要です。<br>正解についての説明：<br>（選択肢）<br>・Cloud Data Fusion<br>この選択肢が正解の理由は以下の通りです。<br>まず、Cloud Data Fusionはフルマネージドのクラウドネイティブデータ統合サービスで、データのクエリ、可視化、変換、分析が可能であり、多様なオンプレミスのデータマートとも連携できます。これにより、データが存在する場所を問わずに、標準的な方法でデータを操作でき、高いデータ品質とセキュリティを確保できます。<br>さらに、Cloud Data Fusionは分離されたツールや一時的なソリューションに頼ることなく、コンテンツ全体の統合を行う能力を持つため、コスト削減や繰り返し作業の削減が可能です。<br>また、Cloud Data Fusionはユーザーフレンドリーなコードレスのインターフェイスを備えており、ETLプロセスの構築が容易で、コーディングに頼ることなくデータパイプラインをビジュアルで設計、デプロイ、管理できます。この機能は、コードレスインターフェイスを好むチームメンバーに特に適しています。<br>したがって、これらの要件を満たすために最適なサービスはCloud Data Fusionと言えます。<br>不正解の選択肢についての説明：<br>選択肢：Dataflow<br>この選択肢が正しくない理由は以下の通りです。<br>Dataflowは強力なデータ処理サービスですが、フルマネージドでコードレスなETLプロセスを提供するわけではありません。そのため、コードレスインターフェースを好むメンバーには適していません。<br>一方、Cloud Data Fusionはフルマネージドでコードレスのデータインテグレーションサービスを提供し、ETLプロセスの構築を容易に行えます。<br>選択肢：Dataprep<br>この選択肢が正しくない理由は以下の通りです。<br>Dataprepはデータクレンジングと前処理に特化したツールであり、データ統合に必要な全ての機能を持っていません。<br>一方、Cloud Data FusionはETLプロセスを担当し、コードレスインターフェースを提供するフルマネージドサービスであり、より要件を満たしています。<br>選択肢：Apache Flink<br>この選択肢が正しくない理由は以下の通りです。<br>Apache Flinkはデータ処理エンジンであり、抽出、変換、ロード（ETL）プロセスに対してコードレスなインターフェースを提供することはできません。<br>しかし、Cloud Data Fusionはフルマネージドのクラウドネイティブなデータ統合サービスであり、視覚的なインターフェースを通じてETLプロセスの定義と管理が可能で、コードレスでの作業も可能です。これが最適解です。'>
<div class='choice'> Cloud Data Fusion</div>
<div class='choice'> Apache Flink</div>
<div class='choice'> Dataflow</div>
<div class='choice'> Dataprep</div>
</div>

<div class='question' data-multiple='false' data-question='問題2<br>あなたは、あなたの会社のeコマースウェブサイトの買い物客のためのMLレコメンデーションモデルを設計しています。レコメンデーションAIを使用して、システムを構築し、テストし、デプロイします。<br>ベストプラクティスに従いつつ、収益を増加させるレコメンデーションをどのように開発すべきですか？' data-answer='0' data-explanation='解説<br>正解は「"よく一緒に購入される商品"の推奨タイプを使用して、注文ごとにショッピングカートのサイズを大きくします」です。<br>この問題では、MLレコメンデーションモデルの設計と収益増加という目標を結びつける方法について問われています。選択肢を考慮する際に注意すべきことは、単にレコメンデーションモデルを作成するのではなく、それが具体的にどのように収益増加につながるかを理解することです。また、会社がeコマースウェブサイトを運営していることも考慮に入れ、その状況に適したレコメンデーションモデルが選択肢の中にあるか確認してください。各選択肢がこの特定の状況にどのように適し、収益増加にどう貢献するかを慎重に考える必要があります。<br>基本的な概念や原則：<br>Recommendation AI：ユーザーの行動と製品カタログデータに基づいて個別のユーザー向けにパーソナライズされた商品推奨を生成するGoogle Cloudのサービスです。<br>よく一緒に購入される商品の推奨タイプ：これは一般的な推奨のタイプで、顧客がすでに選択したアイテムと関連性の高い商品を提示します。これにより、一度の注文で販売する商品の数を増やすことができます。<br>その他の商品が好きかもしれませんの推奨タイプ：これはより一般的な推奨のタイプで、顧客が以前に表示または購入した商品と関連性がある可能性のある他の商品を提示します。<br>ユーザーイベント：これらは顧客の行動を記録するために使用され、その後の商品推奨の生成に使用されます。<br>製品カタログ：これは利用可能な商品のリストで、商品推奨の生成に使用されます。<br>プレースホルダー値：これらは一時的に用いられ、具体的な値が利用可能になるまで空白を埋めます。導入段階でのテストやモデルの実行可能性の検証に使われます。<br>正解についての説明：<br>（選択肢）<br>・"よく一緒に購入される商品"の推奨タイプを使用して、注文ごとにショッピングカートのサイズを大きくします<br>この選択肢が正解の理由は以下の通りです。<br>まず、機械学習ベースのレコメンデーションシステムにより、ユーザーが購入したいと思わなかったかもしれない製品を提示することで、注文の平均サイズを大きくし、結果的に収益を増加させることができます。"よく一緒に購入される商品"の推奨タイプを使用すると、ユーザーが自分のショッピングカートに既にあるアイテムに関連する商品を提案することができます。それは相補的な商品かもしれませんし、似たような商品で、ユーザーが気付かなかった選択肢かもしれません。<br>また、このアプローチはユーザー体験を向上させる可能性があります。ユーザーにとって価値のあるレコメンデーションを提供することは、ユーザーサティスファクションを向上させ、リピートビジネスを生む可能性があります。<br>したがって、"よく一緒に購入される商品"の推奨タイプは、ベストプラクティスに従いつつ、収益を増加させるレコメンデーションを開発する適切な手法です。<br>不正解の選択肢についての説明：<br>選択肢："その他の商品が好きかもしれません"という推奨タイプを使用して、クリックスルー率を高めます<br>この選択肢が正しくない理由は以下の通りです。<br>その他の商品が好きかもしれませんの推奨タイプを使用してクリックスルー率を高める方法は訪問者の関心を引くかもしれませんが、収益を直接増加させる方法ではありません。<br>それに対して、"よく一緒に購入される商品"の推奨タイプの方がユーザーが複数の商品を購入する可能性が高まり、注文ごとのショッピングカートのサイズを増やし、収益を直接増加させる効果があります。<br>選択肢：ユーザーイベントをインポートしてから製品カタログをインポートし、最高品質のイベントストリームを確保します<br>この選択肢が正しくない理由は以下の通りです。<br>ユーザーイベントのインポートおよび製品カタログのインポートは、正確なレコメンデーションを提供するために必要なプロセスですが、これらの操作単体ではショッピングカートのサイズを大きくするような具体的な推奨展開戦略にはなりません。<br>一方、"よく一緒に購入される商品"の推奨タイプを用いることで、直接にショッピングカートのサイズを大きくし、収益を増加させることができます。<br>選択肢：製品データの収集と記録には時間がかかるため、製品カタログのプレースホルダー値を使用してモデルの実行可能性をテストします<br>この選択肢が正しくない理由は以下の通りです。<br>製品カタログのプレースホルダー値を使ってモデルの実行可能性をテストするという手法は、本当の製品データを用いて実施すべきテストを切り詰めてしまうため、収益増加を目指すレコメンデーションの品質にマイナス影響を与えます。<br>それに対して、正解の選択肢では"よく一緒に購入される商品"というリアルなデータを用いてレコメンデーションを行い、ショッピングカートのサイズを増やす効果的な策を提案しています。'>
<div class='choice'> "よく一緒に購入される商品"の推奨タイプを使用して、注文ごとにショッピングカートのサイズを大きくします</div>
<div class='choice'> 製品データの収集と記録には時間がかかるため、製品カタログのプレースホルダー値を使用してモデルの実行可能性をテストします</div>
<div class='choice'> ユーザーイベントをインポートしてから製品カタログをインポートし、最高品質のイベントストリームを確保します</div>
<div class='choice'> "その他の商品が好きかもしれません"という推奨タイプを使用して、クリックスルー率を高めます</div>
</div>

<div class='question' data-multiple='false' data-question='問題3<br>あなたは農業研究チームのMLエンジニアで、作物の画像から葉さび病の斑点を検出して病気の存在を判断する作物病害検出ツールの開発に取り組んでいます。これらの斑点は、形状やサイズが異なる場合があり、病気の重症度に相関しています。あなたは、病気の存在と重症度を高精度で予測するソリューションを開発したいと考えています。<br>この要件を満たすために、どうすればよいですか？' data-answer='0' data-explanation='解説<br>正解は「画像セグメンテーションMLモデルを開発し、さび斑点の境界を特定します」です。<br>この問題では、要件を満たすために適切な機械学習（ML）アプローチを選択することが求められています。特に重要なのは、画像からの斑点検出とその形状やサイズに基づいた病気の重症度の予測です。そのため選択肢を検討する際は、単なる病気の有無の判定や特定の箇所の判定だけでなく、形状やサイズを扱うことができる手法を選ぶことが必要です。<br>基本的な概念や原則：<br>画像セグメンテーション：画像の各ピクセルにラベルを割り当てて、特定のオブジェクトや領域の境界を特定するMLタスクです。形状や大きさが異なる特徴を検出するのに適しています。<br>物体検出：画像内の特定のオブジェクトを検出し、その位置をバウンディングボックス（矩形領域）で示すMLタスクです。形状や大きさが一定で、位置情報だけを知りたいときに便利です。<br>コンピュータビジョン：画像やビデオデータから情報を自動的に抽出するための技術です。テンプレートマッチングなど、特定の形状やパターンを検出するためのアルゴリズムを含みます。<br>画像分類：画像全体が何を示しているか（例えば、病気の有無など）を判定するMLタスクです。画像内の特定の部分ではなく、画像全体を見るので、位置や形状の詳細は考慮されません。<br>正解についての説明：<br>（選択肢）<br>・画像セグメンテーションMLモデルを開発し、さび斑点の境界を特定します<br>この選択肢が正解の理由は以下の通りです。<br>まず、画像セグメンテーションMLモデルは、画像内の各ピクセルがどのオブジェクトに属するかを判別するためのものです。つまり、同一のオブジェクトのピクセルをグルーピングすることで、物体の形状やサイズ、配置を詳細に理解することができます。この性質は、異なる形状やサイズのさび斑点を正確に特定する必要がある本問題において有用です。<br>また、さび斑点の境界を特定することで、それらの面積（重症度）を正確に測定できます。これにより、病気の存在だけでなく重症度も高精度で予測することができます。<br>したがって、画像セグメンテーションMLモデルは作物病害検出ツールの開発に最適であり、農業研究チームの要件を満たします。<br>不正解の選択肢についての説明：<br>選択肢：さびの箇所を特定できる物体検出モデルを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>物体検出モデルは一般的に物体の位置を特定するもので、葉さび病の斑点の境界や形状、サイズなどの細かな特性を特定する能力には欠けています。<br>これに対して、画像セグメンテーションMLモデルは画像内の各ピクセルを特定のクラスに割り当てるため、さび斑点の境界を詳細に特定することができます。<br>選択肢：従来のコンピュータビジョンライブラリを使用して、テンプレートマッチングアルゴリズムを開発します<br>この選択肢が正しくない理由は以下の通りです。<br>従来のコンピュータビジョンライブラリを使用したテンプレートマッチングアルゴリズムでは、形状やサイズが異なる斑点の検出や重症度の予測が難しいです。とくに学習を通じて新しいパターンへの対応を追加することは効率的ではありません。一方画像セグメンテーションMLモデルでは、学習を通し特徴を把握し精度の高い予測が可能です。<br>選択肢：病気の有無を予測する画像分類MLモデルを開発します<br>この選択肢が正しくない理由は以下の通りです。<br>病気の有無を予測する画像分類MLモデルは単純に病気か否かを判断するものですが、これらの斑点の形状やサイズを検出し、それが病気の重症度に相関していることを理解するためには、画像内の特定の領域を特定する画像セグメンテーションMLモデルが適しています。'>
<div class='choice'> 画像セグメンテーションMLモデルを開発し、さび斑点の境界を特定します</div>
<div class='choice'> さびの箇所を特定できる物体検出モデルを作成します</div>
<div class='choice'> 病気の有無を予測する画像分類MLモデルを開発します</div>
<div class='choice'> 従来のコンピュータビジョンライブラリを使用して、テンプレートマッチングアルゴリズムを開発します</div>
</div>

<div class='question' data-multiple='false' data-question='問題4<br>あなたは広告会社に勤めており、最新の広告キャンペーンの効果を把握したいと考えています。そのために、500MBのキャンペーンデータをBigQueryにストリーミングしました。テーブルにクエリを実行し、AI Platformノートブックでpandasデータフレームを使用してクエリの結果を操作したいと考えています。<br>この要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「AI Platform NotebooksのBigQueryセルマジックを使ってデータをクエリし、結果をpandasデータフレームとして取り込みます」です。<br>この問題では、BigQueryにストリーミングしたデータをAI Platformノートブックでpandasデータフレームとして操作する最適な方法を尋ねています。問題のキーポイントは、BigQueryのデータを効率的にpandasデータフレームに変換することです。これを行うための最も直接的で効率的な方法を選ぶ必要があります。ファイルのエクスポートやローカルのダウンロードなどの迂回的な手段は時間とリソースを無駄にしてしまう可能性があります。したがって、BigQueryとpandas間でシームレスなデータ操作と変換を可能にする方法を探すことが求められています。<br>基本的な概念や原則：<br>BigQuery：Google Cloudのフルマネージドなビッグデータ分析ツールです。データをリアルタイムにストリーミングし、SQLのような言語を使って即時にデータを分析することができます。<br>AI Platform Notebooks：Google Cloudのインタラクティブなノートブックサービスで、主に機械学習とデータ分析に使用されます。Apache SparkやTensorFlowなどの主要なオープンソースフレームワークをネイティブにサポートしています。<br>BigQueryセルマジック：AI Platform NotebooksでBigQueryを直接クエリできる便利な機能です。この機能を使うと、ノートブックから直接BigQueryにクエリを送信し、結果をpandasのデータフレームに読み込むことができます。<br>CSVエクスポート：BigQueryのテーブルをCSV形式で出力する機能です。ただし、エクスポートやインポートのプロセスは時間がかかる上、データが大量にある場合は取り扱いが面倒です。<br>Pandas：Pythonで使用されるデータ分析ライブラリです。データフレームという形式でデータを扱うことが可能で、CSVファイルやデータベースからデータを読み込んだり、逆にデータを出力する機能もあります。<br>Bashセル：ノートブック環境でシェルコマンドを実行するためのインターフェースです。コマンドラインでの操作を必要とする一部のタスクには便利ですが、PythonやRのようなプログラミング言語に比べて高度なデータ操作が難しい一面もあります。<br>Google Cloud Storage：Google Cloudの安全で高速なオブジェクトストレージサービスで、バイト単位のデータからペタバイトに及ぶデータまで、さまざまな規模のデータを保存できます。<br>正解についての説明：<br>（選択肢）<br>・AI Platform NotebooksのBigQueryセルマジックを使ってデータをクエリし、結果をpandasデータフレームとして取り込みます<br>この選択肢が正解の理由は以下の通りです。<br>AI Platform NotebooksはJupyter Notebook環境を提供しますが、Jupyter NotebookはBigQueryとの統合があり、特にセルマジックと呼ばれる機能を使えば直接BigQueryクエリを実行できます。BigQueryセルマジックを使ってクエリを実行すると、結果は直接Pandasデータフレームにロードされます。Pandasはデータ分析で広く使われるPythonライブラリで、この機能を使えば直接Pythonでデータ操作が可能です。<br>したがって、この選択肢は広告キャンペーンデータをBigQueryにクエリを実行し、その結果をPandasデータフレームで直接操作するという要件を満たしています。<br>不正解の選択肢についての説明：<br>選択肢：BigQueryからテーブルをCSVファイルとしてGoogle Driveにエクスポートし、Google Drive APIを使用してファイルをnotebookインスタンスに取り込みます<br>この選択肢が正しくない理由は以下の通りです。<br>まず、この選択肢は間接的で時間がかかります。BigQueryからGoogle Driveにデータをエクスポートし、その後、Google Drive APIを使用してノートブックに取り込むという手順は冗長です。<br>また、500MBのデータを扱い、即時性が求められる場合には、選択肢が効率的ではありません。<br>一方、正解の選択肢は直接的で、AI Platform NotebooksのBigQueryセルマジックを使用すれば、データを直接クエリし結果をpandasデータフレームとして取り込むことができます。<br>選択肢：BigQueryからローカルCSVファイルとしてテーブルをダウンロードし、AI Platformのノートブックインスタンスにアップロードします。pandas.read_csvを使用して、ファイルをpandasデータフレームとして取り込みます<br>この選択肢が正しくない理由は以下の通りです。<br>BigQueryからローカルCSVファイルをダウンロードし、AI Platformのノートブックインスタンスにアップロードする手間が不必要になります。BigQueryセルマジックを使用すれば、直接pandasデータフレームにデータを読み込むことができるため、より効率的です。<br>選択肢：AI Platformノートブックのbashセルから、bq extractコマンドを使用してテーブルをCSVファイルとしてCloud Storageにエクスポートし、gsutil cpを使用してデータをノートブックにコピーします。pandas.read_csvを使用して、ファイルをpandasデータフレームとして取り込みます<br>この選択肢が正しくない理由は以下の通りです。<br>この方法は非効率的で冗長性があります。BigQueryセルマジックを使えば、直接的にBigQueryからデータをクエリし、pandasデータフレームとして取り込むことができます。bq extractやgsutil cpを介してCloud Storageにデータを一度エクスポートし、それを再度読み込むステップは必要ありません。'>
<div class='choice'> BigQueryからテーブルをCSVファイルとしてGoogle Driveにエクスポートし、Google Drive APIを使用してファイルをnotebookインスタンスに取り込みます</div>
<div class='choice'> AI Platform NotebooksのBigQueryセルマジックを使ってデータをクエリし、結果をpandasデータフレームとして取り込みます</div>
<div class='choice'> BigQueryからローカルCSVファイルとしてテーブルをダウンロードし、AI Platformのノートブックインスタンスにアップロードします。pandas.read_csvを使用して、ファイルをpandasデータフレームとして取り込みます</div>
<div class='choice'> AI Platformノートブックのbashセルから、bq extractコマンドを使用してテーブルをCSVファイルとしてCloud Storageにエクスポートし、gsutil cpを使用してデータをノートブックにコピーします。pandas.read_csvを使用して、ファイルをpandasデータフレームとして取り込みます</div>
</div>

<div class='question' data-multiple='false' data-question='問題5<br>あなたは、BigQueryに格納されている50,000レコードを含むデータセットに基づいて回帰モデルを学習する必要があります。データには、負の値を含む可能性のあるターゲット変数を持つ、合計20のカテゴリおよび数値的特徴が含まれます。モデルのパフォーマンスを最大化しながら、労力とトレーニング時間を最小限に抑える必要があります。<br>この回帰モデルをトレーニングするために、どのようなアプローチを取るべきですか？' data-answer='1' data-explanation='解説<br>正解は「BQML XGBoost回帰を使ってモデルを訓練します」です。<br>この問題では、データ量、特徴の種類、目標の制約（パフォーマンスの最大化と労力・時間の最小化）等に注目する必要があります。また、ターゲット変数に負の値がある可能性があるという条件も注視し、その特性を理解した上で解答することが必要です。選択肢を見る際は、上記の条件を最も適切に満たすアプローチを選びます。特に、労力とトレーニング時間を最小限に抑える手法を選択することがポイントとなります。<br>基本的な概念や原則：<br>BQML XGBoost回帰：BigQueryの機能の一つで、回帰モデルのトレーニングをBigQuery内部で実行することができます。強力な機械学習アルゴリズムであるXGBoostを利用し、データの大規模分析と効率的な予測を可能にします。<br>BigQuery：Google Cloudのフルマネージドな大規模データウェアハウスサービスです。SQLクエリを使用してデータの分析や機械学習モデルのトレーニングができます。<br>TensorFlow：Googleが開発したオープンソースの機械学習ライブラリです。深層学習やニューラルネットワークのモデル構築とトレーニングに利用されます。<br>AutoML Tables：Google Cloudのサービスで、テーブルデータに対する自動機械学習モデリングを可能にします。データの前処理からモデルの最適化までを自動で行います。<br>早期停止：機械学習のトレーニング中に、モデルのパフォーマンスが一定期間改善しない場合にトレーニングを停止する手法です。過学習を防ぎ、トレーニング時間とリソースを節約します。<br>正解についての説明：<br>（選択肢）<br>・BQML XGBoost回帰を使ってモデルを訓練します<br>この選択肢が正解の理由は以下の通りです。<br>BQML（BigQuery Machine Learning）はGoogle Cloudのサービスの一部で、大量のデータに対して機械学習モデルを直接訓練し適用することを可能にします。これによって、データのエクスポートや他のツールへの連携といった手間が省けます。BQMLはXGBoost回帰もサポートしています。XGBoostはカテゴリと数値的特徴を共に扱うことができ、負の値もターゲット変数として扱うことができます。これらは本問の要件をすべて満たしています。<br>さらに、XGBoostは強力な勾配ブースティングアルゴリズムであり、モデルのパフォーマンスを最大化するのに有用です。<br>また、BQMLを使用すればデータをBigQueryから移動させることなくモデル訓練が可能なため、労力と訓練時間を大幅に削減することができます。<br>したがって、この問題に対する最適な解決策はBQML XGBoost回帰を使ってモデルを訓練することです。<br>不正解の選択肢についての説明：<br>選択肢：カスタムTensorFlow DNNモデルを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>カスタムTensorFlow DNNモデルを作成する選択肢は、労力とトレーニング時間が大きくなる可能性があるため、問題の条件を満たしません。対してBQML XGBoost回帰を使用すると、BigQuery内で直接モデルを訓練でき、労力とトレーニング時間を最小限に抑えることができます。<br>選択肢：AutoML Tableを使用して、早期に停止することなくモデルをトレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>AutoML Tableは機能豊富でパワフルですが、訓練時間と労力の最小化が要求される場合には最適ではありません。<br>一方、BQML XGBoost回帰はデータセットがBigQuery内に存在する場合に特に効率的であり、必要な労力とトレーニング時間を大幅に軽減します。<br>選択肢：AutoML Tableを使用して、RMSLEを最適化目標としてモデルをトレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>AutoML Tableは非常にパワフルで高度な機能を持っていますが、利用料金が高く、またモデルの訓練時間も長いです。ブロック数が50,000と限られている場合、BQMLのXGBoost回帰を使用することで、トレーニング時間を大幅に節約することができます。<br>また、AutoML Tableの最適化目標としてRMSLEを使用することは可能ですが、訓練に時間とコストがかかるため、労力とトレーニング時間を最小限に抑えるという要件を満たすには最適な選択ではありません。'>
<div class='choice'> AutoML Tableを使用して、RMSLEを最適化目標としてモデルをトレーニングします</div>
<div class='choice'> BQML XGBoost回帰を使ってモデルを訓練します</div>
<div class='choice'> AutoML Tableを使用して、早期に停止することなくモデルをトレーニングします</div>
<div class='choice'> カスタムTensorFlow DNNモデルを作成します</div>
</div>

<div class='question' data-multiple='false' data-question='問題6<br>コードを書くことなく、カスタムカテゴリを持つカスタマーレビューのセンチメントを予測するモデルを迅速に構築し、トレーニングする必要があります。またあなたは、ゼロからモデルをトレーニングするのに十分なデータがありません。一方で、結果として得られるモデルは、高い予測性能を持つ必要があります。<br>要件を満たすために、どのサービスを使うべきですか？' data-answer='2' data-explanation='解説<br>正解は「AutoML Natural Language」です。<br>この問題では、コードを書くことなく、高い予測性能を持つカスタムカテゴリのセンチメント予測モデルを作るためのサービス選択を問われています。更に大切な点として、限られたデータからモデルを学習する必要があることが示されています。選択肢を見る際には、各サービスが持つ能力や特性、そしてその適用可能性に着目するべきです。具体的には、コード作成を必要としない、トレーニングデータが少ない中でもパフォーマンスが保証される、カスタムカテゴリに対応が可能なニーズを満たすサービスを探すことが重要です。<br>基本的な概念や原則：<br>AutoML Natural Language：Google Cloudのサービスで、高度な機械学習モデルをコーディングなしで作成・トレーニングすることができます。カスタムテキスト分類、センチメント分析、エンティティ解析が可能です。<br>Cloud Natural Language API：Google Cloudのサービスで、テキストからエンティティ、センチメント、その他の言語特性を理解することができます。ただし、カスタムカテゴリに対するセンチメント予測を行う新規モデルの構築やトレーニングはサポートしません。<br>AI Hub：Google Cloudのプラットフォームで、AIプロジェクトの開発を加速するためのツールやリソースが提供されています。ただし、自身でモデルをゼロからトレーニングするのではなく、共有・再利用可能なAIコンポーネントを検索して使用することが主な目的です。<br>AI Platform Training：Google Cloudのサービスで、機械学習モデルのトレーニングとデプロイを行うことができます。ただし、使用するには既存のデータと特定のアルゴリズムが必要であり、ゼロからモデルをトレーニングするためのデータが不足している状況では不適切です。<br>正解についての説明：<br>（選択肢）<br>・AutoML Natural Language<br>この選択肢が正解の理由は以下の通りです。<br>まず、AutoML Natural Languageはカスタムしたカテゴリ設定やセンチメント分析のためのモデルを構築するという要件にマッチします。<br>また、このサービスはユーザーがコードを書くことなく機能を利用できるので、コードを書くことなくモデルを構築するという課題をクリアします。<br>次に、AutoMLは事前トレーニングされたモデルにユーザーのデータを追加してファインチューニングすることで、自前のデータが少なくても高い予測性能をもつモデルを作成できるという特性があります。このため、自前で十分なデータを持っていないという状況でも、AutoML Natural Languageは高精度な予測モデルを提供することができ、要件を満たす適切な選択肢です。<br>不正解の選択肢についての説明：<br>選択肢：Cloud Natural Language API<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud Natural Language APIは予測結果を生成するプレビルトのサービスであり、カスタムカテゴリを持つカスタムモデルのトレーニングはサポートしていません。<br>一方、AutoML Natural Languageではカスタムカテゴリのラベリングによりデータを学習し、高い予測性能を持つモデルを構築し、トレーニングすることができます。<br>選択肢：AI Hubが作成したJupyterノートブック<br>この選択肢が正しくない理由は以下の通りです。<br>AI Hubが作成したJupyterノートブックを使用すると、手作業でのコーディングやモデルトレーニングが必要な場合があります。これは"コードを書くことなく"迅速に"モデルを構築するという要件に反します。<br>一方、AutoML Natural Languageは、コードを書かずに既存のデータから高予測性能のカスタムカテゴリを持つセンチメント分析モデルを作成することができます。<br>選択肢：AI Platform Trainingの組み込みアルゴリズム<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platform Trainingの組み込みアルゴリズムは、モデルをゼロからトレーニングする必要がありますが、問題文では十分なデータが無いと記述されています。<br>一方、AutoML Natural Languageは十分なデータが無くてもプリトレーニングモデルをベースにカスタムカテゴリ予測を行えます。'>
<div class='choice'> AI Platform Trainingの組み込みアルゴリズム</div>
<div class='choice'> Cloud Natural Language API</div>
<div class='choice'> AutoML Natural Language</div>
<div class='choice'> AI Hubが作成したJupyterノートブック</div>
</div>

<div class='question' data-multiple='false' data-question='問題7<br>あなたは、ソーシャルメディア企業でMLエンジニアとして働いており、ユーザーのプロフィール写真のビジュアルフィルターを開発しています。そのためには、人の顔の周りのバウンディングボックスを検出するMLモデルを訓練する必要があります。あなたはこのフィルタを会社のiOSベースの携帯電話アプリケーションで使いたいと考えています。コード開発を最小限に抑え、携帯電話での推論に最適化されたモデルにしたいと考えています。<br>この要件を満たすために、どうすればよいですか？' data-answer='3' data-explanation='解説<br>正解は「AutoML Visionを使用してモデルをトレーニングし、"Core ML用にエクスポート"オプションを使用します」です。<br>この問題では、人の顔を検出するMLモデルを訓練し、iOSデバイスで動作するアプリケーションとして最適化する方法が求められています。開発の効率化と携帯電話での推論最適化を重視しています。したがって、コード開発を最小化し、iOS対応を考慮する必要があります。それぞれの選択肢で提案された"AutoML Vision"、"Core ML用にエクスポート"、"Coral用にエクスポート"、"TensorFlow.js用にエクスポート"、"TensorFlow Liteの変換"などのオプションについて、それらがiOSベースの携帯電話アプリケーションでの推論に最適かどうか、また、コーディングの量をどの程度削減できるかを判断して答えを出します。<br>基本的な概念や原則：<br>AutoML Vision：Google Cloudの機械学習ツールの一つで、画像認識モデルを自動で訓練することができます。ユーザーは自身のデータセットをアップロードし、モデルのトレーニングと評価を行うことができます。<br>Core ML：Appleの機械学習フレームワークです。iOSデバイス上で機械学習モデルを使用するためのツールです。<br>Coral：Googleが開発したエッジデバイス用のAI Platformです。しかし、iOSアプリ開発には直接利用できません。<br>TensorFlow.js：ウェブブラウザやNode.js環境で動作するJavaScript用のTensorFlowライブラリです。しかし、iOSの携帯電話アプリケーションには直接適用しづらいです。<br>TensorFlow Lite（TFLite）：移動性のあるデバイスや組み込みデバイス向けの軽量機械学習ライブラリです。しかし、カスタムTensorFlowモデルの訓練とTFLiteへの変換はコード開発の手間がかかります。<br>正解についての説明：<br>（選択肢）<br>・AutoML Visionを使用してモデルをトレーニングし、"Core ML用にエクスポート"オプションを使用します<br>この選択肢が正解の理由は以下の通りです。<br>まず、AutoML Visionを使用することで、特別な機械学習の調整を必要とせずに、顔のバウンディングボックスを検出するMLモデルを簡単に訓練することができます。これは顔認識際のタスクに特化したモデルを提供しているため、高度なMLエキスパートのスキルがなくても優れた性能を発揮します。<br>さらに、エクスポートオプションの一つに"Core ML用にエクスポート"というものがあります。Core MLはAppleが提供している機械学習フレームワークであり、iOSアプリ開発者がネイティブの速度で機械学習モデルを実装するためのものです。このオプションを利用することで、携帯電話での推論に最適化されたモデルを効率的に生成できます。<br>したがって、コード開発を最小限に抑えながら、iOSベースの携帯電話アプリケーションでのユーザ体験を最適化するためには、"AutoML Visionを使用してモデルをトレーニングし、"Core ML用にエクスポート"オプションを使用します"とするのが最良の選択です。<br>不正解の選択肢についての説明：<br>選択肢：AutoML Visionを使用してモデルをトレーニングし、"Coral用にエクスポート"オプションを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>Coral用にエクスポートは、GoogleのCoralデバイスでモデルを実行するためのものであり、iOSアプリケーションで使用するためのものではありません。<br>一方、"Core ML用にエクスポート"はiOSデバイスでの実行が最適化されており、問題で求められている要件を満たします。<br>選択肢：AutoML Visionを使用してモデルをトレーニングし、"export for TensorFlow.js"オプションを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>iOSベースの携帯電話アプリケーションで使用するためには、"Core ML用にエクスポート"オプションを選択するのが適切です。"export for TensorFlow.js"オプションはWebブラウザでの実行を容易にすることを目的としており、iOSベースのアプリケーションには最適化されていません。<br>選択肢：カスタムTensorFlowモデルをトレーニングし、TensorFlow Lite（TFLite）に変換します<br>この選択肢が正しくない理由は以下の通りです。<br>問題の要件はコードの開発を最小限に抑え、携帯電話で推論を最適化することですが、カスタムTensorFlowモデルをトレーニングとTFLiteに変換するオプションは開発工数が大きくなります。<br>一方、AutoML Visionを使用すれば、コード開発を最小限に抑え、iOS対応の"Core ML用にエクスポート"オプションを利用できます。'>
<div class='choice'> カスタムTensorFlowモデルをトレーニングし、TensorFlow Lite（TFLite）に変換します</div>
<div class='choice'> AutoML Visionを使用してモデルをトレーニングし、"Coral用にエクスポート"オプションを使用します</div>
<div class='choice'> AutoML Visionを使用してモデルをトレーニングし、"export for TensorFlow.js"オプションを使用します</div>
<div class='choice'> AutoML Visionを使用してモデルをトレーニングし、"Core ML用にエクスポート"オプションを使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題8<br>実稼働中の需要予測パイプラインでは、モデルのトレーニングと予測の前に、Dataflowを使用して未加工データを前処理しています。前処理では、BigQueryに保存されたデータに対してZスコア正規化を行い、BigQueryに書き戻します。新しいトレーニングデータは毎週追加されます。計算時間と手動介入を最小限に抑えることで、プロセスをより効率化したいと考えています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='2' data-explanation='解説<br>正解は「正規化アルゴリズムをBigQueryで使用するためのSQLに変換します」です。<br>この問題では、実稼働の需要予測パイプラインがあり、オリジナルデータの前処理が必要で、各週に新しいトレーニングデータが追加される、という状況では何をすべきかを問われています。既存のプロセスはDataflowを使い、BigQueryのデータにZスコア正規化を行い、BigQueryに書き戻しています。最小限の計算時間と手動介入で効率化を図りたいという要件が出されています。そのため、選択肢を判断する際には、計算時間と手動介入を最小化するための解決策を選びます。<br>基本的な概念や原則：<br>BigQuery：Google Cloudのフルマネージドなデータウェアハウスサービスです。大量のデータに対する分析クエリの実行が可能で、データ前処理も取り扱えます。<br>Dataflow：Google Cloudのデータ処理サービスです。バッチとストリームの両方のデータを一貫性を保って処理することができます。<br>Zスコア正規化：データの値を平均値からの標準偏差の数値で評価することで、異なる範囲の値や単位を比較可能にする処理方法です。<br>SQL：データベースから情報を取得、操作、更新するための標準的な言語です。BigQueryはSQLを使用してデータを操作します。<br>Google Kubernetes Engine：Google Cloud上でコンテナ化されたアプリケーションを実行するためのマネージドサービスです。データ処理に使用することも可能ですが、BigQueryに直接SQLを使用する方が効率的です。<br>TensorFlow Feature Column API：TensorFlowのFeature Column APIは、エンティティの特性（フィーチャ）をエンコードするためのAPIです。ここでの選択肢としては不適切です。<br>BigQuery用Dataprocコネクタ：BigQueryとApache Sparkの間でデータを移動させるための接続を提供します。しかし、データを移動するコストと時間を考えると、直接BigQuery上でSQLを使用した方が効率的です。<br>正解についての説明：<br>（選択肢）<br>・正規化アルゴリズムをBigQueryで使用するためのSQLに変換します<br>この選択肢が正解の理由は以下の通りです。<br>まず、Dataflowは大量のデータを抽出、変換、ロードなどの処理を行うためのサービスですが、この問題は前処理が主にBigQuery内での正規化であることから、その処理をBigQuery内で完結させるべきです。なぜならBigQueryは、SQLを使用した大規模なデータの操作と分析に優れた性能を発揮しますからです。<br>次に、確かにDataflowを使用することでデータへの正規化処理を行うことは可能ですが、それはより複雑なデータ処理や、BigQueryだけでは対応できないデータソースに対しての処理の場合に適しています。<br>しかし、この場合、データはすでにBigQueryに存在し、また正規化という操作もSQLで可能なため、無駄なデータ移動を避けて全ての処理をBigQuery内で行うことがより効率的です。<br>更に、SQLを利用したBigQueryのクエリにより、正規化アルゴリズムを自動的にスケールさせることができます。これにより、毎週の新しいトレーニングデータへの対応も容易になります。これらは、計算時間と手動介入を最小限に抑える要件を満たします。<br>不正解の選択肢についての説明：<br>選択肢：Google Kubernetes Engineを使ってデータを正規化します<br>この選択肢が正しくない理由は以下の通りです。<br>Google Kubernetes Engineを使ってデータを正規化するアプローチは、データ処理に必要な手間や計算時間を大幅に増加させる可能性があります。<br>一方、正規化アルゴリズムをBigQueryで使用するためのSQLに変換する方が、計算時間と手動介入を最小限に抑えるという要件に沿っています。<br>選択肢：TensorFlowのFeature Column APIでnormalizer_fn引数を使用します<br>この選択肢が正しくない理由は以下の通りです。<br>TensorFlowのFeature Column APIでnormalizer_fn引数を使用すると、前処理はモデルの内部で行われます。<br>しかし、BigQueryのデータを前もって正規化したい場合や、計算時間と手動介入を最小化したい場合、これでは不適切です。<br>それに対して、正規化をSQLで記述することで、BigQuery自体が前処理を行い、効率的な処理が可能になります。<br>選択肢：BigQuery用のDataprocコネクタを使用して、Apache Sparkでデータを正規化します<br>この選択肢が正しくない理由は以下の通りです。<br>BigQuery用のDataprocコネクタを使用してApache Sparkでデータを正規化すると、計算時間は増え、かつ必要な操作も複雑になります。<br>一方で、正規化アルゴリズムをBigQueryで使用するためのSQLに変換すると、仲介者なしに直接BigQuery内で処理が可能になり、計算時間を最小限に抑えられます。'>
<div class='choice'> BigQuery用のDataprocコネクタを使用して、Apache Sparkでデータを正規化します</div>
<div class='choice'> Google Kubernetes Engineを使ってデータを正規化します</div>
<div class='choice'> 正規化アルゴリズムをBigQueryで使用するためのSQLに変換します</div>
<div class='choice'> TensorFlowのFeature Column APIでnormalizer_fn引数を使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題9<br>現在BigQueryに格納されている複数の構造化データセットに対して、分類ワークフローを構築する必要があります。分類は何度も実行されるため、コードを書かずに以下のステップを完了させたいと考えています：<br>- 探索的データ分析<br>- 特徴選択<br>- モデル構築<br>- トレーニング<br>- ハイパーパラメータのチューニングと提供<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「分類タスクを実行するためにAutoML Tableを設定します」です。<br>この問題では、BigQueryに格納された構造化データセットに対し、探索的データ分析からハイパーパラメータのチューニングまでを含む分類ワークフローをコードレスに構築する方法について問われています。これを理解するためには、Google Cloudの各サービスが提供する機能とその範囲、特にAutoML TablesやBigQuery ML、AI Platformといった機械学習関連のサービスの使い方を理解しておく必要があります。選択肢を検討する際には、各サービスが手続きの全ステップをカバーできるかどうかを重視してください。<br>基本的な概念や原則：<br>AutoML Tables：Google Cloudのサービスで、大量の構造化データを扱うための自動機械学習モデルを生成します。探索的データ分析、特徴選択、モデル構築、トレーニング、ハイパーパラメータのチューニングといった一連の機械学習ワークフローを自動化します。<br>探索的データ分析：データの主要な特性を理解するための統計的手法です。データのパターンや異常を探すことを目的とします。<br>特徴選択：モデルの性能を高めるために使用される、データセットから最も有用な特徴を選択するプロセスです。<br>モデル構築とトレーニング：機械学習アルゴリズムを使用して、データから学習するモデルを構築し、そのモデルをトレーニングするプロセスです。<br>ハイパーパラメータチューニング：機械学習モデルの性能を最適化するためのプロセスで、モデルの学習プロセスを制御するパラメータを調整します。<br>BigQuery ML：SQLクエリを使用して、BigQuery内部で機械学習モデルを構築とトレーニングをするサービスです。ただし、ハイパーパラメータのチューニングは提供していません。<br>AI Platform：機械学習モデルのビルド、トレーニング、デプロイが一元管理できるプラットフォームです。しかし、コードレスで全ての処理を行うことはできません。<br>正解についての説明：<br>（選択肢）<br>・分類タスクを実行するためにAutoML Tableを設定します<br>この選択肢が正解の理由は以下の通りです。<br>AutoML Tablesは、構造化データセットに対して機械学習モデルを生成し、予測を行うツールです。AutoML Tablesは探索的データ分析、特徴選択、モデル構築、トレーニング、ハイパーパラメータチューニングなど、モデルのライフサイクル全体を管理します。特に、分類タスクにおいて大量のデータセットを効率的に処理し、十分なパフォーマンスを達成することが期待できます。<br>BigQueryとのネイティブな統合により、BigQueryに格納されたデータに直接アクセスし、そのデータを使用してモデルのトレーニングと評価を行うことができます。これにより、データのエクスポートや変換といった余分な工程を省略し、効率的に分類タスクを達成することができます。<br>したがって、特にコードを書くことなく、反復的な分類タスクを行うためにAutoML Tablesを使用することは、効率的で賢明な選択です。<br>不正解の選択肢についての説明：<br>選択肢：BigQuery MLタスクを実行して、分類のためにロジスティック回帰を実行します<br>この選択肢が正しくない理由は以下の通りです。<br>BigQuery MLを利用したロジスティック回帰は、分類ワークフローを実行できますが、探索的データ分析、特徴選択、モデル構築、トレーニング、ハイパーパラメータのチューニングと提供の機能を全て含むわけではありません。<br>それに対して、AutoML Tableはこれら全ての機能をコードなしに提供します。<br>選択肢：AI Platformノートブックを使用して、pandasライブラリで分類モデルを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platformノートブックを使用してpandasライブラリで分類モデルを実行すると、探索的データ分析や特徴選択など全ての工程においてコードの記載が必要です。問題の要件である"コードを書かずに"分類ワークフローを構築するとは反する方法論です。反対にAutoML Tableはコード無しでハイパーパラメータのチューニングやモデルのトレーニングが可能です。<br>選択肢：AI Platformを使用して、ハイパーパラメータチューニング用に設定された分類モデルジョブを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platformを使用してハイパーパラメータチューニング用に設定された分類モデルジョブを実行する方法は、モデルの構築、トレーニング、ハイパーパラメータのチューニングは可能ですが、探索的データ分析や特徴選択に対応していません。<br>それに対して、AutoML Tablesはこれら全てのステップをコードなしで行うことができます。'>
<div class='choice'> AI Platformを使用して、ハイパーパラメータチューニング用に設定された分類モデルジョブを実行します</div>
<div class='choice'> 分類タスクを実行するためにAutoML Tableを設定します</div>
<div class='choice'> AI Platformノートブックを使用して、pandasライブラリで分類モデルを実行します</div>
<div class='choice'> BigQuery MLタスクを実行して、分類のためにロジスティック回帰を実行します</div>
</div>

<div class='question' data-multiple='false' data-question='問題10<br>あなたの会社のマーケティング活動に基づく売上予測のデータセットが与えられました。データは構造化されてBigQueryに格納されており、データアナリストのチームによって慎重に管理されています。あなたは、データの予測能力に関する洞察を提供するレポートを作成する必要があります。あなたは、単純なモデルや多層ニューラルネットワークを含む、異なる洗練されたレベルの複数のMLモデルを実行するよう依頼されました。実験結果を収集する時間は数時間しかありません。<br>このタスクを最も効率的かつセルフサービスで完了するために、どのGoogle Cloudツールを使用すべきですか？' data-answer='2' data-explanation='解説<br>正解は「BigQuery MLを使用して複数の回帰モデルを実行し、そのパフォーマンスを分析します」です。<br>この問題では、効率的に短時間で異なるレベルのマシンラーニングモデルを実行するための最適なGoogle Cloudツールを選ぶことが求められています。問題文から、データはすでにBigQueryに格納されており、数時間で複数のモデルの実験結果を収集することが必要なことがわかります。また、作業はセルフサービスで行うことが求められています。これらの事実を踏まえて、Google Cloudの各ツールが提供する機能とその利便性を考慮し、マシンラーニングモデルのトレーニングと評価に最適なツールを選ぶことが重要です。<br>基本的な概念や原則：<br>BigQuery：Google Cloudの完全マネージドな大規模データウェアハウスで、SQLクエリを用いてデータの分析が可能です。構造化された大量のデータを扱えます。<br>BigQuery ML：BigQuery上で直接機械学習モデルを作成・実行するための機能です。データエクスポートやモデルの訓練といった複雑なプロセスが不要で、SQL-likeコマンドによる操作が可能です。<br>Dataproc：Google Cloudの完全マネージドのHadoopとSpark環境です。大規模なデータ分析やデータ処理を行うことができますが、セットアップや管理に時間がかかる場合があります。<br>Vertex AI Workbench：ユーザーが管理するJupyter Notebook環境で、様々なオープンソースのマシンラーニングライブラリを利用することができます。<br>scikit-learn：Pythonの機械学習ライブラリで、一般的な機械学習アルゴリズムを提供しますが、ユーザがモデル選択やパラメータ調整を自己管理する必要があります。<br>Vertex AI：Google Cloudの統合マシンラーニングプラットフォームです。データ準備からモデルトレーニング、そしてデプロイまでの全てをカバーしますが、カスタムモデルのトレーニングには時間と専門知識が必要です。<br>正解についての説明：<br>（選択肢）<br>・BigQuery MLを使用して複数の回帰モデルを実行し、そのパフォーマンスを分析します<br>この選択肢が正解の理由は以下の通りです。<br>BigQuery MLはGoogle Cloudが提供する、BigQuery内での機械学習モデルの作成、訓練、評価、予測を実現するためのツールです。既存のSQLスキルを活用し、データを移動させることなくBigQuery内でMLモデルを構築できるのが強みです。<br>この問題では、データは既にBigQueryに格納されていて、複数のMLモデルで予測を行う必要があり、時間も限られています。BigQuery MLを使用すると、BigQueryの環境内で直接回帰モデルを作成、訓練、評価し、そのパフォーマンスを分析することができます。これによりデータの移動や他の環境でのモデル訓練などの手間を省くことができ、効率的にタスクを完了できます。<br>また、BigQuery MLはセルフサービス型で使用可能なため、ユーザー自身がリソースを管理せずに機械学習タスクを実行できます。<br>不正解の選択肢についての説明：<br>選択肢：Dataprocを使ってBigQueryからデータを読み込み、SparkMLを使って複数のモデルを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>DataprocとSparkMLを使用すると時間と労力がかかり、セルフサービスの観点からは最適ではありません。<br>また、BigQuery MLを使用すると直接BigQuery内でMLを実行でき、速度と効率が向上します。<br>選択肢：Vertex AI Workbenchのユーザー管理型ノートブックとscikit-learnコードを使用して、さまざまなMLアルゴリズムとパフォーマンスメトリクスを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>Vertex AI Workbenchとscikit-learnコードを使用すると、MLの実装とパフォーマンスメトリクスの計算が自由に行えますが、セットアップやデプロイに時間がかかるため、指定された数時間という短い時間枠での実験結果収集には不適しています。これに対してBigQuery MLは、既存のBigQuery環境内で直接MLモデルを訓練・評価できるため、時間効率が格段に高くなります。<br>選択肢：様々なMLアルゴリズムを備えたBigQueryからデータを読み込み、Vertex AIでカスタムTensorFlowモデルをトレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>Vertex AIではカスタムTensorFlowモデルのトレーニングを行えますが、セットアップに手間がかかり、様々なMLモデルを数時間内に試すのは時間的に厳しいです。<br>一方、BigQuery MLはSQLクエリを使用してBigQuery内で直接モデルをトレーニング・評価するため、より効率的です。'>
<div class='choice'> 様々なMLアルゴリズムを備えたBigQueryからデータを読み込み、Vertex AIでカスタムTensorFlowモデルをトレーニングします</div>
<div class='choice'> Dataprocを使ってBigQueryからデータを読み込み、SparkMLを使って複数のモデルを実行します</div>
<div class='choice'> BigQuery MLを使用して複数の回帰モデルを実行し、そのパフォーマンスを分析します</div>
<div class='choice'> Vertex AI Workbenchのユーザー管理型ノートブックとscikit-learnコードを使用して、さまざまなMLアルゴリズムとパフォーマンスメトリクスを実行します</div>
</div>

<div class='question' data-multiple='false' data-question='問題11<br>あなたは、需要が大幅に増加している玩具メーカーに勤めています。品質管理検査員が製品の欠陥をチェックする時間を短縮するために、MLモデルを構築する必要があります。より迅速な欠陥検出が優先事項です。工場には信頼できるWi-Fiがありません。あなたの会社は、新しいMLモデルをできるだけ早く実装したいと考えています。<br>どのモデルを使用すべきですか？' data-answer='2' data-explanation='解説<br>正解は「AutoML Vision Edgeモバイル低レイテンシ-1モデル」です。<br>この問題では、モデル選びの優先事項と現場の制限を理解し、それを基に適切なモデルを選択することが求められています。一つは迅速な欠陥検出が求められることで、これは低レイテンシ（遅延）モデルが求められていることを示しています。もう一つは現場で信頼性のあるWi-Fiが存在しないという状況です。これはモデルがオンライン接続に頼らず、エッジ（現場）で動作する必要性を示しています。また、新しいMLモデルを迅速に実装したいとの要望もあるので、モデル開発の効率性も考慮する必要があります。<br>基本的な概念や原則：<br>AutoML Vision Edge：自動的に機械学習（ML）モデルを訓練し、画像の認識を行うGoogle Cloudのサービスです。Edgeデバイス用のモデルを訓練し、モバイルデバイスやIoTデバイスなどのエッジ環境で使用することができます。<br>モバイル低レイテンシ-1モデル：AutoML Vision Edgeで生成される、低レイテンシ（遅延時間）が優先されたモデルです。高速な判断が要求される場面に適しています。<br>モバイル高精度-1モデル：AutoML Vision Edgeで生成される、高精度が優先されたモデルです。精密さが求められる場面に適していますが、レイテンシは低レイテンシ-1モデルに比べて高くなります。<br>AutoML Vision：Google Cloudの自動機械学習（AutoML）サービスの一部で、カスタムビジョンモデルを作成します。クラウド上で推論を行っているために、モバイル機器上で動作させる場合はインターネット接続が必要です。<br>モバイル汎用-1モデル：AutoML Vision Edgeで生成される、汎用的なパフォーマンスが得られるモデルです。特定の要件（高速さや精度など）がない場合に利用します。<br>正解についての説明：<br>（選択肢）<br>・AutoML Vision Edgeモバイル低レイテンシ-1モデル<br>この選択肢が正解の理由は以下の通りです。<br>まず、製品欠陥を迅速にチェックするためには、画像認識のような視覚ベースの手法がよく適用されます。そういった場面で特に有効なのが、Google CloudのAutoML Vision Edgeです。AutoML Visionは、カスタマイズした画像認識モデルを構築するためのツールで、これにより品質検査のプロセスを自動化し、劇的に時間を短縮することが可能になります。<br>さらに、"モバイル低レイテンシ-1モデル"を選択することで、Wi-Fiに依存せず、エッジデバイス（品質検査を行う機械の操作パネルなど）で直接動作する高速なMLモデルが手に入ります。これによって、工場内での信号が不安定な状況でも、製品の欠陥検出が迅速に行えます。<br>最後に、AutoML Vision Edgeは、学習データをアップロードし、モデルを自動生成するため、新しいMLモデルを素早く使い始めることができます。これは会社が最速でMLモデルの導入を行いたいと考えている状況に対応しています。<br>不正解の選択肢についての説明：<br>選択肢：AutoML Vision Edgeモバイル高精度-1モデル<br>この選択肢が正しくない理由は以下の通りです。<br>ユースケースにおいて最も優先事項となっているのは、欠陥検出の迅速さであり、それを第一に考えるとAutoML Vision Edgeモバイル高精度-1モデルは最適ではありません。これは、高精度モデルは精度が優れている一方で、レイテンシ（応答速度）が高くなる傾向があります。適切な選択肢は、低レイテンシを重視するAutoML Vision Edgeモバイル低レイテンシ-1モデルと考えます。<br>選択肢：AutoML Visionモデル<br>この選択肢が正しくない理由は以下の通りです。<br>AutoML Visionモデルはインターネット接続が必要で、工場内で信頼できるWi-Fiがない状況とは一致しません。<br>対照的に、AutoML Vision Edgeモバイル低レイテンシ-1モデルはエッジデバイス上で、オフラインでも動作します。これにより信頼性あるWi-Fi接続が得られない状況でも欠陥検出が可能になります。<br>選択肢：AutoML Vision Edgeモバイル汎用-1モデル<br>この選択肢が正しくない理由は以下の通りです。<br>AutoML Vision Edgeモバイル汎用-1モデルは、様々なタスクに対応できる一方で、レイテンシを最小限に抑えるための特化がありません。<br>一方、AutoML Vision Edgeモバイル低レイテンシ-1モデルは、その名の通りレイテンシを最小化することを重視しています。そのため、より迅速な欠陥検出が優先事項であるこのケースには、AutoML Vision Edgeモバイル低レイテンシ-1モデルの方が適しています。'>
<div class='choice'> AutoML Vision Edgeモバイル高精度-1モデル</div>
<div class='choice'> AutoML Vision Edgeモバイル汎用-1モデル</div>
<div class='choice'> AutoML Vision Edgeモバイル低レイテンシ-1モデル</div>
<div class='choice'> AutoML Visionモデル</div>
</div>

<div class='question' data-multiple='false' data-question='問題12<br>書面によるサポートケースの大規模なコーパスがあり、テクニカルサポート、請求サポート、その他の問題の3つのカテゴリに分類できます。今後の書面によるリクエストをいずれかのカテゴリに自動的に分類するサービスを迅速に構築、テスト、デプロイする必要があります。<br>あなたはパイプラインをどのように構成すればよいですか？' data-answer='3' data-explanation='解説<br>正解は「AutoML Natural Languageを使って分類器を構築し、テストします。モデルをREST APIとしてデプロイします」です。<br>この問題では、書面によるサポートケースを自動的に分類するためのシステムを構築する方法について尋ねられています。分類のタスクは自然言語処理の一部であるため、テキスト分析に強いツールやサービスを考えることが必要です。また、問題は"迅速に"システムを構築、テスト、デプロイすることが必要であると指定しているため、短期間で効果的な結果を提供できるマシンラーニングの手法やサービスが適切であると考えられます。そのため選択肢を見る際には、これらの要素を頭に入れておくと良いでしょう。<br>基本的な概念や原則：<br>AutoML Natural Language：機械学習モデルを自動的にトレーニングして自然言語理解を可能にするGoogle Cloudのサービスです。特別なスキルを必要とせずにモデルを訓練し、REST APIでデプロイできます。<br>Cloud Natural Language API：Google Cloudの自然言語処理（NLP）サービスの一部で、テキストのエンティティ抽出、感情分析、構文解析などを行うことができます。しかし、カスタムのテキスト分類を実行する機能は提供していません。<br>BigQuery ML：Google Cloudのビッグデータアナリティクスウェアハウス、BigQuery内で直接機械学習モデルを作成、トレーニング、評価、推論することができます。しかし、高度なテキスト分類タスクには適していません。<br>BERT：深層双方向トランスフォーマー（BERT）は自然言語理解モデルで、高度なテキスト理解を可能にします。ただし、この選択肢は自然言語理解モデルのトレーニングに時間と専門知識を必要とします。<br>Vertex AI：Google Cloudの統合MLOpsプラットフォームで、データサイエンテストとMLエンジニアがモデルを開発、トレーニング、デプロイ、監視するための一元化された場所を提供します。<br>正解についての説明：<br>（選択肢）<br>・AutoML Natural Languageを使って分類器を構築し、テストします。モデルをREST APIとしてデプロイします<br>この選択肢が正解の理由は以下の通りです。<br>まず、AutoML Natural Languageはテキストデータの処理と分析に優れたサービスで、自動的に機械学習モデルを構築して、文書を特定のカテゴリに分類します。既存の書面サポートケースを活用してモデルを学習させることで、新たなケースをテクニカルサポート、請求サポート、その他の問題のいずれかのカテゴリに正確に分類できます。<br>そして、AutoML Natural Languageは学習したモデルをREST APIとしてデプロイできます。これによりアプリケーションから直接、またはCloud Functionsを経由してモデルにアクセスし、新たなサポートケースをリアルタイムで分類することが可能になります。これにより、大規模なサポートケースの分類が自動化され、迅速にデプロイが可能になります。<br>したがって、AutoML Natural Languageを使用して分類器を構築し、テストすることが最も適しています。<br>不正解の選択肢についての説明：<br>選択肢：Cloud Natural Language APIを使用して、受信したケースを分類するためのメタデータを取得します<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud Natural Language APIはテキスト分析ツールであり、受信ケースを自動的にカテゴリ分けすることはできません。<br>ただし、AutoML Natural Languageを使用すれば、カスタム分類器を訓練し直し、特定のカテゴリにリクエストを自動分類することができます。<br>選択肢：BigQuery MLを使用して、受信リクエストを分類するためのロジスティック回帰モデルを構築し、テストします。BigQuery MLを使用して推論を実行します<br>この選択肢が正しくない理由は以下の通りです。<br>BigQuery MLは基本的に表形式のデータで教師あり学習のモデル作成に適しており、特に、テキストデータでの自然言語処理や書面によるリクエストの自動分類には不向きです。<br>一方、AutoML Natural Languageは自然言語処理を行う機能を持ち、テキストデータを分類する際に適しています。<br>選択肢：GoogleのBERT訓練済みモデルを使用してTensorFlowモデルを作成します。分類器を構築してテストし、Vertex AIを使用してモデルをデプロイします<br>この選択肢が正しくない理由は以下の通りです。<br>GoogleのBERT訓練済みモデルを使用してTensorFlowモデルを作成し、Vertex AIでデプロイする選択肢は可能ですが、モデルの作成やデプロイに専門知識や時間が必要です。<br>一方、AutoML Natural Languageは自動的にモデルを訓練、テスト、デプロイするため、迅速さを求めるならこちらが適しています。'>
<div class='choice'> BigQuery MLを使用して、受信リクエストを分類するためのロジスティック回帰モデルを構築し、テストします。BigQuery MLを使用して推論を実行します</div>
<div class='choice'> Cloud Natural Language APIを使用して、受信したケースを分類するためのメタデータを取得します</div>
<div class='choice'> GoogleのBERT訓練済みモデルを使用してTensorFlowモデルを作成します。分類器を構築してテストし、Vertex AIを使用してモデルをデプロイします</div>
<div class='choice'> AutoML Natural Languageを使って分類器を構築し、テストします。モデルをREST APIとしてデプロイします</div>
</div>

<div class='question' data-multiple='false' data-question='問題13<br>あなたはBigQuery MLで、顧客の自社製品購入の可能性を予測する線形回帰モデルを構築しています。モデルは、主要な予測要素として都市名変数を使用します。モデルをトレーニングして提供するためには、データを列で整理する必要があります。予測可能な変数を維持しながら、最小限のコーディングでデータを準備したいと考えています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「Dataprepを使用して、ワンホットエンコーディング方式で都市名カラムを変換し、各都市をバイナリ値のカラムにします」です。<br>この問題では、機械学習モデルのデータ準備において最小限のコーディングでデータを整形し、尚かつ予測可能な変数を維持する方法を探してます。都市名というカテゴリ型の変数をモデルの予測要素として使用すること、そして最小限のコーディングという要請を満たす必要があるため、都市名（カテゴリ変数）をどのように数値化（エンコード）すべきかにフォーカスを当て、選択肢を評価すべきです。<br>基本的な概念や原則：<br>BigQuery ML：Google Cloud上のインフラストラクチャで機械学習モデルを構築し、予測を生成するためのサービスです。SQLのみで毎日の分析に統合することができます。<br>Dataprep：データのクリーニングとトランスフォーメーションを視覚的に行えるフルマネージドのクラウドサービスです。ユーザーはコーディングなしで直感的なインターフェースから繰り返し可能なパイプラインを構築できます。<br>ワンホットエンコーディング：カテゴリ変数を扱う一種の表現方法で、各カテゴリ値を一意の整数にマッピングする代わりに、バイナリベクトルを用います。与えられたカテゴリに対応する要素だけが1で、残りはすべて0です。<br>TensorFlow：Googleの開発したオープンソースの機械学習ライブラリです。深層学習やニューラルネットワークのような高度な機械学習モデルの設計、トレーニング、テストが可能です。<br>Cloud Data Fusion：ビッグデータの統合を容易にする、全面的にマネージドされた、オープンソースのデータインテグレーションサービスです。データパイプラインの設計、データの移行や変換を視覚的に操作できます。<br>正解についての説明：<br>（選択肢）<br>・Dataprepを使用して、ワンホットエンコーディング方式で都市名カラムを変換し、各都市をバイナリ値のカラムにします<br>この選択肢が正解の理由は以下の通りです。<br>まず、ワンホットエンコーディングは、カテゴリー型の変数を機械学習モデルに適した形に変換する一般的な手法です。この手法は、各カテゴリー値をそれぞれ新しいバイナリ列（0または1の値を持つ）に変換します。特に、この問題の都市名のようなカテゴリーデータに対しては最適な方法であり、多クラスのカテゴリーデータを機械学習モデルで扱うための最も簡易的で効率良い方法です。<br>次に、Google Cloud Dataprepは、データをクリーニング、変換、そして準備するための高度なデータ分析サービスで、手順が自動化されており、このような適切なエンコーディングを容易に行うことができます。<br>また、BigQueryと直接連携が可能であり、BigQuery MLで使用するデータの前処理に適しています。<br>したがって、ワンホットエンコーディングとDataprepを組み合わせて使用することは、上記の要件を満たす最適な解決策です。<br>不正解の選択肢についての説明：<br>選択肢：TensorFlowを使用して、語彙リストを持つカテゴリ変数を作成します。語彙ファイルを作成し、モデルの一部としてBigQuery MLにアップロードします<br>この選択肢が正しくない理由は以下の通りです。<br>TensorFlowを使用して語彙リストを作成、BigQuery MLへのアップロードという手順は複雑であり、主にディープラーニングモデルの作成に適した方法です。<br>一方、Dataprepを用いたワンホットエンコーディングは、最小限のコーディングで変数の準備を可能にします。<br>選択肢：BigQueryで都市情報を含むカラムを含まない新しいビューを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>新しいビューを作成して都市情報のカラムを除去すると、主要な予測要因である都市名の情報が失われてしまいます。このアプローチでは予測に必要な変数が維持されません。<br>一方、Dataprepを使ったワンホットエンコーディングならば、都市名というカテゴリカルな変数をバイナリ値のカラムに変換し、予測モデルの入力として活用することができます。<br>選択肢：Cloud Data Fusionを使用して、各都市を1、2、3、4、5とラベル付けされた地域に割り当て、その番号を使ってモデル内の都市を表現します<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud Data Fusionを使用して都市を1から5までのラベル付けした地域に割り当てるアプローチは、都市間の順序関係を暗示しますが、実際にはそうした順序関係が存在しないことから不適切です。<br>一方で、ワンホットエンコーディングでは、それぞれの都市が別々のバイナリ値のカラムに変換され、順序関係なく処理が可能です。'>
<div class='choice'> TensorFlowを使用して、語彙リストを持つカテゴリ変数を作成します。語彙ファイルを作成し、モデルの一部としてBigQuery MLにアップロードします</div>
<div class='choice'> Dataprepを使用して、ワンホットエンコーディング方式で都市名カラムを変換し、各都市をバイナリ値のカラムにします</div>
<div class='choice'> BigQueryで都市情報を含むカラムを含まない新しいビューを作成します</div>
<div class='choice'> Cloud Data Fusionを使用して、各都市を1、2、3、4、5とラベル付けされた地域に割り当て、その番号を使ってモデル内の都市を表現します</div>
</div>

<div class='question' data-multiple='false' data-question='問題14<br>あなたは、ソーシャルメディアアプリケーションのために、ユーザが提出したプロフィール写真が要件を満たしているかどうかを予測するMLモデルを構築する必要があります。アプリケーションは、写真が要件を満たしているかどうかをユーザに通知します。<br>アプリケーションが要件を満たさない写真を誤認しないようにするために、どのようにモデルを構築すればよいですか？' data-answer='2' data-explanation='解説<br>正解は「偽陰性を最小化するために、AutoMLを使用してモデルのリコールを最適化します」です。<br>この問題では、MLモデルの評価指標とモデルの設計について理解が求められています。特に、"要件を満たさない写真を誤認しないようにする"という要求に注意が必要です。これは偽陽性（要件を満たさないのに満たさないと認識するケース）の最小化ではなく、偽陰性（要件を満たさないのに満たすと認識するケース）の最小化に関連します。このため、適切なモデル評価指標としてリコール（偽陰性の最小化）が重要です。<br>基本的な概念や原則：<br>AutoML：機械学習の専門知識があまり必要ないユーザーでも、カスタムMLモデルを構築できるようにするGoogle Cloudのサービスです。<br>リコール：すべてのポジティブな事例のうちモデルがどれだけ正しく予測できたかを示す評価指標です。偽陰性（実際はポジティブだがネガティブと予測したもの）を最小化するために重要です。<br>偽陰性：モデルが誤ってネガティブと予測したポジティブな事例です。この問題では、アプリケーションが要件を満たす写真を誤って要件を満たさないと判断すること例が該当します。<br>F1値：適合率と再現率の調和平均です。偽陽性と偽陰性の間のバランスを取るための評価指標です。<br>Vertex AI Workbench：クラウド上でJupyterLabベースのノートブック環境を提供するサービスです。ユーザーがソースコードのバージョン管理、共有、再現性を維持しながらAIの実験と開発を行うことができます。<br>正解についての説明：<br>（選択肢）<br>・偽陰性を最小化するために、AutoMLを使用してモデルのリコールを最適化します<br>この選択肢が正解の理由は以下の通りです。<br>まず、問題の成り立ちから考えてみます。アプリケーションが要件を満たさない写真を誤認するというのは、すなわち偽陽性（要件を満たさない写真を満たしていると間違える）を避けることです。<br>そして、偽陰性（要件を満たしている写真を満たしていないと誤認する）を最小限に抑えることで、アプリケーションの信頼性と使用者の満足度を向上させることができます。そのため、偽陰性を最小化することは、写真が要件を満たしているかどうかを正確に識別するための鍵です。<br>次に、モデルのリコールを考えてみます。リコールは、要件を満たす写真のうち、モデルが正しく識別できる割合を表しています。<br>したがって、リコールを最適化することで、偽陰性を最小化することが可能になります。<br>最後に、AutoMLを使用することで、モデルのリコールを簡単に最適化することができます。AutoMLは、エンドユーザが機械学習のエキスパートである必要がないという大きな利点があります。ユーザは、モデルのトレーニングに使用するデータを提供するだけでよく、AutoMLが最適なモデルを自動的に生成します。これにより、要件を満たさない写真を最小限に抑えながら、高いリコールを達成することが可能になります。<br>不正解の選択肢についての説明：<br>選択肢：偽陽性と偽陰性の精度のバランスをとるために、AutoMLを使ってモデルのF1値を最適化します<br>この選択肢が正しくない理由は以下の通りです。<br>F1値を最適化する方法は偽陽性と偽陰性のバランスを取る場合に適していますが、今回の問題では偽陰性（要件を満たす写真を満たさないと誤認すること）を最小化することが求められています。これは、リコールの最適化で実現できます。<br>選択肢：Vertex AI Workbenchのユーザー管理型ノートブックを使用して、プロフィール写真の要件を満たす写真の例を3倍多く持つカスタムモデルを構築します<br>この選択肢が正しくない理由は以下の通りです。<br>単にプロフィール写真の要件を満たす写真の例を多くするだけでは、偽陰性を最小化するためのモデル最適化は行われません。<br>逆に、選ばれる写真のバランスが崩れる可能性があります。<br>一方、AutoMLを使用することでモデルのリコールを最適化し、偽陰性を最小化することが可能になります。<br>選択肢：Vertex AI Workbenchのユーザー管理型ノートブックを使って、プロフィール写真の条件を満たさない写真の例を3倍多く持つカスタムモデルを構築します<br>この選択肢が正しくない理由は以下の通りです。<br>ただ単に不適合な写真の例を増やすだけでは、モデルの偽陰性を特定して最小化するという問題解決には直接繋がりません。<br>一方、AutoMLを使用してモデルのリコールを最適化することは、偽陰性を最も効果的に減らす方法とされているため、こちらが正しい選択です。'>
<div class='choice'> Vertex AI Workbenchのユーザー管理型ノートブックを使って、プロフィール写真の条件を満たさない写真の例を3倍多く持つカスタムモデルを構築します</div>
<div class='choice'> Vertex AI Workbenchのユーザー管理型ノートブックを使用して、プロフィール写真の要件を満たす写真の例を3倍多く持つカスタムモデルを構築します</div>
<div class='choice'> 偽陰性を最小化するために、AutoMLを使用してモデルのリコールを最適化します</div>
<div class='choice'> 偽陽性と偽陰性の精度のバランスをとるために、AutoMLを使ってモデルのF1値を最適化します</div>
</div>

<div class='question' data-multiple='false' data-question='問題15<br>Kerasでカスタマイズされたディープニューラルネットワークを設計し、購入履歴に基づいて顧客の購入を予測する必要があります。複数のモデルアーキテクチャを使用してモデルのパフォーマンスを調査し、トレーニングデータを保存し、同じダッシュボードで評価指標を比較できるようにしたいと考えています。<br>この要件を満たすために、どうすればよいですか？' data-answer='3' data-explanation='解説<br>正解は「Kubeflow Pipelinesで実験を作成し、複数の実行を整理します」です。<br>この問題では、ディープラーニングのカスタムモデル設計、パフォーマンスの調査、トレーニングデータの保存、そして評価指標の比較能力を必要とする顧客購入予測のタスクについて理解することが求められています。その際、Kerasという特定の深層学習ライブラリと、複数のモデルアーキテクチャに焦点を当てる必要があります。大切なのは、それぞれの選択肢がこれらの要件をどの程度満たしているかを評価し、最も適した解決策を選択することです。<br>基本的な概念や原則：<br>Kubeflow Pipelines：機械学習ワークフローの作成と共有を助けるフレームワークです。評価指標の比較や実験の整理が可能です。複数のモデルアーキテクチャのパフォーマンスを調査するときに便利です。<br>Keras：Pythonで書かれた、ニューラルネットワークモデルを構築するための高水準APIです。簡潔でありながら、深度学習モデルの作成と訓練を柔軟に行うことができます。<br>AutoML Tables：Google Cloudのフルマネージドサービスで、構造化データから自動的に予測モデルを生成できます。しかし、カスタムモデルの設計や探索はサポートしていません。<br>Cloud Composer：Apache Airflowベースのフルマネージドオーケストレーションサービスです。ワークフローのスケジューリングと管理を助けますが、モデル設計の比較やトレーニングデータの保存はサポートしていません。<br>AI Platform：Google Cloudの機械学習モデルのトレーニング、デプロイ、予測を行うプラットフォームです。しかし、複数の実行をまとめて整理する機能や評価指標のダッシュボードが用意されているわけではありません。<br>正解についての説明：<br>（選択肢）<br>・Kubeflow Pipelinesで実験を作成し、複数の実行を整理します<br>この選択肢が正解の理由は以下の通りです。<br>Kubeflow Pipelinesは、機械学習のワークフローを簡素化し、管理するためのオープンソースフレームワークです。そのため、カスタムのディープニューラルネットワークの設計やモデルのパフォーマンスの調査などの機械学習ワークフローを管理に適しています。<br>また、Kubeflow Pipelinesはモデルのトレーニングデータを保存し、評価する機能を提供します。これにより、同じダッシュボードで複数のモデルのパフォーマンスを容易に比較することができます。<br>また、"実験を作成し、複数の実行を整理する"ことは、一つのモデルで異なるパラメータを試したり、異なるモデル間でパフォーマンスを比較するためにも重要です。Kubeflow Pipelinesでは、これらの"実験"は個々の"実行"をサポートし、個々の実行のパフォーマンスと結果を追跡し比較するためのメカニズムを提供します。これは、モデルのパフォーマンスを評価し、最適なモデルを選択するためには欠かせない機能であり、このシナリオの要件に適しているため、この選択肢が正解です。<br>不正解の選択肢についての説明：<br>選択肢：AutoML Tableを使用して複数のモデルを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>AutoML Tablesは自動機械学習をサポートしていますが、カスタムディープニューラルネットワークを設計するためのKerasのような柔軟性は提供しません。<br>また、複数のモデルアーキテクチャを調査し、同じダッシュボードで比較することも難しいです。<br>これに対し、Kubeflow Pipelinesは実験の実行と管理を可能にし、要件を満たします。<br>選択肢：Cloud Composerを使用して複数のトレーニング実行を自動化します<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud Composerはワークフローの自動化とオーケストレーションに用いられますが、複数のモデルアーキテクチャの性能評価や、同一ダッシュボードでの評価指標の比較、トレーニングデータの保存といった要件には対応していません。対してKubeflow Pipelinesは実験管理と評価指標の視覚化を行うことが可能で、これらの要件を満たします。<br>選択肢：AI Platform上で類似したジョブ名で複数のトレーニングジョブを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platform上で類似したジョブ名で複数のトレーニングジョブを実行しても、モデルの評価指標を同じダッシュボードで比較する機能は提供されていません。<br>これに対して、Kubeflow Pipelinesは実験を作成し複数の実行を整理し、同じダッシュボードで評価指標を比較する機能があります。'>
<div class='choice'> Cloud Composerを使用して複数のトレーニング実行を自動化します</div>
<div class='choice'> AutoML Tableを使用して複数のモデルを作成します</div>
<div class='choice'> AI Platform上で類似したジョブ名で複数のトレーニングジョブを実行します</div>
<div class='choice'> Kubeflow Pipelinesで実験を作成し、複数の実行を整理します</div>
</div>

<div class='question' data-multiple='false' data-question='問題16<br>あなたのチームは、あるプラットフォームのサポート要求を分類するために、Google CloudでMLソリューションを作成することを命じられました。あなたは要件を分析し、分類器を構築するためにTensorFlowを使用することを決定しました。MLプラットフォームにはKubeflow Pipelinesを使用します。時間を節約するために、完全に新しいモデルを構築する代わりに、既存のリソースをベースにしてマネージドサービスを使用します。<br>どのように分類器を構築するべきですか？' data-answer='2' data-explanation='解説<br>正解は「AI Platform上で確立されたテキスト分類モデルを使用して、転移学習を行います」です。<br>この問題では、Google CloudでのMLソリューション作成と分類器の構築に焦点を当てています。MLの知識はもちろんですが、具体的にはマネージドサービス、転移学習、TensorFlowの使用など、具体的な技術の理解が問われています。また、選択肢を見ると、Google Cloudの異なるソリューションが提供するサービスの違いを理解する必要があります。言い換えれば、それぞれのサービスが提供する機能とそれが問題の要件にどのように対応するかを理解することが重要です。<br>基本的な概念や原則：<br>TensorFlow：Googleが開発したオープンソースの機械学習フレームワークです。ディープラーニングから自然言語処理まで、幅広い機械学習のタスクに応用可能です。<br>Kubeflow Pipelines：機械学習のワークロードをKubernetes上で効率的に管理するためのツールです。再利用可能なコンポーネントとポータブルパイプラインを提供します。<br>転移学習：既存の学習済みモデルをベースに、新たなタスクに対応する部分だけを追加学習する手法です。新しいモデル作成のコストと時間を節約します。<br>AI Platform：Google Cloudの機械学習モデルの開発、トレーニング、デプロイを一元化するプラットフォームです。もし既存のモデルが提供されていれば、転移学習も可能です。<br>Natural Language API：Google Cloudの自然言語処理用APIです。感情分析、エンティティ分析、シンタックス分析などを提供しますが、特定の分類タスクに再訓練することはサポートされていません。<br>AutoML Natural Language：自然言語分析のタスクに対するモデルを自動的に生成するサービスです。そのままでは一般的なタスクに対するモデルしか生成できませんので、問題の要件に合わせてモデルをカスタマイズする必要があります。<br>正解についての説明：<br>（選択肢）<br>・AI Platform上で確立されたテキスト分類モデルを使用して、転移学習を行います<br>この選択肢が正解の理由は以下の通りです。<br>AI PlatformはGoogle CloudのマネージドMLサービスであり、既存のリソースを活用するために適しています。確立されたテキスト分類モデルを使用するという部分は、新しいモデルの作成時間を節約するために重要です。<br>また、テキスト分類はサポート要求の分類に直結しているため、この選択肢はタスクに対して適切です。<br>最後に、転移学習の使用は、事前に訓練されたモデルを基にして新しいタスクに適用させるための強力な手法であり、これにより、訓練データが少なくてもまたは訓練時間が限られている状況でも良好なパフォーマンスを得ることができます。余分な時間やリソースを割くことなく適切な結果を得るために、AI Platformと転移学習の組み合わせが適していると言えます。<br>不正解の選択肢についての説明：<br>選択肢：Natural Language APIを使用して、サポートリクエストを分類します<br>この選択肢が正しくない理由は以下の通りです。<br>Natural Language APIはプレビルトサービスであり、特定の分類タスクに対してカスタマイズする能力が制限されており、TensorFlowを使用したモデル構築の要件を満たしません。対してAI PlatformはTensorFlowを利用して転移学習を行い、既存のモデルに自社データを適用することができます。<br>選択肢：AutoML Natural Languageを使用して、サポート要求分類器を構築します<br>この選択肢が正しくない理由は以下の通りです。<br>AutoML Natural Languageでは新規のモデル作成となるため、時間を節約するという要求に合致しません。<br>一方、AI Platformで既存のテキスト分類モデルを使用すれば、転移学習でカスタマイズが可能なので、時間を節約することができます。<br>選択肢：AI Platform上で確立されたテキスト分類モデルをそのまま使用して、サポートリクエストを分類します<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platform上の確立されたモデルをそのまま使用すると、それが特定のサポート要求の分類に適しているとは限りません。<br>一方、転移学習を行うことでは、既存のモデルをベースにして新たなデータに適用させることが可能になるため、より精度の高い分類が期待できます。'>
<div class='choice'> Natural Language APIを使用して、サポートリクエストを分類します</div>
<div class='choice'> AutoML Natural Languageを使用して、サポート要求分類器を構築します</div>
<div class='choice'> AI Platform上で確立されたテキスト分類モデルを使用して、転移学習を行います</div>
<div class='choice'> AI Platform上で確立されたテキスト分類モデルをそのまま使用して、サポートリクエストを分類します</div>
</div>

<div class='question' data-multiple='false' data-question='問題17<br>あなたは、コンタクトセンターをモダナイズしたいと考えている大手テクノロジー企業に勤めています。あなたは、リクエストをより迅速に適切なサポートチームにルーティングできるように、着信コールを製品別に分類するソリューションの開発を依頼されました。あなたはすでにSpeech-to-Text APIを使用して通話を書き起こしています。データの前処理と開発時間を最小限に抑えたいと考えています。<br>どのようにモデルを構築すべきですか？' data-answer='3' data-explanation='解説<br>正解は「AutoMlL Natural Languageを使用して、分類用のカスタムエンティティを抽出します」です。<br>この問題では、コンタクトセンターの改善を求められるシナリオに直面しています。要点は、既にSpeech-to-Text APIを使用して通話を文字に変換しており、ソリューションの開発にかかる前処理の時間と、開発時間を最小限に抑えることです。選択肢の中から、それらの要件を満たし、製品別のルーティング効率を向上するソリューションを見つけることが求められています。どの選択肢が最もコミットメントの少ない解決策を提供し、その結果として前処理と開発時間を最小限に抑えるかを考慮することが重要です。<br>基本的な概念や原則：<br>AutoML Natural Language：Google Cloudの自動機械学習（AutoML）サービスの一部で、自然言語処理タスクのカスタムモデル作成を支援します。ユーザーがカスタムエンティティやカテゴリを定義できます。<br>Speech-to-Text API：Google Cloudの音声認識サービスで、音声をテキストに変換します。これにより、音声データの自動文字起こしが可能です。<br>エンティティ抽出：テキストから具体的な情報（人、場所、イベントなど）を識別し抽出する自然言語処理のタスクです。この機能を利用して、製品名やカテゴリなど特定の情報を識別できます。<br>AI Platform Training：Google Cloudのマシンラーニングモデルの訓練とデプロイのためのプラットフォームです。ユーザーは自分のカスタムコードやGoogle Cloudの組み込みアルゴリズムを利用できます。<br>Cloud Natural Language API：Googleの自然言語理解の技術を活用して、テキストから情報を抽出できるAPIです。エンティティ分析、感情分析、構文解析などの機能を提供します。<br>正解についての説明：<br>（選択肢）<br>・AutoMlL Natural Languageを使用して、分類用のカスタムエンティティを抽出します<br>この選択肢が正解の理由は以下の通りです。<br>AutoML Natural Languageを用いることで、製品別に分類するモデルを構築するための素晴らしいリソースとなります。このサービスは、前処理や複雑なモデル構築の必要性を低減し、高度な自然言語処理の問題を解決するための機械学習モデルを生成するためのツールを提供します。<br>AutoML Natural Languageは、テキスト分析を行い、その結果を基に自動的に学習し、高精度のモデルを生成します。これにより、通話内容から意味豊かな洞察を得ることが可能となり、それを基にコールを製品別に分類することができます。<br>さらに、AutoML Natural Languageはカスタムエンティティ抽出機能を提供しており、これにより書き起こされた通話から製品名や固有の用語などの特定の情報を自動で識別、抽出することができます。これにより適切なサポートチームに迅速にルーティングするための情報が得られます。<br>したがって、開発時間の削減とデータの前処理の最小化を達成しつつ、高度な自然言語処理を行うためには、AutoML Natural Languageを使用するのがベストな選択です。<br>不正解の選択肢についての説明：<br>選択肢：AI Platform Trainingの組み込みアルゴリズムを使用して、カスタムモデルを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platform Trainingの組み込みアルゴリズムを使用する場合、データの前処理や開発時間が増える恐れがあります。これは、モデルの学習や評価に必要なデータ形式への変換、ハイパーパラメータ選択などを自分で配置・設定する必要があります。<br>一方、AutoML Natural Languageは、データの前処理や学習、評価を自動化し最小限に抑えることができます。<br>選択肢：Cloud Natural Language APIを使用して、分類用のカスタムエンティティを抽出します<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud Natural Language APIは既存のエンティティを抽出するだけであり、分類を目的としたカスタムエンティティを特定できません。<br>一方、AutoML Natural Languageはカスタムエンティティ抽出をサポートし、製品別に分類するタスクに適しています。<br>選択肢：書き起こされた通話から商品キーワードを特定するカスタムモデルを構築し、分類アルゴリズムにキーワードを通します<br>この選択肢が正しくない理由は以下の通りです。<br>カスタムモデルを構築し、分類アルゴリズムにキーワードを通す方法は前処理および開発時間が増大します。しかし正解のAutoML Natural Languageを使用すれば、カスタムエンティティ抽出の自動化により前処理と開発時間を削減できます。'>
<div class='choice'> AI Platform Trainingの組み込みアルゴリズムを使用して、カスタムモデルを作成します</div>
<div class='choice'> 書き起こされた通話から商品キーワードを特定するカスタムモデルを構築し、分類アルゴリズムにキーワードを通します</div>
<div class='choice'> Cloud Natural Language APIを使用して、分類用のカスタムエンティティを抽出します</div>
<div class='choice'> AutoMlL Natural Languageを使用して、分類用のカスタムエンティティを抽出します</div>
</div>


            <!-- 他の問題も同様に追加 -->
        </div>

        <h2 id="question"></h2>
        <ul class="choices" id="choices"></ul>
        <button onclick="checkAnswer()">採点</button>
        <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
        <div class="result" id="result"></div>
    </div>

    <script>
        let currentQuestionIndex = 0;
        let correctCount = 0;
        const questions = [];

        document.addEventListener('DOMContentLoaded', () => {
            const questionElements = document.querySelectorAll('#quiz-data .question');
            questions.push(...Array.from(questionElements).map(questionElement => ({
                question: questionElement.getAttribute('data-question').replace(/\\n/g, '<br>'),
                choices: Array.from(questionElement.querySelectorAll('.choice')).map((choice, index) => ({
                    text: choice.innerHTML.replace(/\\n/g, '<br>'),  // innerHTMLに変更
                    index: index
                })),
                correctAnswer: questionElement.getAttribute('data-answer').split(',').map(Number),
                explanation: questionElement.getAttribute('data-explanation').replace(/\\n/g, '<br>'),
                multiple: questionElement.getAttribute('data-multiple') === 'true'
            })));
            showQuestion();
        });

        function shuffleArray(array) {
            for (let i = array.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [array[i], array[j]] = [array[j], array[i]];
            }
        }

        function showQuestion() {
            const questionElement = document.getElementById('question');
            const choicesContainer = document.getElementById('choices');
            const currentQuestion = questions[currentQuestionIndex];

            shuffleArray(currentQuestion.choices);

            questionElement.innerHTML = currentQuestion.question;
            choicesContainer.innerHTML = '';

            currentQuestion.choices.forEach((choice, i) => {
                const li = document.createElement('li');
                const input = document.createElement('input');
                const label = document.createElement('label');

                input.type = currentQuestion.multiple ? 'checkbox' : 'radio';
                input.name = 'choice';
                input.value = choice.index;
                input.id = 'choice' + i;

                label.htmlFor = 'choice' + i;
                label.innerHTML = choice.text;  // textContentをinnerHTMLに変更

                li.appendChild(input);
                li.appendChild(label);
                choicesContainer.appendChild(li);
            });

            document.getElementById('result').textContent = "";
            document.getElementById('nextButton').style.display = 'none';
        }

        function checkAnswer() {
            const currentQuestion = questions[currentQuestionIndex];
            const selectedChoices = Array.from(document.querySelectorAll('input[name="choice"]:checked'))
                                        .map(checkbox => parseInt(checkbox.value))
                                        .sort();
            const resultElement = document.getElementById('result');
            
            if (selectedChoices.length > 0) {
                const isCorrect = currentQuestion.multiple
                    ? selectedChoices.toString() === currentQuestion.correctAnswer.sort().toString()
                    : selectedChoices.length === 1 && selectedChoices[0] === currentQuestion.correctAnswer[0];
                
                if (isCorrect) {
                    resultElement.innerHTML = "正解です！<br>" + currentQuestion.explanation;
                    resultElement.style.color = "green";
                    correctCount++; // 正解数をカウント
                } else {
                    resultElement.innerHTML = "残念、不正解です。<br>" + currentQuestion.explanation;
                    resultElement.style.color = "red";
                }
                document.getElementById('nextButton').style.display = 'inline';
            } else {
                resultElement.textContent = "回答を選択してください。";
                resultElement.style.color = "orange";
            }
        }

        function nextQuestion() {
            currentQuestionIndex++;
            
            if (currentQuestionIndex < questions.length) {
                showQuestion();
            } else {
                showFinalResult();
            }
        }

        function showFinalResult() {
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2>問題終了！</h2>
                <p>あなたの正解数は ${correctCount} / ${questions.length} です。</p>
                <button onclick="restartQuiz()">再挑戦する</button>
            `;
        }

        function restartQuiz() {
            correctCount = 0;
            currentQuestionIndex = 0;

            // クイズのUI全体を初期化
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2 id="question"></h2>
                <ul class="choices" id="choices"></ul>
                <button onclick="checkAnswer()">採点</button>
                <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
                <div class="result" id="result"></div>
            `;

            // 初期化後に最初の問題を表示
            showQuestion();
        }        
    </script>
</body>
</html>