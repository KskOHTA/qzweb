<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google Professional Machine Learning Engneer問題集 01</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="quiz-container">
        <div id="quiz-data" style="display: none;">


<div class='question' data-multiple='false' data-question='問題1<br>あなたは、カスタマーサポートチケットがサポートエージェントにルーティングされる前に、有益なメタデータでリッチ化するためのサーバレスMLシステムでアーキテクチャを設計しています。あなたは、チケットの優先度を予測し、チケットの解決時間を予測し、エージェントがサポートリクエストを処理する際に戦略的な意思決定を支援するためにセンチメント分析を実行する一連のモデルが必要です。チケットにはドメイン固有の用語や専門用語は含まれません。<br>提案するアーキテクチャにおいて、エンリッチメント用のCloud Functionsは以下のエンドポイントと接続する予定です：<br>- エンドポイント1：オフラインでトレーニングした"優先度"に関する分類モデルの推論結果を取得できます。<br>- エンドポイント2：オフラインでトレーニングした"解決時間"に関する回帰モデルの推論結果を取得できます。<br>- エンドポイント3：センチメント分析の結果を取得できます。<br>エンリッチメント用のCloud Functionsが呼び出すエンドポイントのバックエンドはどのようなサービスを使用するべきですか？' data-answer='2' data-explanation='解説<br>正解は「1 = AI Platform、2 = AI Platform、3 = Cloud Natural Language API」です。<br>この問題では、ユーザーから作成されたサポートチケットを機械学習で分析し、それに基づいてチケットのルーティングや優先順位付けを行うためのシステムを設計している状況について考える必要があります。チケットにはドメイン固有の用語や専門用語が含まれておらず、チケットの優先度と解決時間の予測、およびセンチメント分析が必要であるとされています。提案されたアーキテクチャでは、データの豊かさを増やすためのCloud Functionと、Firebaseと双方向通信を行うエンドポイント、オフラインでの分類と回帰の学習を行う部分を示しています。この情報をもとに、それぞれのエンドポイントがどのような役割を果たすべきか、どのAPIを使うべきかを考えることが求められています。<br>基本的な概念や原則：<br>AI Platform：機械学習モデルのトレーニングとデプロイを効率化し、スケーラブルな予測を可能にするGoogle Cloudのサービスです。<br>Cloud Functions：サーバレス環境でのイベント駆動型の関数実行を可能にするGoogle Cloudのサービスです。コードを直接デプロイし、各種イベントに基づいて実行されます。<br>Firebase：Google Cloudのモバイルとウェブアプリケーションの開発を助けるプラットフォームです。認証、データベース、ストレージなど、アプリケーション開発に必要な機能を提供します。<br>Cloud Natural Language API：テキストからエンティティ、感情、構文を解析するGoogle CloudのAPIです。ドメイン固有の用語や専門用語を理解する能力を持ちます。<br>AutoML：カスタム機械学習モデルを容易に作成、トレーニング、デプロイするためのGoogle Cloudのサービスです。特別な機械学習の知識を必要とせずに使えます。<br>正解についての説明：<br>（選択肢）<br>・1 = AI Platform、2 = AI Platform、3 = Cloud Natural Language API<br>この選択肢が正解の理由は以下の通りです。<br>まず、AI Platformはカスタム機械学習モデルのトレーニングと推論を提供するサービスで、このケースでは"優先度"の分類モデルと"解決時間"の回帰モデルの両方をオフラインでトレーニングし、それらの推論結果を提供するのに適しています。これにより、エンドポイント1と2にはAI Platformを使用するのが理想的です。<br>一方で、エンドポイント3はセンチメント分析を行うためにCloud Natural Language APIを使用するのが最適です。このAPIは自然言語のテキストを分析し、その感情や意図を理解するのに特化しており、サポートチケットのセンチメント分析に適しています。<br>したがって、1 = AI Platform、2 = AI Platform、3 = Cloud Natural Language APIという組み合わせが最も適切です。<br>不正解の選択肢についての説明：<br>選択肢：1 = AI Platform、2 = AI Platform、3 = AutoML Vision<br>この選択肢が正しくない理由は以下の通りです。<br>エンドポイント3にAutoML Visionが使用されていますが、これは画像解析に特化したサービスです。このシナリオではテキストベースのセンチメント分析が必要なので、AutoML Visionは適切ではありません。<br>選択肢：1 = AI Platform, 2 = AI Platform, 3 = AutoML Natural Language<br>この選択肢が正しくない理由は以下の通りです。<br>エンドポイント3にAutoML Natural Languageが使用されています。これはテキスト分析に適していますが、Cloud Natural Language APIの方が既存のモデルを使用して簡単にセンチメント分析が可能であり、より効率的です。<br>選択肢：1 = Cloud Natural Language API、2 = AI Platform、3 = Cloud Vision API<br>この選択肢が正しくない理由は以下の通りです。<br>エンドポイント1にCloud Natural Language APIを使用していますが、このシナリオでは優先度分類にカスタムモデルが必要であり、Cloud Natural Language APIはその用途には適していません。<br>また、エンドポイント3にCloud Vision APIを使用していますが、これは画像解析用であり、テキストベースのセンチメント分析には適していません。'>
<div class='choice'> 1 = AI Platform、2 = AI Platform、3 = AutoML Vision</div>
<div class='choice'> 1 = Cloud Natural Language API、2 = AI Platform、3 = Cloud Vision API</div>
<div class='choice'> 1 = AI Platform、2 = AI Platform、3 = Cloud Natural Language API</div>
<div class='choice'> 1 = AI Platform, 2 = AI Platform, 3 = AutoML Natural Language</div>
</div>

<div class='question' data-multiple='false' data-question='問題2<br>あなたはTensorFlowでDNN回帰を訓練し、予測特徴のセットを使って住宅価格を予測します。デフォルトの精度はtf.float64で、標準的なTensorFlowの推定器を使用します：<br>モデルのパフォーマンスは良好ですが、本番環境にデプロイする直前に、現在の配信レイテンシが10ms@90パーセンタイルであり、現在CPUで配信していることが判明しました。本番環境の要件では、モデルのレイテンシは8ms@90パーセンタイルを想定しています。レイテンシ要件を達成するためなら、多少の性能低下は受け入れるつもりです。<br>したがって、モデルの予測値がどの程度低下するかを評価しながら、レイテンシを改善する計画です。<br>サービングのレイテンシを迅速に下げるには、まず何を試すべきですか？' data-answer='0' data-explanation='解説<br>正解は「浮動小数点の精度をtf.float16に下げることで、SavedModelに量子化を適用します」です。<br>この問題では、TensorFlowを使用して構築したモデルの配信レイテンシを改善するための最初の手段が尋ねられています。ここで重要なポイントは、モデルのレイテンシを改善するための策として浮動小数点の精度を変更すること、そしてそれを行うためにSavedModelに量子化を適用することです。また、多少の性能低下は許容するという条件もあります。それ故に、問題を解く上でモデルの予測精度とレイテンシのバランスを考慮し、既存のコンフィグレーションに対して効率的な改善策を選ぶことが求められています。<br>基本的な概念や原則：<br>TensorFlow：機械学習と深層学習を行うためのオープンソースライブラリです。Dataflowグラフを使用して各種算術演算を効率的に行うことができます。<br>DNN回帰：深層ニューラルネットワーク（DNN）を用いた回帰分析のことです。DNNは多層のニューラルネットワークを利用して複雑なパターンや関連性を捉えることができます。<br>tf.float64とtf.float16：TensorFlowで利用されるデータ型の一つです。tf.float16はtf.float64に比べて精度は低いものの、計算速度とメモリ使用量が少なくなるため、パフォーマンスの改善に寄与します。<br>SavedModel：TensorFlowモデルを外部で効率的に利用できるように保存する形式です。モデルのアーキテクチャ、学習したパラメータ、エクスポート時の設定等が一緒に保存され、独立したシステムや異なるプラットフォームでの利用が可能です。<br>量子化：ニューラルネットワークのモデルサイズを削減し、推論速度を向上させるための手法の一つです。浮動小数点数を固定小数点数に変換することで、計算精度を少し犠牲にしてメモリ使用量や計算コストを削減します。<br>CPUとGPU：コンピュータのプロセッサの種類です。一般的に、GPUは並列処理に優れており、機械学習や深層学習のような大量の計算を必要とするタスクに適しています。CPUはシリアルな計算に長けています。<br>ドロップアウト率：ニューラルネットワークの訓練中にランダムにノードを削除する割合を示します。過学習を防ぐ効果がありますが、ドロップアウト率が高すぎるとモデルの学習能力が低下する恐れがあります。<br>正解についての説明：<br>（選択肢）<br>・浮動小数点の精度をtf.float16に下げることで、SavedModelに量子化を適用します<br>この選択肢が正解の理由は以下の通りです。<br>まず、浮動小数点精度をtf.float16に下げることで、モデルが効率的に動作するようになります。それは、tf.float16はtf.float64よりもメモリや計算負荷を少なく使うからです。これにより、モデルのレイテンシが向上します。この選択肢は、レイテンシが10ms@90パーセンタイルであることを鑑みて、最も即座に効果が表れる方法です。量子化はモデルサイズを縮小し、メモリ消費と推定速度を改善するための一般的なテクニックであり、これにより配信レイテンシも大幅に改善されます。なお、精度の多少の落ち込みはその対価として受け入れられるとすでに選択されているので、この選択肢は全体として最適です。これら全ての要素が適しているため、この解答選択肢が正しいと判断されます。<br>不正解の選択肢についての説明：<br>選択肢：CPUサービスからGPUサービスに切り替えます<br>この選択肢が正しくない理由は以下の通りです。<br>GPUを使用すると一般的にパフォーマンスは向上しますが、レイテンシは必ずしも改善しない可能性があります。<br>また、CPUからGPUへの切り替えはコストが増加する可能性があります。<br>それに対し、量子化によって精度を僅かに下げることでレイテンシを改善する方が効率的であるため、選択肢が正しくありません。<br>選択肢：ドロップアウト率を0.8に上げ、モデルを再トレーニングします<br>この選択肢が正しくない理由は以下の通りです。<br>ドロップアウト率を上げると、モデルのトレーニング時にニューロンを無効化する割合が増えますが、これはレイテンシの改善には直接貢献しません。<br>また、ドロップアウト率を極端に高く設定すると、モデルが過学習を防ぐために多くの情報を失う可能性があり、モデルの性能が大幅に低下するリスクがあります。<br>一方、正解の選択肢である浮動小数点の精度を下げることは、モデルのサイズを削減し、計算の速度を上げることが可能で、これがレイテンシの改善に寄与します。<br>選択肢：TensorFlow Servingパラメータを調整することにより、_PREDICTモードでドロップアウト率を0.8に上げます<br>この選択肢が正しくない理由は以下の通りです。<br>ドロップアウト率を上げると、モデルの複雑さが減少し過学習を防ぎますが、これは訓練フェーズで行われる操作であり、本番環境のレイテンシ改善には寄与しません。レイテンシ改善の観点からは、モデルの複雑さよりも計算量や学習パラメータの改善が重要です。そのため、モデルの浮動小数点の精度を下げるのが適切です。'>
<div class='choice'> 浮動小数点の精度をtf.float16に下げることで、SavedModelに量子化を適用します</div>
<div class='choice'> TensorFlow Servingパラメータを調整することにより、_PREDICTモードでドロップアウト率を0.8に上げます</div>
<div class='choice'> ドロップアウト率を0.8に上げ、モデルを再トレーニングします</div>
<div class='choice'> CPUサービスからGPUサービスに切り替えます</div>
</div>

<div class='question' data-multiple='false' data-question='問題3<br>あなたは、多人数同時参加型オンライン（MMO）ゲームを開発するゲーム会社に勤めています。あなたは、プレイヤーが今後2週間以内に10ドル以上のアプリ内購入を行うかどうかを予測するTensorFlowモデルを構築しました。このモデルの予測は、各ユーザーのゲーム体験を最適化するために使用されます。ユーザーデータはBigQueryに保存されます。<br>コスト、ユーザー体験、管理の容易さを最適化しながら、モデルをどのように提供すべきですか？' data-answer='3' data-explanation='解説<br>正解は「モデルをストリーミングDataflowパイプラインに組み込みます。アプリ内購入イベントがPub/Subでパブリッシュされるたびに予測を行い、データをCloud SQLにプッシュします」です。<br>この問題では、ユーザーの行動を予測するTensorFlowモデルの提供方法について問われています。モデルの提供手段には、Dataflowパイプライン、BigQuery ML、Vertex AI Prediction、モバイルアプリなど多種多様なものが存在し、一見するとどれも可能性としては考えられます。しかし、コスト、ユーザエクスペリエンス、管理の容易さを最適化することが求められているため、それぞれの特性と問題の条件をよく見極める必要があります。とくに本問題では、予測はアプリ内購入イベントごとに行われるという点と、ユーザーデータがBigQueryに保存されているという点に注目して解答を選ぶことが重要です。<br>基本的な概念や原則：<br>TensorFlow：機械学習モデルを構築し、訓練するためのオープンソースライブラリです。深層学習から機械学習まで、多くのアプリケーションで使用されます。<br>MMOゲーム：多人数で同時に参加し、オンラインでプレイするゲームのことです。大量のユーザーデータを扱うことが特徴です。<br>BigQuery：Google Cloudの大規模データウェアハウスサービスです。高速なSQLクエリを使用して大量のデータを操作・分析します。<br>ストリーミングDataflowパイプライン：リアルタイムデータの処理と分析を行うためのGoogle Cloudのサービスです。データの複雑な変換や集計をリアルタイムに行うことができます。<br>Cloud Pub/Sub：Google Cloudのリアルタイムメッセージングサービスです。サーバー間で大量のメッセージを送受信するために使用されます。<br>Cloud SQL：Google Cloudの完全管理型リレーショナルデータベースサービスです。MySQL、PostgreSQL、SQL Serverのデータベースエンジンを提供します。<br>BigQuery ML：BigQueryデータから直接機械学習モデルを作成・使用するための機能です。バッチ処理による予測作成が可能ですが、リアルタイムの分析には向いていません。<br>正解についての説明：<br>（選択肢）<br>・モデルをストリーミングDataflowパイプラインに組み込みます。アプリ内購入イベントがPub/Subでパブリッシュされるたびに予測を行い、データをCloud SQLにプッシュします<br>この選択肢が正解の理由は以下の通りです。<br>まず、ストリーミングDataflowパイプラインを使用すると、リアルタイムのデータ処理とモデル予測が可能になります。これにより、プレイヤーの行動に対する反応が迅速になり、ユーザー体験が向上します。<br>また、Dataflowはフルマネージドサービスであるため、管理の負荷は軽減されます。<br>次に、Pub/Subはリアルタイムのデータをパブリッシュし、処理するための信頼性が高くスケーラブルなメッセージングサービスで、アプリ内購入イベントのトリガーとして適しています。<br>最後に、予測結果をCloud SQLにプッシュすることで、アプリケーションが予測結果を簡単に取得できるようになります。これらを組み合わせることで、コスト効率、ユーザー体験、管理の容易さの観点で最適なモデルの提供が可能になります。<br>不正解の選択肢についての説明：<br>選択肢：モデルをBigQuery MLにインポートします。BigQueryからバッチでデータを読み込んで予測を行い、データをCloud SQLにプッシュします<br>この選択肢が正しくない理由は以下の通りです。<br>BigQuery MLを使用した場合、ユーザー体験が最適化されません。これはBigQueryリクエストのレスポンスタイムが予測可能ではなく、またバッチ処理での対応となるためにリアルタイムでの対応が不可能です。<br>一方、ストリーミングDataflowパイプラインはリアルタイム処理を可能にし、プレイヤーの体験を最適化します。<br>選択肢：モデルをVertex AI Predictionにデプロイします。Cloud Bigtableからバッチでデータを読み込んで予測を行い、データをCloud SQLにプッシュします<br>この選択肢が正しくない理由は以下の通りです。<br>モデルをVertex AI Predictionにデプロイし、Cloud Bigtableからバッチでデータを読み込む方法ではリアルタイムの予測が困難であり、ユーザー体験の最適化に制約が出ます。正解のストリーミングDataflowパイプラインの方法では、リアルタイムに予測を行うことが可能なため、ユーザー体験をより最適化することができます。<br>選択肢：モバイルアプリケーションにモデルを組み込みます。アプリ内購入イベントがPub/Subで公開されるたびに予測を行い、データをCloud SQLにプッシュします<br>この選択肢が正しくない理由は以下の通りです。<br>モバイルアプリケーションにモデルを組み込むことは、ユーザー体験の観点で望ましくないだけでなく、アップデートや修正が困難になるため管理の容易さも損なわれます。<br>一方、ストリーミングDataflowパイプラインにモデルを組み込むことで、リアルタイムで予測を行いつつもゲーム体験を妨げず、モデルの管理も容易になります。'>
<div class='choice'> モバイルアプリケーションにモデルを組み込みます。アプリ内購入イベントがPub/Subで公開されるたびに予測を行い、データをCloud SQLにプッシュします</div>
<div class='choice'> モデルをVertex AI Predictionにデプロイします。Cloud Bigtableからバッチでデータを読み込んで予測を行い、データをCloud SQLにプッシュします</div>
<div class='choice'> モデルをBigQuery MLにインポートします。BigQueryからバッチでデータを読み込んで予測を行い、データをCloud SQLにプッシュします</div>
<div class='choice'> モデルをストリーミングDataflowパイプラインに組み込みます。アプリ内購入イベントがPub/Subでパブリッシュされるたびに予測を行い、データをCloud SQLにプッシュします</div>
</div>

<div class='question' data-multiple='false' data-question='問題4<br>あなたはテキスト分類モデルを学習しました。以下のSignatureDefがあります：<br>signature_def[&#39;serving_default&#39;]:<br>The given SavedModel SignatureDef contains the following input(s):<br>inputs[&#39;text&#39;] tensor_info:<br>dtype: DT_STRING<br>shape: (-1, 2)<br>name: serving_default_text: 0<br>The given SavedModel SignatureDef contains the following output(s):<br>outputs[&#39;Softmax&#39;] tensor_info:<br>dtype: DT_FLOAT<br>shape: (-1, 2)<br>name: StatefulPartitionedCall:0<br>Method name is: tensorflow/serving/predict<br>TensorFlowサービスを提供するコンポーネントサーバーを起動し、次を使用して予測を取得するHTTPリクエストを送信しようとしました：<br>headers = {"content-type": "application/json"}<br>json_response = requests.post(&#39;http: //localhost:8501/v1/models/text_model:predict&#39;, data=data, headers=headers)<br>predictリクエストの正しい書き方はどれですか？' data-answer='2' data-explanation='解説<br>正解は「data = json.dumps({<br>signature_name: "serving_default",<br>instances: [[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;], [&#39;e&#39;, &#39;f&#39;]]<br>})」です。<br>この問題では、TensorFlowのSignatureDefとそれに基づいたHTTPリクエストの作成方法を理解することが求められています。SignatureDefによれば、入力テンソル&#39;text&#39;は形状が(-1, 2)の文字列型です。つまり、任意の数の2次元配列を期待しています。したがって、"instances"には2要素の毎のサブリストを持つリストを提供すべきです。リクエストの作成時に、SignatureDefの指示に従うことが重要です。<br>基本的な概念や原則：<br>TensorFlow：機械学習ライブラリで、深層学習や機械学習アルゴリズムの構築とトレーニングを可能にします。<br>SignatureDef：TensorFlowモデルの特定の関数（入力から出力へのマッピング）を定義するインターフェースです。この関数はSavedModel内で名前付きとなり、利用可能な入出力や適用方法の情報を提供します。<br>DT_STRING：TensorFlowのデータ型の一種で、文字列のテンソルを表します。<br>tensorflow/serving：TensorFlowモデルをサービスとして提供するオープンソースソフトウェアです。新しいモデルのバージョンをデプロイしたり、クライアントからのリクエストを処理して予測を返すことができます。<br>json.dumps()：Pythonの標準ライブラリの関数で、PythonオブジェクトをJSON文字列に変換します。<br>テキスト分類モデル：自然言語処理のタスクの一つで、与えられたテキストを一つまたは複数のカテゴリに分類します。<br>正解についての説明：<br>（選択肢）<br>・data = json.dumps({<br>signature_name: "serving_default",<br>instances: [[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;], [&#39;e&#39;, &#39;f&#39;]]<br>})<br>この選択肢が正解の理由は以下の通りです。<br>まず、リクエストで使用するデータはサーバで読み取り可能な形式であることが必要であり、Pythonのjson.dumps関数はPythonオブジェクトをJSON文字列にシリアライゼーションするので適切です。<br>また、"signature_name": "serving_default"はSavedModelの特定のSignatureDefを呼び出すために必要です。ここでは"tensorflow/serving/predict"として指定されていますが、このケースではデフォルトである"serving_default"を使用しています。<br>さらに、"instances"の部分では、モデルに渡すデータをリスト形式で指定しています。これは、入力テンソルの形状が（-1、2）であるためです。ここでは、3組のインスタンス（[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;], [&#39;e&#39;, &#39;f&#39;]）を提供しています。これらはそれぞれ2つのテキスト要素を含むリストであり、インプット形状に適合しています。<br>したがって、このリクエストはSavedModelを適切に呼び出し、3組のテキストデータを入力として提供します。これにより、予測を正しく取得することができます。<br>不正解の選択肢についての説明：<br>選択肢：data = json.dumps({<br>signature_name: "serving_default",<br>instances: [[&#39;ab&#39;, &#39;bc&#39;, &#39;cd&#39;]]<br>})<br>この選択肢が正しくない理由は以下の通りです。<br>提供されたSignatureDefによれば、入力テンソルの形状は(-1, 2)でなければなりません。各インスタンスは2要素のリストでなければならず、不正解選択肢のインスタンスは3要素のリストとなっており、期待される形状と一致しません。正解選択肢の各インスタンスは2要素のリストであるため、入力テンソルの期待される形状に一致します。<br>選択肢：data = json.dumps({<br>signature_name: "serving_default",<br>instances: [[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;]]<br>})<br>この選択肢が正しくない理由は以下の通りです。<br>SignatureDefの定義によれば、入力テンソルの形状は(-1, 2)となっています。これは一つのインスタンスが正確に2つの要素を含む配列でなければならないことを示しています。不正解の選択肢では1つのインスタンスに6つの要素があり、モデルの期待する形状に一致していないため、リクエストはエラーとなります。<br>正解の選択肢では、各インスタンスが2つの要素を持つ3つの配列を含むため、モデルの期待する入力形状に一致します。<br>選択肢：data = json.dumps({<br>signature_name: "serving_default",<br>instances: [[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;d&#39;, &#39;e&#39;, &#39;f&#39;]]<br>})<br>この選択肢が正しくない理由は以下の通りです。<br>まず、この選択肢では、各インスタンスが3つの要素を持つ2つのリストを送信しています。<br>しかし、SignatureDefのshapeが（-1,2）で定義されているため、各インスタンスは2つの要素を持つリストでなければなりません。<br>したがって、この形状の入力は予測リクエストとして受け入れられません。'>
<div class='choice'><br>data = json.dumps({<br>"signature_name": "serving_default",<br>"instances": [[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;]]<br>})</div>
<div class='choice'><br>data = json.dumps({<br>"signature_name": "serving_default",<br>"instances": [[&#39;ab&#39;, &#39;bc&#39;, &#39;cd&#39;]]<br>})</div>
<div class='choice'><br>data = json.dumps({<br>"signature_name": "serving_default",<br>"instances": [[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;], [&#39;e&#39;, &#39;f&#39;]]<br>})</div>
<div class='choice'><br>data = json.dumps({<br>"signature_name": "serving_default",<br>"instances": [[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;d&#39;, &#39;e&#39;, &#39;f&#39;]]<br>})</div>
</div>

<div class='question' data-multiple='false' data-question='問題5<br>あなたは国際的な大企業でデータサイエンスチームを率いています。あなたのチームがトレーニングするモデルのほとんどは、GPUを搭載したAI Platform上で高レベルのTensorFlow APIを使用した大規模モデルです。あなたのチームは通常、モデルの新バージョンを反復処理するのに数週間から数カ月を要します。一方で、あなたは最近、チームの支出を見直すよう求められました。<br>モデルのパフォーマンスに影響を与えることなく、Google Cloudのコンピュートコストをどのように削減すべきですか？' data-answer='3' data-explanation='解説<br>正解は「Google Kubernetes Engine上でKuberflowを使用したトレーニングに移行し、チェックポイント付きのプリエンプティブVMを使用します」です。<br>この問題では、Google Cloudのコンピュートコスト削減を目指し、その一方で、モデルのパフォーマンスに影響を与えないアプローチが求められています。具体的には、大規模モデルのトレーニングに使用される高レベルのTensorFlow APIを用いたAI Platformの運用に関して、コスト削減を達成するための変更点を考えます。優先事項は、モデルのパフォーマンスを維持しつつ、コンピュートコストの削減を達成することです。そのためには、AI Platformから他のサービスへの移行やトレーニング方法の変更、特定の仮想マシンの使用などを検討します。選択肢の中から、これらの要素を考慮した最適な解答を選ぶ必要があります。<br>基本的な概念や原則：<br>AI Platform：Google Cloudの機械学習サービスで、モデルのトレーニング、ホスティング、予測を一元化したプラットフォームです。<br>Google Kubernetes Engine（GKE）：Google Cloudの完全マネージドなKubernetesサービスです。コンテナ化されたアプリケーションを構築、デプロイ、スケール、運用するための環境を提供します。<br>Kubeflow： Kubernetes上での機械学習ワークフローをシンプルにするオープンソースプロジェクトです。Kubeflowを使用することで、機械学習作業を簡略化し、機械学習パイプラインの構築と展開を容易にします。<br>プリエンプティブVM：GKEで使用できるコスト効率の良い仮想マシンです。割引価格で利用可能ですが、通常のVMと異なり、予告なく停止される可能性があります。<br>チェックポイント：機械学習モデルのトレーニング中に取られる一時保存点です。トレーニングが途中で失敗した場合にトレーニングを再開するために使われます。<br>分散トレーニングジョブ：一つの大きなタスクを複数のマシン上で同時に実行するトレーニング手法です。大規模なモデルのトレーニング時間を短縮するのに有効です。<br>正解についての説明：<br>（選択肢）<br>・Google Kubernetes Engine上でKuberflowを使用したトレーニングに移行し、チェックポイント付きのプリエンプティブVMを使用します<br>この選択肢が正解の理由は以下の通りです。<br>まず、Kubeflowは、マシンラーニングモデルの開発とトレーニングを行うためのKubernetes上のオープンソースプラットフォームで、Tensorflow APIと共に使用できます。Google Kubernates Engine（GKE）は、Kubernetesアプリケーションを簡単にデプロイ、管理できる環境を提供します。KubeflowをGKE上で動作させることにより、一貫性と可視性が向上し、また学習処理を分散させてコスト効率を向上させることができます。<br>またプリエンプティブVMは、正規のインスタンスよりも大幅にコストが低いという特性を持つ一方で、それらは必要に応じてGoogleによって中断される可能性があります。<br>ただし、ここではそれが問題にはなりません。これは、Kubeflowはトレーニング中に周期的にチェックポイントを保存するため、中断が発生した場合でも、最後のチェックポイントから処理を再開することができます。つまり、モデルのトレーニングパフォーマンスには影響を与えません。<br>したがって、GKE上でKubeflowを使用し、プリエンプティブVMを用いる方針は、コンピュートコスト削減と、モデルのパフォーマンス維持を両立する最善の選択です。<br>不正解の選択肢についての説明：<br>選択肢：AI Platformを使用して、チェックポイント付きの分散トレーニングジョブを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platformで分散トレーニングジョブを実行すると、トレーニング中に予想外の中断があってもチェックポイントから再開できますが、通常のインスタンスの使用ではコスト削減にはつながらず、コストを最適化する目的には適していません。<br>対照的に、プリエンプティブVMを使うと、利用可能なときにのみコンピューティングリソースが使用されるためコスト削減につながります。<br>選択肢：AI Platformを使用して、チェックポイントなしで分散トレーニングジョブを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>チェックポイントなしで分散トレーニングジョブを実行すると、仮にジョブが中断された場合、残念ながら途中の成果を保存する手段がなく、全てやり直す必要が生じます。それにより余計なコストを発生させる可能性があるため、コスト削減の観点からは適切な選択肢ではありません。<br>選択肢：Google Kubernetes Engine上でKuberflowを使ったトレーニングに移行し、チェックポイントなしでプリエンプティブVMを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>プリエンプティブVMは停止される可能性があり、そのためチェックポイントがない場合、計算が失われ、作業が中断されます。<br>したがって、コストを削減しつつモデルのパフォーマンスに影響を与えないためには、チェックポイント付きのプリエンプティブVMの使用が必要です。'>
<div class='choice'> AI Platformを使用して、チェックポイントなしで分散トレーニングジョブを実行します</div>
<div class='choice'> Google Kubernetes Engine上でKuberflowを使ったトレーニングに移行し、チェックポイントなしでプリエンプティブVMを使用します</div>
<div class='choice'> AI Platformを使用して、チェックポイント付きの分散トレーニングジョブを実行します</div>
<div class='choice'> Google Kubernetes Engine上でKuberflowを使用したトレーニングに移行し、チェックポイント付きのプリエンプティブVMを使用します</div>
</div>

<div class='question' data-multiple='false' data-question='問題6<br>あなたは、都市の交通部門で働いており、市内のバス路線の利用率が来年にどの程度変化するかを予測するモデルを開発する任務を担っています。過去数年間のバス利用データを基にTensorFlowモデルを作成し、そのモデルをVertex AIにデプロイしました。モデルの各予測において、どの路線属性が予測に最も大きな影響を与えるかを知る必要があります。この情報を得るために、あなたはどのような手法を採用すべきですか？' data-answer='0' data-explanation='解説<br>正解は「Vertex Explainable AIを使用します。各予測リクエストにexplain&#39;キーワードを付けて送信し、サンプリングシャープレイ法を用いて特徴属性を取得します」です。<br>この問題では、都市の交通部門の一員として、既存のTensorFlowモデルを使って実施するバス路線の利用予測により、どの路線の属性が最も予測に影響を与えるかを判断することが求められています。問題の重要な点は、入手可能なデータ、使用するAIのモデル、そして特に複雑な分析を行い、モデル予測に最も大きな影響を及ぼすルートの特性を把握する予定であるかどうかです。また、ツールの適切な選択と使用によって、複雑なAI解析とシミュレーションをどのように最適に行うか、という点が問われています。<br>基本的な概念や原則：<br>Vertex AI：Google Cloudの統合的なMLOpsプラットフォームで、モデルのトレーニングからデプロイまでを管理します。<br>TensorFlow：機械学習および深層学習アプリケーション作成のための開放的なソフトウェアライブラリです。<br>Vertex Explainable AI：モデルの各予測で特徴の重要性を解明するツールです。特徴の重要性を理解することで、モデルの予測結果に対する洞察を深めることができます。<br>サンプリングシャープレイ法：ゲーム理論に基づいた特徴重要度の計算方法で、特徴の貢献度を解明するために使用されます。<br>BigQuery：Google Cloudのフルマネージド型のビッグデータ分析ツールで、SQLクエリを使用して大量のデータを分析できます。<br>Lasso回帰分析：特徴選択を行う統計学の手法で、不要な特徴をモデルから除去します。<br>Google CloudのWhat-Ifツール：モデルの予測結果を視覚的に探索し、特定の特徴が予測にどの程度影響するかを理解するためのツールです。<br>正解についての説明：<br>（選択肢）<br>・Vertex Explainable AIを使用します。各予測リクエストにexplain&#39;キーワードを付けて送信し、サンプリングシャープレイ法を用いて特徴属性を取得します<br>この選択肢が正解の理由は以下の通りです。<br>まず、Vertex Explainable AIは、機械学習モデルによる予測が特定の結果に寄与した要因を理解することを支援するGoogle Cloudの機能です。モデルの予測に影響を与えた特徴の重要性を説明するスコアを提供します。これは、どの路線属性がバス利用率予測に最も大きな影響を与えるかを理解するのに直接役立つ情報を提供します。<br>次に、各予測リクエストに&#39;explain&#39;キーワードを付けて送信すると、それによりVertex Explainable AIが活性化します。これにより、予測に最も寄与した特徴を特定するための情報を取得することが可能になります。<br>最後に、サンプリングシャープレイ法は、特徴の重要度を決定するのに使用される方法の一つです。これは特徴がモデル予測にどれだけ寄与したかを解析するためのゲーム理論の手法であり、信頼性が高いとされています。<br>したがって、この方法を用いれば、あなたが必要とする信頼性の高い情報を取得することができます。<br>不正解の選択肢についての説明：<br>選択肢：予測結果をBigQueryにストリームします。BigQueryのCORR(X1, X2)関数を使用して、各特徴とターゲット変数間のピアソン相関係数を計算します<br>この選択肢が正しくない理由は以下の通りです。<br>ピアソン相関係数は線形の相関性を見つける算出方法であり、複雑な関係を解析できません。<br>一方、Vertex Explainable AIのサンプリングシャープレイ法は、特徴属性が目的変数に与える影響の範囲、方向性も考慮し、より詳細な洞察を提供します。<br>選択肢：Vertex AI Workbenchのユーザー管理型ノートブックを使用して、モデルに対してLasso回帰分析を実行し、強いシグナルを提供しない特徴を除去します<br>この選択肢が正しくない理由は以下の通りです。<br>Lasso回帰分析を使用して強いシグナルを提供しない特徴を除去すると、予測に影響を与える各路線属性を特定するという本来の要件とは異なります。<br>また、Vertex AI Workbenchのノートブックを使用しても、各予測リクエストで最も影響力のある特徴属性を動的に取得することはできません。<br>選択肢：Google CloudのWhat-Ifツールを使って、個々の特徴を除外したときのモデルのパフォーマンスを調べます。モデルから除外したときにパフォーマンスが最も大きく低下した特徴の順に、特徴の重要度をランク付けします<br>この選択肢が正しくない理由は以下の通りです。<br>Google CloudのWhat-Ifツールは、特徴の影響力を調べることができますが、個々の予測に対してどの特徴が最も影響を与えているかを明示的に示す能力を持っています。つまり、各予測における特定の路線属性の影響を直接確認することはできません。これに対してVertex Explainable AIでは、各予測の説明性を提供するため、このシナリオの要件をより正確に満たします。'>
<div class='choice'> Vertex Explainable AIを使用します。各予測リクエストにexplain&#39;キーワードを付けて送信し、サンプリングシャープレイ法を用いて特徴属性を取得します</div>
<div class='choice'> Google CloudのWhat-Ifツールを使って、個々の特徴を除外したときのモデルのパフォーマンスを調べます。モデルから除外したときにパフォーマンスが最も大きく低下した特徴の順に、特徴の重要度をランク付けします</div>
<div class='choice'> 予測結果をBigQueryにストリームします。BigQueryのCORR(X1, X2)関数を使用して、各特徴とターゲット変数間のピアソン相関係数を計算します</div>
<div class='choice'> Vertex AI Workbenchのユーザー管理型ノートブックを使用して、モデルに対してLasso回帰分析を実行し、強いシグナルを提供しない特徴を除去します</div>
</div>

<div class='question' data-multiple='false' data-question='問題7<br>あなたは最近、Kerasを使ってディープラーニングモデルを開発し、さまざまなトレーニング戦略を試しています。まず、単一のGPUを使用してモデルをトレーニングしましたが、トレーニングプロセスが遅すぎました。次に、tf.distribute.MirroredStrategyを使用して4つのGPUにトレーニングを分散しましたが（他の変更はありません）、トレーニング時間の短縮は見られませんでした。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「バッチサイズを大きくします」です。<br>この問題では、ディープラーニングのトレーニングプロセスを高速化するための最適な戦略を選ぶことが求められています。問題文を注意深く読んで、変更前と変更後の状況、特にトレーニング戦略に対する特定の反応を理解することが重要です。各選択肢が問題の状況にどのように適合するかを検討し、最も効果的な変更をもたらすものを選びます。分析は技術的な理解だけでなく、事実に基づいた観察にも焦点を当てるべきです。<br>基本的な概念や原則：<br>Keras：Pythonで書かれたオープンソースのニューラルネットワークライブラリです。機械学習モデルの設計と訓練を容易にします。<br>ディープラーニングモデル：複数の隠れ層を持つニューラルネットワークです。高度な特徴抽出が可能であるため、画像認識や自然言語処理など、複雑なタスクに適しています。<br>tf.distribute.MirroredStrategy：複数のGPUやワーカーを使用して訓練を行うためのTensorFlowのAPIです。同期されたトレーニングを可能にします。<br>バッチサイズ：各訓練ステップでネットワークに供給されるデータの数です。バッチサイズを大きくすると、トレーニングプロセスが一度により多くのデータを処理するため、トレーニングが速くなる可能性があります。<br>トレーニング戦略：ディープラーニングモデルの訓練を最適化するための方針や手段です。分散トレーニング、バッチサイズの調整、学習率のスケジューリングなどが該当します。<br>正解についての説明：<br>（選択肢）<br>・バッチサイズを大きくします<br>この選択肢が正解の理由は以下の通りです。<br>まず、トレーニングの分散が効果的に機能するためには、バッチサイズが適切であることが重要です。tf.distribute.MirroredStrategyを使用する際には、そのバッチサイズが使用するGPUの数に比例して大きくなるように調整する必要があります。4つのGPUにトレーニングを分散する場合、一般的にはバッチサイズを4倍にすると良いとされています。バッチサイズが小さすぎると、各GPUが処理するデータの量が少なくなり、GPUの計算能力を十分に活用することができず、トレーニング時間が短縮されない可能性があります。<br>したがって、バッチサイズを大きくすることで、同時に処理できるデータの量が増え、各GPUの計算能力を最大限に活用することができ、結果としてトレーニング時間の短縮が見込めます。<br>不正解の選択肢についての説明：<br>選択肢：tf.distribute.Strategy.experimental_distribute_datasetでデータセットを配布します<br>この選択肢が正しくない理由は以下の通りです。<br>tf.distribute.Strategy.experimental_distribute_datasetでデータセットを配布すると、データの分配は改善されるかもしれませんが、既に4つのGPUにトレーニングは分散しているので、大きな効果は見込めません。<br>それに対して、バッチサイズを大きくすると、各更新ごとに処理されるデータ量が増え、GPUの並列実行能力を活用してトレーニング速度を向上させることができます。<br>選択肢：カスタムトレーニングループを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>カスタムトレーニングループを作成すると、ユーザーが細かく制御できるようになりますが、必ずしもパフォーマンスが向上するわけではありません。<br>対照的に、バッチサイズを大きくすれば、1回のトレーニングで処理するデータ量が増え、結果、全データに対するトレーニング回数が減り、トレーニング時間が短縮されます。<br>選択肢：tf.distribute.TPUStrategyでTPUを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>tf.distribute.TPUStrategyでTPUを使用する方法は、モデルのトレーニング速度を向上させるかもしれませんが、既に4つのGPUを使用してトレーニングの分散を試みても結果が出なかったことから、単純にハードウェアを変えるだけでは問題解決にはなりません。<br>一方、バッチサイズを大きくすると、同時に処理するデータの量が増えてGPUの並列処理能力を活かしやすくなり、トレーニング時間が短縮される可能性があります。'>
<div class='choice'> tf.distribute.TPUStrategyでTPUを使用します</div>
<div class='choice'> バッチサイズを大きくします</div>
<div class='choice'> カスタムトレーニングループを作成します</div>
<div class='choice'> tf.distribute.Strategy.experimental_distribute_datasetでデータセットを配布します</div>
</div>

<div class='question' data-multiple='false' data-question='問題8<br>あなたは、学習時間を短縮したセマンティック画像セグメンテーションのためのディープラーニングモデルをトレーニングしています。ディープラーニングVMイメージの使用中に、以下のエラーが発生しました：<br>リソース &#39;projects/deeplearning-platforn/zones/europe-west4-c/acceleratorTypes/nvidia-tesla-k80&#39; が見つかりません。<br>トラブルシューティングをするために、あなたはどうすればよいですか？' data-answer='2' data-explanation='解説<br>正解は「選択したリージョンで必要なGPUが利用可能であることを確認します」です。<br>この問題では、Google Cloudにおけるリソース失敗のエラーメッセージに対する適切な対処方法を見極めることが求められています。エラーメッセージは特定のGPUが見つからないと言っており、そのGPUが存在するかどうか確認することが要求されます。選択肢を見る際には、そのGPUが利用可能かどうか、クォータの存在、GPUメモリの量などに注意を払いながらエラーメッセージに対応した適切な解決策を選びましょう。<br>基本的な概念や原則：<br>モデルトレーニング：機械学習の一環で、初期の学習データセットを使用してモデルをトレーニングするプロセスです。データを使用してモデルにパターンを学習させ、予測や分類を行います。<br>リソースが見つからない：Google Cloudの一部として機能するリソースが見つからない、または存在しない場合のエラーメッセージです。<br>リージョンとゾーン：リージョンはGoogle Cloudのリソースが存在する地理的ロケーションで、ゾーンはリージョン内の特定のエリアです。リソースの可用性はリージョンとゾーンによる。<br>ディープラーニングVMイメージ：ディープラーニング環境の設定と管理を簡素化するための仮想マシンイメージです。<br>GPU（Graphics Processing Unit）：高速な数値計算を提供するハードウェアユニットです。機械学習などのデータ主導型タスクで非常に有用です。<br>クォータ：Google Cloudのサービスで利用可能なリソースの最大量を制限するシステムです。特定のサービスの使用量を一定の範囲内に抑えるために使用されます。<br>正解についての説明：<br>（選択肢）<br>・選択したリージョンで必要なGPUが利用可能であることを確認します<br>この選択肢が正解の理由は以下の通りです。<br>エラーメッセージは、指定した &#39;nvidia-tesla-k80&#39; というタイプのリソースがプロジェクトの指定したリージョンで見つからないことを示しています。これは主に、そのリージョンが指定したタイプのリソースをサポートしていない場合や、そのリソースがその時点で使用できない場合に発生します。具体的には、nvidia-tesla-k80のような特定のGPUは、すべてのリージョンで利用可能とは限りません。そのため、エラーを解決するためにはまず、選択したリージョン &#39;europe-west4-c&#39; で必要なGPU &#39;nvidia-tesla-k80&#39; が利用可能であることを確認する必要があります。これにより、指定したリソースを適切にアクセスできる設定を可能にするための適切なステップを踏むことができます。<br>不正解の選択肢についての説明：<br>選択肢：選択したリージョンにGPUクォータがあることを確認します<br>この選択肢が正しくない理由は以下の通りです。<br>エラーメッセージはGPUが存在しないことを示しているため、GPUクォータが存在するかどうかを確認することが問題の解決には繋がりません。<br>一方、選択したリージョンで該当のGPUが利用可能か確認することにより、具体的なGPUの存在を確認でき、問題解決に繋がります。<br>選択肢：選択したリージョンにプリエンプティブGPUクォータがあることを確認します<br>この選択肢が正しくない理由は以下の通りです。<br>エラーメッセージは、指定したリージョン内での特定のGPUタイプの存在を問いていますが、プリエンプティブGPUクォータは使用可能なGPUインスタンスの数に関するものです。正解選択肢はそのGPUがそのリージョンで利用可能かを確認するもので、これが問題解決の適切なアプローチです。<br>選択肢：選択したGPUにワークロードに十分なGPUメモリがあることを確認します<br>この選択肢が正しくない理由は以下の通りです。<br>エラーメッセージは存在しないリソースを指定していることを示しているので、GPUのメモリを確認することは解決に繋がりません。そのため、選択したリージョンで必要なGPUが利用できるか確認するのが適切です。'>
<div class='choice'> 選択したリージョンにGPUクォータがあることを確認します</div>
<div class='choice'> 選択したリージョンにプリエンプティブGPUクォータがあることを確認します</div>
<div class='choice'> 選択したリージョンで必要なGPUが利用可能であることを確認します</div>
<div class='choice'> 選択したGPUにワークロードに十分なGPUメモリがあることを確認します</div>
</div>

<div class='question' data-multiple='false' data-question='問題9<br>あなたはソーシャルメディア企業に勤めています。投稿された画像に車が含まれているかどうかを検出する必要があります。各トレーニング例は正確に1つのクラスのメンバーです。あなたは物体検出ニューラルネットワークをトレーニングし、評価のためにAI Platform Predictionにモデルバージョンをデプロイしました。デプロイする前に、評価ジョブを作成し、AI Platform Predictionモデルバージョンにアタッチしました。精度がビジネス要件の許容値よりも低いことに気づきました。<br>精度を上げるために、モデルの最終層のソフトマックスの閾値をどのように調整すべきですか？' data-answer='3' data-explanation='解説<br>正解は「リコールを減らします」です。<br>この問題では、機械学習モデルの精度向上と評価指標に関する理解が求められています。具体的には、低い精度を上げるためにソフトマックスの閾値をどのように調整すべきかを問われています。リコールを減らすことで、ソフトマックスの閾値が調整され、結果として精度が改善されることを意味します。評価ジョブの結果をベースに、適切な閾値調整のアプローチを選ぶ必要があります。<br>基本的な概念や原則：<br>ソフトマックス関数：ニューラルネットワークの出力層でよく使用される関数で、各クラスに属する確率を表します。<br>リコール：真の陽性の数を真の陽性の数と偽陰性の数で割ったもの。すべての陽性サンプルのうち、正しく陽性と予測されたものの割合を示します。<br>閾値調整：モデルの出力確率を分類するための閾値の調整。閾値を上げるとリコールが下がり、閾値を下げるとリコールが上がります。不正解率とリコールのバランスを調整するために使用されます。<br>偽陽性：実際には陰性でありますが、陽性と予測されたケース。予測モデルの誤分類率の一部を占めます。<br>偽陰性：実際には陽性でありますが、陰性と予測されたケース。予測モデルの誤分類率の一部を占めます。<br>精度：予測モデルが正しく予測したケースの割合。モデルの性能の一部を示します。<br>正解についての説明：<br>（選択肢）<br>・リコールを減らします<br>この選択肢が正解の理由は以下の通りです。<br>物体検出問題の場合、ソフトマックスの閾値を調節することは、適合率（精度）とリコール（感度）のバランスを調整する行為になります。適合率は、正しく識別された正の例の割合を示し、リコールは、実際の正の例をどれだけ識別できたかを示します。<br>適合率を向上させるには、モデルがポジティブと判断するための閾値を高く設定します。<br>しかし、これにより、閾値を満たさずに"ポジティブ"と判断できなかったインスタンスが増えるため、リコールは減ります。つまり、具体的な手段としては閾値を上げて適合率を上げるアクションですが、その結果としてリコールが減少します。<br>したがって、適合率（精度）を向上させるためには、"リコールを減らします"との選択肢が最適です。<br>不正解の選択肢についての説明：<br>選択肢：リコールを増やします<br>この選択肢が正しくない理由は以下の通りです。<br>リコールを増やすと、真陽性が増え、つまり、車が含まれていると予測が正しかった画像が増える一方で、偽陽性（車がないのに車があると誤って予測した）も増えてしまいます。つまり、誤った予測が増える可能性があり、結果として精度が下がります。<br>それに対して、リコールを減らすと偽陽性が減るため、全体の精度が上がる可能性があります。<br>選択肢：偽陽性の数を増やします<br>この選択肢が正しくない理由は以下の通りです。<br>偽陽性の数を増やすという行為は、むしろ精度を下げる可能性があります。これは偽陽性を増やすことは、誤った識別が増加するためです。<br>それに対して、正解の"リコールを減らします"は、不要な識別を避けることで精度を高める効果があります。<br>選択肢：偽陰性の数を減らします<br>この選択肢が正しくない理由は以下の通りです。<br>偽陰性の数を減らすというアプローチは直接的にソフトマックスの閾値の調整とは関連がありません。ソフトマックスの閾値を調整することにより、モデルが予測ラベルを決定するための確信度を操作します。<br>対照的に、リコールを減らすという選択はモデルの閾値を調整して、真陽性の予測を増やすことを意味します。これが精度の向上に直接寄与します。'>
<div class='choice'> リコールを増やします</div>
<div class='choice'> 偽陽性の数を増やします</div>
<div class='choice'> 偽陰性の数を減らします</div>
<div class='choice'> リコールを減らします</div>
</div>

<div class='question' data-multiple='false' data-question='問題10<br>あなたはモバイルゲーム会社のMLエンジニアです。あなたのチームのデータサイエンテストは最近TensorFlowモデルをトレーニングし、あなたはこのモデルをモバイルアプリケーションにデプロイする責任を負っています。あなたは、現在のモデルの推論レイテンシが本番要件を満たしていないことに気づきました。あなたは推論時間を50%短縮する必要があり、レイテンシ要件を達成するためにモデルの精度が多少低下することを受け入れるつもりです。<br>新しいモデルをトレーニングすることなく、待ち時間を短縮するために、まずどのモデル最適化手法を試すべきですか？' data-answer='3' data-explanation='解説<br>正解は「ダイナミックレンジ量子化」です。<br>この問題では、一部の精度を犠牲にして推論時間を短縮する方法を求めています。このような観点から、モデルの最適化手法を選ぶ必要があります。また、重要なポイントとして、"新しいモデルをトレーニングすることなく"という条件があるので、モデルの再トレーニングを必要とする最適化手法は選択肢から外すべきでしょう。これを踏まえ、選択肢の中から最適な手法を選ぶことが問題の解答です。<br>基本的な概念や原則：<br>ダイナミックレンジ量子化：一定の精度を維持しながらModel sizeを削減する手法の一つ。TensorFlow Liteで提供されており、model sizeを4分の1に削減し、ハードウェアアクセラレーションの利点を享受できます。<br>重みプルーニング：ニューラルネットワークの重みを削減（プルーニング）することでモデルのサイズを小さくし、推論速度を向上させる技術。不要な重みを削除することで、効率性を向上させますが、この選択肢は不正解です。<br>モデル蒸留：複雑なモデル（教師モデル）の知識を、より単純なモデル（生徒モデル）に転送する手法。これにより、予測の精度を維持しつつ、推論速度が高まりますが、新しいモデルのトレーニングが必要となるため、この問題の解答とはなりません。<br>次元削減：高次元のデータを低次元に変換し、計算コストを削減するとともに、データの可視化を促進します。しかし、新しい特徴空間を作る必要があり、新たなモデルのトレーニングが必要です。<br>正解についての説明：<br>（選択肢）<br>・ダイナミックレンジ量子化<br>この選択肢が正解の理由は以下の通りです。<br>まず、ダイナミックレンジ量子化は、TensorFlow Liteが提供するモデル最適化ツールで、32ビット浮動小数点数を8ビット固定小数点数に変換することでモデルのサイズを大幅に削減します。このサイズ削減は、モデルのロード時間とインフラストラクチャの要求を減らし、ワークロードのレイテンシを改善します。<br>さらに、量子化により推論の計算速度が向上し、推論時間が短縮されます。なお、この手法は推論精度が多少低下する恐れがありますが、それは問いの要件を満たしています。<br>したがって、ダイナミックレンジ量子化は新しいモデルをトレーニングすることなくレイテンシを短縮するための適切な手法と言えます。<br>不正解の選択肢についての説明：<br>選択肢：重みプルーニング<br>この選択肢が正しくない理由は以下の通りです。<br>重みプルーニングはモデルのサイズを縮小する手法でありますが、推論レイテンシには大きく寄与しません。<br>それに対して、ダイナミックレンジ量子化はモデルの重みを低精度の表現に変換し、推論速度を向上させ、推論レイテンシを短縮する効果があります。<br>選択肢：モデル蒸留<br>この選択肢が正しくない理由は以下の通りです。<br>モデル蒸留は、大規模なモデルから知識を抽出して小規模なモデルを作る手法ですが、新しいモデルのトレーニングが必要になります。<br>一方、ダイナミックレンジ量子化は既存のモデルの数値を効率的に表現し、レイテンシを削減する手法で、再訓練不要です。<br>選択肢：次元削減<br>この選択肢が正しくない理由は以下の通りです。<br>次元削減はデータの視覚化やデータ圧縮に用いられる手法であり、一般にモデルのパフォーマンス向上に直接的に寄与するものではありません。<br>一方、ダイナミックレンジ量子化はTensorFlowモデルのサイズを縮小し、推論時間を短縮する効果があります。これは特にモバイルデバイスでのパフォーマンス向上に効果的な手法です。'>
<div class='choice'> 次元削減</div>
<div class='choice'> モデル蒸留</div>
<div class='choice'> 重みプルーニング</div>
<div class='choice'> ダイナミックレンジ量子化</div>
</div>

<div class='question' data-multiple='false' data-question='問題11<br>あなたは、AI PlatformでMLモデルを開発し、本番環境に移行したいと考えています。しかし、1秒間に数千のクエリを処理し、待ち時間の問題が発生しています。入ってくるリクエストは、Google Kubernetes Engine（GKE）上で動作する複数のKubeflow CPU専用ポッドに分散するロードバランサによって処理されます。あなたの目標は、基盤となるインフラストラクチャを変更することなく、配信レイテンシを改善することです。<br>この要件を満たすために、どうすればよいですか？' data-answer='3' data-explanation='解説<br>正解は「CPU固有の最適化をサポートするために、ソースを使用してTensorFlow Servingを再コンパイルします。サービングノード用の適切なベースライン最小CPUプラットフォームを選択するよう、GKEに指示します」です。<br>この問題では、既存のAI Platform上でのレイテンシの問題に対処する方法を問われています。期待されるアクションは、インフラストラクチャを変更することなく配信レイテンシを改善することです。そのため、ハードウェアや基盤技術の変更要件は無視できますが、ソフトウェアレベルでの最適化や効率化の手段については検討する必要があります。TensorFlow Servingパラメータの調整や特定バージョンへの切り替え、再コンパイルといった選択肢が提示されています。選択肢の中から既存のインフラストラクチャに変更を加えず、効率よく問題解決できる方法を選ぶ必要があります。<br>基本的な概念や原則：<br>AI Platform：Google Cloudの機械学習モデルを開発と運用するサービスです。データの前処理からモデルのトレーニング、予測の実行まで一貫して行うことができます。<br>TensorFlow Serving：機械学習モデルを運用環境でサービングするためのシステムです。高性能で機械学習に特化したところが特徴で、TensorFlow専用のものとして開発されています。<br>ソースコードの再コンパイル：ソースコードを特定の環境やプラットフォームに最適化するために、ソースコードを再コンパイルするという作業です。これによって、特定のCPUやGPUなどに最適化した実行ファイルを生成することができます。<br>Google Kubernetes Engine（GKE）：Google Cloudが提供するKubernetesをベースとしたコンテナオーケストレーションサービスです。アプリケーションのデプロイ、運用、スケーリングを簡単にすることができます。<br>ベースライン最小CPUプラットフォーム：アプリケーションを運用するために必要な最小限のCPUパフォーマンスを指定するものです。これにより、特定のCPUアーキテクチャで正しく動作することを保証します。<br>max_batch_size：TensorFlow Servingで、一度に処理可能なリクエストの最大数を指定するパラメータです。値を大きく設定すると一度に多くのリクエストを処理できますが、レイテンシが増加する可能性があります。<br>tensorflow-model-server-universal：TensorFlow Servingのバージョンの一つで、様々なプラットフォームで動作するよう設計されています。しかし、特定のプラットフォームに対する最適化が行われていない可能性があります。<br>正解についての説明：<br>（選択肢）<br>・CPU固有の最適化をサポートするために、ソースを使用してTensorFlow Servingを再コンパイルします。サービングノード用の適切なベースライン最小CPUプラットフォームを選択するよう、GKEに指示します<br>この選択肢が正解の理由は以下の通りです。<br>まず、TensorFlow Servingをソースから再コンパイルすることで、特定のCPUアーキテクチャの最適化を利用することができます。これは、機械学習の推論タスクを高速化し、レイテンシを改善するための重要な手段です。手元のソースコードからコンパイルすることで、推論計算に使う特定のCPU命令セットを持つマシン上で一番効率的に動作するバイナリを生成することができます。<br>また、GKEにベースラインとなる最小CPUプラットフォームを指定することで、クラスター内の各ノードがそのCPUプラットフォームに対応したものになることを保証します。これにより、TensorFlow Servingのパフォーマンスを最大化し、配信レイテンシを改善することができます。注意点としては、このアプローチが有効なのはKubeflowのCPU専用ポッドが使用する特定のCPU命令セットが存在し、かつそれを活用した最適化が可能な場合です。<br>不正解の選択肢についての説明：<br>選択肢：TensorFlow Servingパラメータのmax_batch_sizeを大幅に増やします<br>この選択肢が正しくない理由は以下の通りです。<br>max_batch_sizeを大幅に増やすことは、レイテンシを増加させる可能性があります。これは、システムがバッチサイズを満たすまでリクエストを待機してしまうためです。これは問題の要件である"配信レイテンシを改善する"ことに反します。正答は高レイテンシの問題を解決するのに適した手法です。<br>選択肢：TensorFlow Servingのtensorflow-model-server-universalバージョンに切り替えます<br>この選択肢が正しくない理由は以下の通りです。<br>tensorflow-model-server-universalバージョンを使用したとしても、特定のCPU固有の最適化は適用されません。<br>したがって、レイテンシの問題は改善されず、インフラストラクチャに変更を加えずに配信レイテンシを改善する目的を達成することができません。<br>選択肢：TensorFlow Servingパラメータのmax_enqueued_batchesを大幅に増やします<br>この選択肢が正しくない理由は以下の通りです。<br>max_enqueued_batchesを増やすと確かに並列処理能力は向上しますが、これがレイテンシの問題を解決するとは限りません。<br>一方で、ソースを再コンパイルしCPU固有の最適化を行うことで、全体のシステムパフォーマンスが改善し、結果として配信レイテンシが低減する可能性が高まります。'>
<div class='choice'> TensorFlow Servingパラメータのmax_enqueued_batchesを大幅に増やします</div>
<div class='choice'> TensorFlow Servingパラメータのmax_batch_sizeを大幅に増やします</div>
<div class='choice'> TensorFlow Servingのtensorflow-model-server-universalバージョンに切り替えます</div>
<div class='choice'> CPU固有の最適化をサポートするために、ソースを使用してTensorFlow Servingを再コンパイルします。サービングノード用の適切なベースライン最小CPUプラットフォームを選択するよう、GKEに指示します</div>
</div>


            <!-- 他の問題も同様に追加 -->
        </div>

        <h2 id="question"></h2>
        <ul class="choices" id="choices"></ul>
        <button onclick="checkAnswer()">採点</button>
        <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
        <div class="result" id="result"></div>
    </div>

    <script>
        let currentQuestionIndex = 0;
        let correctCount = 0;
        const questions = [];

        document.addEventListener('DOMContentLoaded', () => {
            const questionElements = document.querySelectorAll('#quiz-data .question');
            questions.push(...Array.from(questionElements).map(questionElement => ({
                question: questionElement.getAttribute('data-question').replace(/\\n/g, '<br>'),
                choices: Array.from(questionElement.querySelectorAll('.choice')).map((choice, index) => ({
                    text: choice.innerHTML.replace(/\\n/g, '<br>'),  // innerHTMLに変更
                    index: index
                })),
                correctAnswer: questionElement.getAttribute('data-answer').split(',').map(Number),
                explanation: questionElement.getAttribute('data-explanation').replace(/\\n/g, '<br>'),
                multiple: questionElement.getAttribute('data-multiple') === 'true'
            })));
            showQuestion();
        });

        function shuffleArray(array) {
            for (let i = array.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [array[i], array[j]] = [array[j], array[i]];
            }
        }

        function showQuestion() {
            const questionElement = document.getElementById('question');
            const choicesContainer = document.getElementById('choices');
            const currentQuestion = questions[currentQuestionIndex];

            shuffleArray(currentQuestion.choices);

            questionElement.innerHTML = currentQuestion.question;
            choicesContainer.innerHTML = '';

            currentQuestion.choices.forEach((choice, i) => {
                const li = document.createElement('li');
                const input = document.createElement('input');
                const label = document.createElement('label');

                input.type = currentQuestion.multiple ? 'checkbox' : 'radio';
                input.name = 'choice';
                input.value = choice.index;
                input.id = 'choice' + i;

                label.htmlFor = 'choice' + i;
                label.innerHTML = choice.text;  // textContentをinnerHTMLに変更

                li.appendChild(input);
                li.appendChild(label);
                choicesContainer.appendChild(li);
            });

            document.getElementById('result').textContent = "";
            document.getElementById('nextButton').style.display = 'none';
        }

        function checkAnswer() {
            const currentQuestion = questions[currentQuestionIndex];
            const selectedChoices = Array.from(document.querySelectorAll('input[name="choice"]:checked'))
                                        .map(checkbox => parseInt(checkbox.value))
                                        .sort();
            const resultElement = document.getElementById('result');
            
            if (selectedChoices.length > 0) {
                const isCorrect = currentQuestion.multiple
                    ? selectedChoices.toString() === currentQuestion.correctAnswer.sort().toString()
                    : selectedChoices.length === 1 && selectedChoices[0] === currentQuestion.correctAnswer[0];
                
                if (isCorrect) {
                    resultElement.innerHTML = "正解です！<br>" + currentQuestion.explanation;
                    resultElement.style.color = "green";
                    correctCount++; // 正解数をカウント
                } else {
                    resultElement.innerHTML = "残念、不正解です。<br>" + currentQuestion.explanation;
                    resultElement.style.color = "red";
                }
                document.getElementById('nextButton').style.display = 'inline';
            } else {
                resultElement.textContent = "回答を選択してください。";
                resultElement.style.color = "orange";
            }
        }

        function nextQuestion() {
            currentQuestionIndex++;
            
            if (currentQuestionIndex < questions.length) {
                showQuestion();
            } else {
                showFinalResult();
            }
        }

        function showFinalResult() {
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2>問題終了！</h2>
                <p>あなたの正解数は ${correctCount} / ${questions.length} です。</p>
                <button onclick="restartQuiz()">再挑戦する</button>
            `;
        }

        function restartQuiz() {
            correctCount = 0;
            currentQuestionIndex = 0;

            // クイズのUI全体を初期化
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2 id="question"></h2>
                <ul class="choices" id="choices"></ul>
                <button onclick="checkAnswer()">採点</button>
                <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
                <div class="result" id="result"></div>
            `;

            // 初期化後に最初の問題を表示
            showQuestion();
        }        
    </script>
</body>
</html>