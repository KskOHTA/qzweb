<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google Professional Machine Learning Engneer問題集 01</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="quiz-container">
        <div id="quiz-data" style="display: none;">


<div class='question' data-multiple='false' data-question='問題1<br>あなたは最近、自動運転車用の画像セグメンテーションモデルの最初のバージョンを構築しました。このモデルをデプロイした後、あなたは曲線下面積（AUC）メトリクスの減少を観察しました。また、ビデオ録画を分析したところ、混雑の激しい交通ではモデルが失敗しますが、交通量が少ないときには期待どおりに機能することがわかりました。<br>この結果の最も考えられる理由は何ですか？' data-answer='1' data-explanation='解説<br>正解は「このモデルは、交通量の少ない地域ではオーバーフィットし、交通量の多い地域ではアンダーフィットしています」です。<br>この問題では、最初に自動運転車の画像セグメンテーションモデルの動作について説明されています。次に、問題が交通の状況によって異なる動作を示すという情報が与えられます。この情報から、モデルの訓練と実際の運用環境に差異があるということ実に焦点を当てるべきです。選択肢を考慮する時には、モデルのパフォーマンスが交通の状況によって変化する理由を理解することが重要です。<br>基本的な概念や原則：<br>オーバーフィッティング：モデルが訓練データに対して過度に適用し、新しいデータに対する性能が低下する現象です。一部の特定の条件や状況（この場合、交通量が少ない状況）に対してのみ高い性能を発揮します。<br>アンダーフィッティング：モデルが訓練データに対して十分に適用できず、一般化能力が低い状態のことを指します。この場合、交通量の多い状況ではモデルの性能が低下します。<br>AUC（曲線下面積）：受信者操作特性（ROC）曲線下の面積を指し、機械学習モデルの分類性能を評価するための指標です。1に近いほど良いモデルを示します。<br>画像セグメンテーション：画像を複数のセグメント（部分）に分割するプロセスのことです。この手法により、画像の各ピクセルがどのオブジェクトに属するかを特定できます。<br>深層学習モデル：人間の脳のニューロンとシナプスのネットワークを模倣した人工ニューラルネットワークを利用して、データに存在するパターンを学習する人工知能の一部門です。誤差逆伝播法や勾配消失問題など、訓練過程で考慮すべき課題があります。<br>正解についての説明：<br>（選択肢）<br>・このモデルは、交通量の少ない地域ではオーバーフィットし、交通量の多い地域ではアンダーフィットしています<br>この選択肢が正解の理由は以下の通りです。<br>まず、機械学習モデルは学習データの特徴に基づいて予測を行いますが、そのモデルが学習用データに過度に適合（オーバーフィット）すると、新しい未知のデータ（ここでは混雑した交通状況）に対する予測精度が低下します。<br>一方、適度に適合しない（アンダーフィット）モデルは、学習データ内の重要なパターンをつかみきれず、全体的な性能が低くなります。<br>今回の問題の状況では、モデルが交通量の少ない状況では期待通りに機能し、混雑した状況では機能しないため、学習データは交通量の少ない状況が多く、その結果モデルはそのような状況にオーバーフィットし、交通量の多い状況に対してはアンダーフィットしていると解釈できます。これがAUCメトリクスの減少につながっていると考えられます。<br>不正解の選択肢についての説明：<br>選択肢：AUCはこの分類モデルを評価するための正しい指標ではありません<br>この選択肢が正しくない理由は以下の通りです。<br>問題の文脈からはAUCが不適切な指標であるとは結論付けられません。AUCはモデルの性能を評価する一般的な指標であり、画像セグメンテーションモデルの評価にも利用可能です。<br>一方、正解の選択肢は交通量の違いによりモデルの性能が変わる状況をうまく表しており、問題文の状況と一致しています。<br>選択肢：モデルのトレーニングに使用された混雑エリアを表すデータが多すぎます<br>この選択肢が正しくない理由は以下の通りです。<br>まず、この選択肢はモデルのトレーニングデータに混雑エリアのデータが多すぎることを指していますが、設問の説明によればモデルは交通が混雑しているときに失敗していると記載されています。<br>したがって、混雑エリアのデータが多い場合は、モデルがその状況に適用して期待通りに機能するはずです。これは正解の選択肢と矛盾します。正解はモデルが交通量の少ない地域でオーバーフィットし、交通量の多い地域でアンダーフィットしていると指摘しており、この状況が問題の結果を説明しています。<br>選択肢：出力ノードから入力ノードへの逆伝播中に勾配が小さくなり、消滅しています<br>この選択肢が正しくない理由は以下の通りです。<br>提供された情報からは出力ノードから入力ノードへの逆伝播中に勾配が消失しているかどうかを判断する材料はないため、不正解です。<br>一方、正解はモデルの予測結果から判断が可能で、交通の状況によって予測精度が変わることが適切に説明されています。'>
<div class='choice'> モデルのトレーニングに使用された混雑エリアを表すデータが多すぎます</div>
<div class='choice'> このモデルは、交通量の少ない地域ではオーバーフィットし、交通量の多い地域ではアンダーフィットしています</div>
<div class='choice'> 出力ノードから入力ノードへの逆伝播中に勾配が小さくなり、消滅しています</div>
<div class='choice'> AUCはこの分類モデルを評価するための正しい指標ではありません</div>
</div>

<div class='question' data-multiple='false' data-question='問題2<br>あなたは、AI Platformを使用する50人以上のデータサイエンテストからなる成長中のチームで働いています。あなたは、ジョブ、モデル、バージョンをクリーンでスケーラブルな方法で整理する戦略を設計しています。<br>どの戦略を選ぶべきですか？' data-answer='0' data-explanation='解説<br>正解は「ラベルを使用して、リソースを説明的なカテゴリに整理します。作成した各リソースにラベルを適用して、ユーザーがリソースを表示または監視するときに、ラベルによって結果をフィルタリングできるようにします」です。<br>この問題では、AI Platformを使用して仕事をしている成長中のデータサイエンテストのチームのために、効率的な組織作りの戦略を設計する方法を探しています。求められているスケーラブルで清潔な組織法は、ジョブ、モデル、バージョンといったリソースの管理を念頭に置いています。可能性のある戦略には、ラベルを使った説明的なカテゴリーによる整理、IAMのアクセス制限、プロジェクトに分ける方法、Cloud Loggingを用いた監視や分析があります。このような選択肢の中から、データサイエンテストのチームがスケーラブルで整理された方法で作業ができる最適な戦略を見つけることが求められています。<br>基本的な概念や原則：<br>AI Platform：機械学習モデルの訓練、デプロイ、予測を一元化して管理できるGoogle Cloudのフルマネージドサービスです。<br>リソースのラベル：Google Cloudのリソースを整理、フィルタリングし、それらに対する操作を制御するためのメタデータです。<br>IAM権限：Google CloudのIAM（Identity and Access Management）は、特定のユーザーがリソースに対してどのようなアクションを実行できるかを制御します。<br>Google Cloudプロジェクト：Google Cloudのリソースを管理、組織化するための方法です。各プロジェクトは独立した管理領域であり、アクセス制御と設定が適用されます。<br>Cloud Logging：Google Cloudのアプリケーションとサービスからのログを管理、保管し、それらを分析するためのサービスです。<br>BigQuery：Google Cloudの大規模なデータ分析サービスです。SQLクエリを使用してデータを探索し、見つけ出すことができます。<br>正解についての説明：<br>（選択肢）<br>・ラベルを使用して、リソースを説明的なカテゴリに整理します。作成した各リソースにラベルを適用して、ユーザーがリソースを表示または監視するときに、ラベルによって結果をフィルタリングできるようにします<br>この選択肢が正解の理由は以下の通りです。<br>まず、Google Cloudのラベル機能は、リソースを整理し、追跡するための非常に効率的な手法です。ラベルはキーと値のペアを使用してリソースにメタデータを追加することができます。これにより、ユーザーは複雑な環境内でのリソース管理を容易に行うことができます。<br>また、データサイエンテストの多い状況下では、個々のリソースにラベルを付けることで、特定のジョブ、モデル、バージョンの追跡や、それらがどのプロジェクトに関連しているのかを容易に把握することが可能になります。これは特に大規模なチームや成長途上の組織にとって重要なポイントであり、業務の効率化に寄与します。<br>さらに、ラベルを使用すれば、視覚化ツールやオートメーションツールを使用してリソースを監視し、その利用状況を理解することもでき、適切なリソース管理とコスト管理の実現に寄与します。<br>したがって、この選択肢は最も適切な戦略です。<br>不正解の選択肢についての説明：<br>選択肢：AI Platformノートブックに制限的なIAM権限を設定し、単一のユーザーまたはグループのみが特定のインスタンスにアクセスできるようにします<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platformのノートブックに制限的なIAM権限を設定することは、リソースの整理やスケーラブルな管理には寄与しません。正解選択肢のようにラベルを使用することで、リソースを効率的に分類し、管理することができます。<br>選択肢：各データサイエンテストの作業を別のプロジェクトに分け、各データサイエンテストが作成したジョブ、モデル、バージョンにそのユーザーだけがアクセスできるようにします<br>この選択肢が正しくない理由は以下の通りです。<br>各データサイエンテストの作業を別のプロジェクトに分けると、コラボレーションとリソースの再利用が難しくなるため、スケーラブルではありません。ラベルを利用した整理の方が、チーム全体としてリソースを効率的に管理できます。<br>選択肢：AI Platformリソースの使用状況に関する情報を取得するために適切にフィルタリングされたCloud LoggingログのBigQueryシンクを設定します。BigQueryで、ユーザーを使用しているリソースにマッピングするSQLビューを作成します<br>この選択肢が正しくない理由は以下の通りです。<br>BigQueryとCloud Loggingログを用いるアプローチは情報の取得と分析に役立ちますが、リソースの"整理"にそのままつながる訳ではありません。<br>逆に、ラベルを使用すれば、各リソースを直接的に整理し、フィルタリングすることができます。'>
<div class='choice'> ラベルを使用して、リソースを説明的なカテゴリに整理します。作成した各リソースにラベルを適用して、ユーザーがリソースを表示または監視するときに、ラベルによって結果をフィルタリングできるようにします</div>
<div class='choice'> AI Platformリソースの使用状況に関する情報を取得するために適切にフィルタリングされたCloud LoggingログのBigQueryシンクを設定します。BigQueryで、ユーザーを使用しているリソースにマッピングするSQLビューを作成します</div>
<div class='choice'> AI Platformノートブックに制限的なIAM権限を設定し、単一のユーザーまたはグループのみが特定のインスタンスにアクセスできるようにします</div>
<div class='choice'> 各データサイエンテストの作業を別のプロジェクトに分け、各データサイエンテストが作成したジョブ、モデル、バージョンにそのユーザーだけがアクセスできるようにします</div>
</div>

<div class='question' data-multiple='false' data-question='問題3<br>あなたはある銀行のMLエンジニアです。経営陣はあなたに、指紋に基づいて顧客の身元を確認する、MLベースのバイオメトリクス認証をアプリ用に構築するよう依頼しました。指紋は非常に機密性の高い個人情報とみなされ、銀行のデータベースにダウンロードして保存することはできません。<br>このMLモードをトレーニングし、デプロイするために、どの学習戦略を推奨しますか？' data-answer='1' data-explanation='解説<br>正解は「フェデレーテッドラーニング」です。<br>この問題では、非常に機密性の高い個人情報の取り扱いに関する制約と、それを用いて機械学習モデルをトレーニングし、デプロイするという要求を理解することが必要です。データを銀行のデータベースに保存できないので、データを安全に扱いつつ学習する戦略が求められます。選択肢は全て情報の保護対策やプライバシー対策に関連していますが、設問はモデルの学習とデプロイに関するものなので、これらの観点を考慮して解答を選ぶ必要があります。<br>基本的な概念や原則：<br>フェデレーテッドラーニング：個々のデバイス上で機械学習モデルを訓練し、集約することで全体のモデルを改善する学習手法です。個人データを中央のサーバーに送信することなく、プライバシーを保ちながら学習を進めることができます。<br>Cloud Data Loss Prevention API：機密データを検出、分類、保護するためのAPIです。しかし、データ暗号化の問題解決には向いていません。<br>MD5：暗号化の一つの手法ですが、衝突の問題があるため、現在はセキュリティ対策としては非推奨となっています。<br>差分プライバシー：個人のプライバシーを保護するために、一部の情報をランダムに変更または追加し、データ集合から個々のデータを識別できないようにする手法です。ただし、機密データの保存問題には対応していません。<br>正解についての説明：<br>（選択肢）<br>・フェデレーテッドラーニング<br>この選択肢が正解の理由は以下の通りです。<br>個々のデバイスが持つセンシティブなデータをクラウドにアップロードするのではなく、フェデレーテッドラーニングはモデルの学習を各デバイス上で行います。<br>その後、得られた結果（パラメータの更新）だけを集めて、全体の学習モデルを更新します。これによって、機密性の高い個人情報がデバイスを離れることなく、モデルの学習と改善が可能です。<br>今回のシナリオでは、指紋という高度に機密な情報を扱うため、これらの情報がデバイスを離れてデータセンターに保存されることは適切ではありません。そのため、フェデレーテッドラーニングがセキュリティ要件を満たしながら、MLベースのバイオメトリクス認証の実装を可能にする、最良の選択肢です。<br>不正解の選択肢についての説明：<br>選択肢：Cloud Data Loss Prevention API<br>この選択肢が正しくない理由は以下の通りです。<br>Cloud Data Loss Prevention APIは、データのプライバシーやセキュリティのリスクを洗い出し、軽減することが可能ですが、MLモデルのトレーニングやデプロイには使用できません。<br>一方、フェデレーテッドラーニングは、各デバイスでローカルにデータを学習し、その学習結果のみを集約する方式なので、敏感な個人情報が必要なMLモデルのトレーニングに適しています。<br>選択肢：MD5による暗号化<br>この選択肢が正しくない理由は以下の通りです。<br>MD5による暗号化はデータの一方向のハッシュ化手段であり、機密データの保護に対する一定の効果はありますが、元のデータを復元することができないためMLモデルの学習には使えません。<br>一方、フェデレーテッドラーニングはデータのプライバシーを保持したままで分散型の学習を可能にします。<br>選択肢：差分プライバシー<br>この選択肢が正しくない理由は以下の通りです。<br>差分プライバシーは、データ集合から特定の個体の情報を匿名化してデータ分析を可能にする手法であり、この場合指紋データのダウンロードや保存自体が許可されていないため適用できません。<br>一方、フェデレーテッドラーニングはデバイス上でモデル更新を行い、それを集約する学習方法であり、指紋データをデバイス外に持ち出すことなく学習できます。'>
<div class='choice'> 差分プライバシー</div>
<div class='choice'> フェデレーテッドラーニング</div>
<div class='choice'> MD5による暗号化</div>
<div class='choice'> Cloud Data Loss Prevention API</div>
</div>

<div class='question' data-multiple='false' data-question='問題4<br>あなたは銀行のMLエンジニアです。あなたは、顧客がローンの支払いを期限内に行うかどうかを予測するために、AutoML Tableを使ってバイナリ分類モデルを開発しました。その出力は、融資要求の承認または拒絶に使われます。ある顧客のローン要求があなたのモデルによって却下されました。銀行のリスク部門は、モデルの決定に寄与した理由を提供するようあなたに求めています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='2' data-explanation='解説<br>正解は「予測から局所的な特徴の重要性を利用します」です。<br>この問題では、AutoML Tableが開発したバイナリ分類モデルの特定の結果に対する説明可能性を提供する方法について求められています。その結果がどの特徴によって影響されたかを理解する必要があります。問題は、特定の予測に対する説明を要求しているため、全般的な特徴の重要性や閾値の特定ではなく、個々の予測結果に対する局所的な特徴の重要性を考慮することを念頭に置く必要があります。<br>基本的な概念や原則：<br>局所的な特徴の重要性：予測全体ではなく、個々の予測に寄与する特徴の重要度のことです。特定の予測結果について説明する際に有用です。<br>AutoML Tables：構造化データを扱う機械学習モデルを自動的に構築するためのGoogle Cloudのサービスです。特にバイナリ分類、多クラス分類、回帰予測のために設計されています。<br>バイナリ分類：目的変数が二値（はい/いいえ、正/負など）の分類問題です。機械学習の常用タスクの一つです。<br>モデル解釈性：機械学習モデルの予測に対する理解を深めるための方法です。特徴の重要度、局所的な特徴の重要性などを用いて予測結果を説明します。<br>正解についての説明：<br>（選択肢）<br>・予測から局所的な特徴の重要性を利用します<br>この選択肢が正解の理由は以下の通りです。<br>局所的な特徴の重要性は特定の入力データに対するモデルの予測結果に寄与する特徴の重要性を示すものです。これにより、特定の予測がなされた理由を理解することが可能になります。AutoML Tablesでは、予測APIを使用して特徴の重要性を取得することができます。これにより、モデルが特定のローン要求を却下した理由を正確に示すことができ、銀行のリスク部門がその決定を理解し、説明するための根拠を提供できます。この機能はモデルの判断基準を透明化し、さまざまな関係者に対して説明責任を果たすために重要です。<br>したがって、局所的な特徴の重要性を利用することは、このシナリオでの適切な対応です。<br>不正解の選択肢についての説明：<br>選択肢：データサマリーページの目標値との相関を使用します<br>この選択肢が正しくない理由は以下の通りです。<br>データサマリーページの目標値との相関は特徴の一般的な影響力を示しますが、具体的な予測（この場合は特定の顧客のローン申請の却下）に対する特徴の寄与を示すものではありません。<br>それに対して、局所的な特徴の重要性を利用すると、特定の予測に対する各特徴の寄与を知ることができます。<br>選択肢：モデルの評価ページで、特徴の重要度のパーセンテージを使用します<br>この選択肢が正しくない理由は以下の通りです。<br>モデルの評価ページでの特徴の重要度は、全体的なモデルの動作やパフォーマンスを理解するためのものであり、個々の予測結果について寄与した理由を理解するためのものではありません。<br>一方、局所的な特徴の重要性を利用することで、具体的な予測や決定に対する各特徴の影響を理解でき、個々の顧客に対する決定の理由を提供できます。<br>選択肢：特徴量を独立に変化させ、分類を変化させる特徴量ごとの閾値を特定します<br>この選択肢が正しくない理由は以下の通りです。<br>特徴量を独立に変化させ、分類を変化させる特徴量ごとの閾値を特定する方法は特徴量の関連性を無視してしまうため、一貫性のある振る舞いを見ることができません。<br>一方、局所的な特徴の重要性を利用することによって、各特徴が予測にどの程度影響を与えたかを直接示すことができます。'>
<div class='choice'> モデルの評価ページで、特徴の重要度のパーセンテージを使用します</div>
<div class='choice'> データサマリーページの目標値との相関を使用します</div>
<div class='choice'> 予測から局所的な特徴の重要性を利用します</div>
<div class='choice'> 特徴量を独立に変化させ、分類を変化させる特徴量ごとの閾値を特定します</div>
</div>

<div class='question' data-multiple='false' data-question='問題5<br>あなたは、個人を特定できる情報（PII）を含む可能性のあるファイルをGoogle Cloudにストリーミングするリアルタイム予測エンジンを構築しています。Cloud Data Loss Prevention（DLP）APIを使用してファイルをスキャンしたいと考えています。<br>PIIが権限のない個人によってアクセスされないようにするにはどうすればよいですか？' data-answer='0' data-explanation='解説<br>正解は「データの3つのバケット（隔離、機密、および非機密）を作成します。すべてのデータを隔離バケットに書き込みます。DLP APIを使用してそのバケットの一括スキャンを定期的に実行し、データを機密バケットまたは非機密バケットに移動します」です。<br>この問題では、リアルタイムの予測エンジンを構築している中で個人を特定できる情報（PII）を含む可能性のあるファイルの取り扱いについて課題が提出されています。Google CloudのDLP APIを使ってこれらのファイルを安全に処理する最善の方法を選ぶことが求められています。特に、PIIの保護とアクセス制限、そしてこれらのファイルの安全な管理や移動が問題の中心です。適切なデータバケットの設定や情報の振り分け、定期的なスキャンの行い方がこの問題を解決する上で重要な要素です。<br>基本的な概念や原則：<br>Cloud Data Loss Prevention（DLP）：Google Cloudのセキュリティサービスで、セキュリティの脅威やリスクを特定、管理、減少させる機能を提供します。特に、機密情報の保護やデータのプライバシー保護に役立ちます。<br>PII（Personal Identifiable Information）：個人を特定できる情報のことで、名前、住所、電話番号、社会保障番号などが含まれます。PIIは適切に保護すべきデータです。<br>DLP API：Data Loss Prevention APIの略で、Google Cloud DLPをプログラムから操作するためのAPIです。機密データの検出、分類、保護が可能です。<br>バケット：Google Cloud Storageでデータを分類するための単位です。機密なデータを非機密なデータから区別するために、異なるバケットに格納します。<br>BigQuery：Google Cloudのサービスで、大規模なデータセットに対する高速なSQLクエリの実行を可能にします。<br>一括スキャン：DLP APIを使用して、特定のデータソース全体を検索し、機密またはPIIデータを検出するプロセスです。<br>正解についての説明：<br>（選択肢）<br>・データの3つのバケット（隔離、機密、および非機密）を作成します。すべてのデータを隔離バケットに書き込みます。DLP APIを使用してそのバケットの一括スキャンを定期的に実行し、データを機密バケットまたは非機密バケットに移動します<br>この選択肢が正解の理由は以下の通りです。<br>まず、Cloud Data Loss Prevention（DLP）APIを使用して"隔離"バケットのデータを定期的に一括スキャンすることで、予測エンジンを通じてストリーミングされた新しいデータに含まれる可能性のある個人を特定できる情報（PII）を効果的に検出できます。このようにして検出された機密情報を含むデータは"機密"バケットに移動され、それ以外のデータは"非機密"バケットに移動されます。<br>これにより、権限のない個人がアクセス可能な範囲内にあるのは非機密データだけになり、機密情報は適切に保護されます。これはPIIデータを保護するための一般的な方法であり、Cloud DLP APIのような自動検出ツールを利用することでリアルタイムの予測エンジンでも実現できます。<br>不正解の選択肢についての説明：<br>選択肢：すべてのファイルをGoogle Cloudにストリーミングし、データをBigQueryに書き込みます。DLP APIを使用して定期的にテーブルの一括スキャンを行います<br>この選択肢が正しくない理由は以下の通りです。<br>まず、リアルタイム予測エンジンを使用しているため、すぐにPIIのデータを分離する必要があります。<br>しかし、この選択肢ではBigQueryにすべてのデータを書き込んでからDLP APIを使用して一括スキャンを行うため、書き込みとスキャンの間に時間差が発生し、その間に権限のない個人がPIIにアクセスする可能性があります。<br>選択肢：すべてのファイルをGoogle Cloudにストリーミングし、データのバッチをBigQueryに書き込みます。データをBigQueryに書き込んでいる間に、DLP APIを使用してデータの一括スキャンを実行します<br>この選択肢が正しくない理由は以下の通りです。<br>BigQueryにPIIを含む可能性のあるデータを直接書き込んだ場合、書き込みとスキャンの間のラグ時間で権限のないユーザーがデータにアクセスする可能性があります。隔離バケットを用いて一旦データを隔離することで、このようなリスクを防ぐことができます。<br>選択肢：機密データと非機密データの2つのバケットを作成します。すべてのデータを非機密バケットに書き込みます。DLP APIを使用してそのバケットの一括スキャンを定期的に実行し、機密データを機密バケットに移動します<br>この選択肢が正しくない理由は以下の通りです。<br>機密データと非機密データの2つのバケットを作成し、すべてのデータを非機密バケットに書き込む場合、DLP APIのスキャンが行われるまでの間、潜在的なPIIが一時的に公開され、非許可のアクセスが可能になる可能性があります。よって、最初に全てのデータを隔離バケットに入れ、その後分類する方が適切です。'>
<div class='choice'> データの3つのバケット（隔離、機密、および非機密）を作成します。すべてのデータを隔離バケットに書き込みます。DLP APIを使用してそのバケットの一括スキャンを定期的に実行し、データを機密バケットまたは非機密バケットに移動します</div>
<div class='choice'> 機密データと非機密データの2つのバケットを作成します。すべてのデータを非機密バケットに書き込みます。DLP APIを使用してそのバケットの一括スキャンを定期的に実行し、機密データを機密バケットに移動します</div>
<div class='choice'> すべてのファイルをGoogle Cloudにストリーミングし、データのバッチをBigQueryに書き込みます。データをBigQueryに書き込んでいる間に、DLP APIを使用してデータの一括スキャンを実行します</div>
<div class='choice'> すべてのファイルをGoogle Cloudにストリーミングし、データをBigQueryに書き込みます。DLP APIを使用して定期的にテーブルの一括スキャンを行います</div>
</div>

<div class='question' data-multiple='false' data-question='問題6<br>あなたは製造業のデータサイエンスチームに所属しています。あなたは、数億レコードある会社の過去の販売データをレビューしています。探索的データ分析のために、平均値、中央値、最頻値などの記述統計量を計算し、仮説検定のための複雑な統計検定を実施し、時系列での特徴の変化をプロットする必要があります。計算リソースを最小限に抑えながら、できるだけ多くの売上データを分析に使用したいと考えています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='0' data-explanation='解説<br>正解は「BigQueryを使用して記述統計量を計算します。Vertex Albenchユーザー管理型ノートブックを使用して、タイムプロットを可視化し、統計分析を実行します」です。<br>この問題では、大量の販売データを効率的に分析し、記述統計量を計算し、統計検定を行い、時系列プロットを作成する方法を探しています。問題から対象となるデータが大量であり、探索的データの分析が求められていることが理解できます。また、限定された計算リソースを最小限に抑えつつ最大限の売上データを分析することが求められています。その上で描かれている具体的な作業は平均値、中央値、最頻値などの記述統計量の計算、仮説検定のための統計検定、時系列での特徴の変化を描出することであり、これらの要素から適切なGoogle Cloudのサービスを選択することが解決策の中心です。<br>基本的な概念や原則：<br>BigQuery：Google Cloudのフルマネージドなエンタープライズデータウェアハウスで、大規模な分析用途向けです。数億、数十億のレコードを超高速に分析することができます。<br>記述統計量：データの特性を要約して示す統計量です。平均値、中央値、最頻値などが含まれます。<br>Vertex AI：Google Cloudの統合型MLプラットフォームで、データサイエンテストやMLエンジニアがモデル開発からデプロイまで一貫したワークフローをトレースできます。<br>ノートブック：コーディング、数式の表現、ビジュアライゼーションなどを一つのドキュメントで行えるツールで、データ分析や機械学習に用いられます。<br>Google Data Studio：Google Cloudのビジュアライゼーションツールで、データの視覚化や報告を作成できます。<br>仮説検定：統計的手法を使用して仮説が正しい確率を評価する手法です。データから導かれた結果が偶然に起こったのか否かを判断します。<br>探索的データ分析：統計的手法を用いて未知のデータセットを調査するアプローチで、データの特性やパターンを理解し、問題解決に役立てることが目的です。<br>正解についての説明：<br>（選択肢）<br>・BigQueryを使用して記述統計量を計算します。Vertex Albenchユーザー管理型ノートブックを使用して、タイムプロットを可視化し、統計分析を実行します<br>この選択肢が正解の理由は以下の通りです。<br>まず、BigQueryは大量のデータを高速に率直なSQL構文で操作できるGoogle Cloudのフルマネージド、サーバレス型のエンタープライズデータウェアハウスです。数億レコードのような大規模なデータセットに対する計算能力を持っているため、平均値や中央値、最頻値といった記述統計量の計算に適しています。<br>また、BigQueryはサーバレス型のサービスなので、使用していないときは経費をかけずにクエリも迅速に実行でき、計算リソースを最小限に抑える要件を満たします。<br>次に、Vertex AIの一部であるVertex AI Notebooksは、記述統計量だけでなく機械学習や統計分析などを実行するためのフレキシブルな環境を提供します。ユーザー管理型のノートブックは、自分の好みに合わせてカスタマイズでき、必要なライブラリやツールを簡単に追加できます。統計分析や仮説検定を行うためにはVertex Albenchユーザー管理型ノートブックが適しています。<br>さらに、Jupyterノートブックはデータの可視化にも優れ、時系列での特徴の変化をプロットする要件も満たします。<br>不正解の選択肢についての説明：<br>選択肢：Google Data Studioでタイムプロットを可視化します。データセットをVertex Altex Workbenchのユーザー管理型ノートブックにインポートします。このデータを使って記述統計量を計算し、統計解析を実行します<br>この選択肢が正しくない理由は以下の通りです。<br>Google Data Studioは可視化ツールであり、複雑な統計的分析や記述統計量の計算には適していません。<br>また、数億レコードのデータをVertex AI Workbenchのノートブックにインポートすることは、計算リソースを必要以上に消費し、コストと時間が増大する可能性があります。<br>一方、BigQueryは大量のデータを効率的に分析できます。<br>選択肢：Vertex Albenchユーザー管理型ノートブックインスタンスをスピンアップし、データセットをインポートします。このデータを使って統計解析やビジュアル解析を行います<br>この選択肢が正しくない理由は以下の通りです。<br>数億レコードのデータセットを直接インポートしてVertex AI中で統計解析を行うと、計算リソースが大量に必要になり、最小限のリソースでの分析という条件を満たしません。<br>一方、BigQueryで記述統計量を計算すると、大量のデータを効率的に分析できます。<br>選択肢：BigQueryを使用して記述統計量を計算し、Google Data Studioを使用してタイムプロットを可視化します。Vertex Altex Workbenchユーザー管理型ノートブックを使用して統計解析を実行します<br>この選択肢が正しくない理由は以下の通りです。<br>Google Data Studioはデータの視覚化とレポート作成に強力ですが、複雑な統計解析や仮説検定の機能を持っていません。そのため、複雑な統計分析が必要な場合には不適切です。<br>一方、Vertex AIのユーザー管理型ノートブックでは、強力な計算と分析機能を活用できます。<br>また、"Vertex Altex Workbench"は存在しないため選択肢が間違っています。'>
<div class='choice'> BigQueryを使用して記述統計量を計算します。Vertex Albenchユーザー管理型ノートブックを使用して、タイムプロットを可視化し、統計分析を実行します</div>
<div class='choice'> Vertex Albenchユーザー管理型ノートブックインスタンスをスピンアップし、データセットをインポートします。このデータを使って統計解析やビジュアル解析を行います</div>
<div class='choice'> Google Data Studioでタイムプロットを可視化します。データセットをVertex Altex Workbenchのユーザー管理型ノートブックにインポートします。このデータを使って記述統計量を計算し、統計解析を実行します</div>
<div class='choice'> BigQueryを使用して記述統計量を計算し、Google Data Studioを使用してタイムプロットを可視化します。Vertex Altex Workbenchユーザー管理型ノートブックを使用して統計解析を実行します</div>
</div>

<div class='question' data-multiple='false' data-question='問題7<br>あなたは大企業のコンタクトセンターのMLエンジニアです。あなたは、録音された電話での会話から顧客のセンチメントを予測するセンチメント分析ツールを構築する必要があります。コンタクトセンターに電話をかけてきた顧客の性別、年齢、文化の違いが、モデル開発パイプラインと結果のどの段階にも影響しないようにしながら、モデルを構築する最善のアプローチを特定する必要があります。<br>この要件を満たすために、どうすればよいですか？' data-answer='3' data-explanation='解説<br>正解は「音声をテキストに変換し、文章に基づいて感情を抽出します」です。<br>この問題では、個々の顧客の特性が結果に影響を与えないように、音声録音から顧客のセンチメントを分析するツールの構築方法について問われています。問題文の情報を正確に把握し、音声からの情報抽出方法を選ぶ際に、その方法が顧客のセンチメントだけを反映し、顧客の個々の特性（性別、年齢、文化的背景等）に影響されないことを確認することが必要です。<br>基本的な概念や原則：<br>音声認識：人間の音声言語をテキストデータに変換するテクノロジーです。録音された音声から情報を抽出する際に使用されます。<br>センチメント分析：テキストデータからその内容の感情や態度を分析するプロセスです。顧客の感情を理解するためによく使用されます。<br>機械学習モデル：特定のタスクを自動で行うように設計されたアルゴリズムや数学的モデルのことです。データから学習して、何か特定の作業を自動化します。<br>構文解析：テキストの構造を分析するプロセスで、文法や意味を理解するために使用されます。<br>不偏性：AIシステムが特定のグループに対して偏った結果を出さないようにする考え方です。AIモデルの訓練データは、対象とする全てのグループを公平に代表するよう選ばれるべきです。<br>正解についての説明：<br>（選択肢）<br>・音声をテキストに変換し、文章に基づいて感情を抽出します<br>この選択肢が正解の理由は以下の通りです。<br>まず、音声をテキストに変換することで、性別、年齢、文化の違いがモデル開発パイプラインと結果に影響を及ぼす可能性を大幅に削減できます。これは、音声の特徴など、個々の話者の特性を省き、情報の核心であるテキストの内容に焦点を当てるためです。<br>したがって、性別、年齢、文化などの個人の特性によるバイアスを避けることができます。<br>次に、文章に基づいて感情を抽出することにより、コンテキストとニュアンスをより正確に理解することができます。これは、感情分析の精度を高めるために重要です。<br>また、テキストベースの感情抽出は一般的なテキスト分析と同じように扱うことができるため、既存のNLP（自然言語処理）ツールや技術を利用して効率的に分析を進めることができます。<br>したがって、"音声をテキストに変換し、文章に基づいて感情を抽出する"アプローチは、上記の要件を満たすために最適な方法です。<br>不正解の選択肢についての説明：<br>選択肢：音声をテキストに変換し、単語に基づいてモデルを構築します<br>この選択肢が正しくない理由は以下の通りです。<br>単語単位でモデルを構築する方法は文脈を無視するため、顧客の感情を正確に理解するのが難しくなります。一方で文章全体に基づいて感情を抽出することで、より正確にセンチメント分析を行うことができます。<br>選択肢：音声記録から直接センチメントを抽出します<br>この選択肢が正しくない理由は以下の通りです。<br>音声記録から直接センチメントを抽出すると、話者の性別、年齢、文化による音声の特徴が分析結果に影響し、不公平な結果を導き出しやすくなります。<br>一方、音声をテキストに変換してセンチメントを抽出すると、これらの要素からの影響は軽減されます。<br>選択肢：音声をテキストに変換し、構文解析を用いて感情を抽出します<br>この選択肢が正しくない理由は以下の通りです。<br>顧客の性別、年齢、文化の違いを取り除くためには、感情分析に構文解析を用いるとそれらの要素を含む可能性があります。そのため、文章全体に基づいて感情を抽出する方が、必要な要件を満たす確率が高いです。'>
<div class='choice'> 音声記録から直接センチメントを抽出します</div>
<div class='choice'> 音声をテキストに変換し、構文解析を用いて感情を抽出します</div>
<div class='choice'> 音声をテキストに変換し、単語に基づいてモデルを構築します</div>
<div class='choice'> 音声をテキストに変換し、文章に基づいて感情を抽出します</div>
</div>

<div class='question' data-multiple='false' data-question='問題8<br>あなたは、新しい動画ストリーミングプラットフォームを開発している会社に勤めています。あなたは、ユーザーが次に見るべき動画を提案するレコメンデーションシステムの作成を依頼されました。AI倫理チームによるレビューの後、あなたは開発を開始することが承認されました。あなたの会社のカタログにある各ビデオアセットには有用なメタデータ（コンテンツの種類、リリース日、国など）がありますが、過去のユーザーイベントデータはありません。<br>最初のバージョンでは、どのようにレコメンデーションシステムを構築すべきですか？' data-answer='3' data-explanation='解説<br>正解は「機械学習なしで製品を立ち上げます。コンテンツのメタデータに基づく単純なヒューリスティックを使用して、ユーザーに類似した動画を推薦し、将来的に推薦モデルを開発できるように、ユーザーのイベントデータの収集を開始します」です。<br>この問題では、過去のユーザーイベントデータが無い状態でどのようにレコメンデーションシステムを構築すべきかを問われています。ユーザーの行動履歴が不足しているため、初期段階ではメタデータに基づくヒューリスティックを使うことが求められる点が重要です。また、将来的にレコメンデーションモデルを開発できるように、ユーザーのイベントデータの収集を始める必要があります。一方で、最初から高度な機械学習モデルを走らせるのではなく、ある程度ユーザーデータが蓄積されてからその投入を考えることも大切です。この問題では、初期段階での高度なテクノロジーの採用よりも、状況に適したシンプルなアプローチを選ぶことが理想的な解答です。<br>基本的な概念や原則：<br>ヒューリスティック：問題解決や意思決定において、厳密なロジックや明確な方法よりも経験や直感に基づいて行われる方法です。AIやレコメンデーションシステムを最初に建設する際には、初期の推薦の根拠として使用されることがあります。<br>メタデータ：データを説明、整理、管理するために使用されるデータのデータです。動画ストリーミングサービスでは、動画コンテンツのタイトル、リリース日、ジャンルなどの情報がこれに該当します。<br>イベントデータ：ユーザーの行動やシステムのイベントに関するデータです。レコメンデーションシステムでは、ユーザーの視聴履歴やクリック履歴などのユーザーイベントデータが重要な役割を果たします。<br>AI倫理：AIシステム設計と運用における道徳的、社会的な面の考慮です。AIチームがレコメンデーションシステムの設計と実装を進める前に、この視点からのレビューが行われることがあります。<br>機械学習：AIの分野で、特定のタスクを効果的に実行するためにシステムが経験から学習する技術のことです。しかし、適切な量と質の訓練データが必要で、存在しない場合はヒューリスティックやエキスパートシステムを使用することがあります。<br>公開データセット：全ての人々が利用できるように公開されたデータの集合です。しかし、特定のビジネスニーズやコンテキストに完全には適していない可能性もあります。<br>オートエンコーダ：機械学習モデルの一種で、主にデータの圧縮やノイズ除去に使用されます。しかし、これを使用するには十分な量と質の訓練データが必要です。<br>正解についての説明：<br>（選択肢）<br>・機械学習なしで製品を立ち上げます。コンテンツのメタデータに基づく単純なヒューリスティックを使用して、ユーザーに類似した動画を推薦し、将来的に推薦モデルを開発できるように、ユーザーのイベントデータの収集を開始します<br>この選択肢が正解の理由は以下の通りです。<br>まず、ユーザーの過去のイベントデータが不足しているため、初めから深い機械学習モデルに頼ることは難しいです。そのため、ビデオアセットに付随するメタデータに基づく単純なヒューリスティック（経験則に基づく解決策）を使用して、利用者に対する推薦を始めることが理にかなっています。機械学習の効果的なレコメンデーションシステムは、大量の学習データを必要とするので、ここで推薦する作業を開始することで、ユーザーの反応に基づくデータ収集を開始することができます。これにより、今後更なる改善のために機械学習モデルを開発する際に、必要なユーザーの行動ヒストリーデータを手に入れることができます。つまり、この選択肢は、手元のデータを活用しつつ、後続の改善に備えるというバランスの良さを持っています。<br>不正解の選択肢についての説明：<br>選択肢：機械学習なしで製品をローンチします。ユーザーにビデオをアルファベット順に提示し、将来レコメンダーモデルを開発できるように、ユーザーのイベントデータの収集を開始します<br>この選択肢が正しくない理由は以下の通りです。<br>ビデオをアルファベット順に提示するというアプローチは、ユーザー体験にとって価値が低く、無作為すぎます。<br>対照的に、コンテンツのメタデータに基づくヒューリスティックを使用すると、ユーザーにとってより関連性のあるコンテンツの提供が可能になります。<br>選択肢：機械学習で製品を立ち上げます。MovieLensのような一般に公開されているデータセットを使用して、レコメンデーションAIを使用してモデルを訓練し、この訓練されたモデルをあなたのデータに適用します<br>この選択肢が正しくない理由は以下の通りです。<br>MovieLensなどの公開データセットを使ってレコメンデーションモデルを訓練したとしても、それは必ずしも自社のビデオのメタデータやユーザーの動向に適合するわけではありません。そのため、自社の基準や特性に応じたモデル設計が必要となるためです。<br>また、過去のユーザーイベントデータがない初期段階では、メタデータに基づくヒューリスティックを用い、ユーザーイベントデータ収集を開始するのが適切と言えます。<br>選択肢：機械学習で製品を立ち上げます。TensorFlowを使ってコンテンツのメタデータにオートエンコーダを学習させ、各動画の埋め込みデータを生成します。これらのエンベッディングの類似性に基づいてコンテンツをクラスタリングし、同じクラスターから動画を推薦します<br>この選択肢が正しくない理由は以下の通りです。<br>まず、過去のユーザーイベントデータがないという前提の下では、機械学習を用いた製品の立ち上げは効果的ではありません。TensorFlowを用いるその方法はユーザー行動データに基づいて学習しますが、今回はそのデータがありません。それに対して正解の選択肢は、現有のメタデータをベースにした単純な推奨ロジックを使用しつつ、今後のモデル開発に向けたデータの収集を進める戦略を提案しています。'>
<div class='choice'> 機械学習で製品を立ち上げます。MovieLensのような一般に公開されているデータセットを使用して、レコメンデーションAIを使用してモデルを訓練し、この訓練されたモデルをあなたのデータに適用します</div>
<div class='choice'> 機械学習なしで製品をローンチします。ユーザーにビデオをアルファベット順に提示し、将来レコメンダーモデルを開発できるように、ユーザーのイベントデータの収集を開始します</div>
<div class='choice'> 機械学習で製品を立ち上げます。TensorFlowを使ってコンテンツのメタデータにオートエンコーダを学習させ、各動画の埋め込みデータを生成します。これらのエンベッディングの類似性に基づいてコンテンツをクラスタリングし、同じクラスターから動画を推薦します</div>
<div class='choice'> 機械学習なしで製品を立ち上げます。コンテンツのメタデータに基づく単純なヒューリスティックを使用して、ユーザーに類似した動画を推薦し、将来的に推薦モデルを開発できるように、ユーザーのイベントデータの収集を開始します</div>
</div>

<div class='question' data-multiple='false' data-question='問題9<br>あなたはサブスクリプションベースの企業に勤めています。あなたは、顧客が年間サブスクリプションを更新しない可能性である顧客解約を予測するために、ツリーとニューラルネットワークのアンサンブルをトレーニングした。平均的な予測は15%の解約率ですが、ある顧客については、モデルは70%の確率で解約すると予測しました。その顧客は製品の使用履歴が30%あり、ニューヨークに住んでいて、1997年に顧客になりました。あなたは、実際の予測である70％の解約率と平均予測との違いを説明する必要があります。あなたはVertex Explainable AIを使いたいと考えています。<br>この要件を満たすために、どうすればよいですか？' data-answer='2' data-explanation='解説<br>正解は「Vertex Explainable AIでサンプリングされたShapley説明を構成します」です。<br>この問題では、Vertex Explainable AIを使用して、ある特定の顧客が解約するという予測結果が出た場合のその理由を説明する方法を問います。問題には、解約率を予測するためのモデルと、予測結果について顧客に説明を求められたということ実が提示されています。Vertex Explainable AIにおける異なる説明方法についての理解と、それぞれがどのようなシナリオや目的に適しているのかという理解が求められます。適切な説明方法を選択するためには、解釈可能性と予測の精度との間のバランスを考慮して調整する必要があります。<br>基本的な概念や原則：<br>Vertex AI： Google Cloudの一部で、AIモデリングと運用作業を統一的に行い、AI開発を加速するプラットフォームです。<br>Vertex Explainable AI：AIモデルの動作を明確にするためのツールです。各予測や特徴重要度、モデルの動作を説明するサンプリングされたShapley説明を提供します。<br>Shapley説明：協力ゲーム理論から派生した、特徴の貢献度を評価する方法です。予測に対する各特徴の影響を公平に分配するために使用されます。<br>ローカルサロゲートモデル：複雑なモデルの予測を説明するために用いられる単純なモデルです。ローカルサロゲートモデルは、特定のデータポイントに焦点を当て、個々の予測を説明することを目指します。<br>グラデーションの説明：特徴量の微小な変動が予測にどの程度影響を及ぼすかを説明するための方法です。モデルが線形でない場合や特徴間の相互作用がある場合は、不十分な情報を提供する可能性があります。<br>特徴の効果の測定：特徴の重みに特徴値をかけることで、その特徴が予測にどのくらい影響を持つかを評価する方法です。これは線形モデルには適していますが、非線形モデルでは必ずしも適していない可能性があります。<br>正解についての説明：<br>（選択肢）<br>・Vertex Explainable AIでサンプリングされたShapley説明を構成します<br>この選択肢が正解の理由は以下の通りです。<br>まず、Vertex AIはGoogle Cloudの統合されたML（Machine Learning）プラットフォームであり、予測モデルのトレーニングから展開までの一連のプロセスを統合的に管理することができます。その一部として提供されているのが、Explainable AIです。Explainable AIは、モデルの予測結果に対して"なぜその結果になったのか"という説明を提供する機能を持っています。これにより、モデルが特定の予測を行った理由や該当データに対するモデルの挙動を理解しやすくなります。特にShapley説明は、特徴量の寄与度を把握するための有効な手法であり、特定の予測結果が平均からどれだけ外れているのか、またその要因が何であるのかを詳細に分析することができます。<br>したがって、平均予測と顧客別の予測との違いを理解し、説明するためには、Shapley説明を用いたExplainable AIが適切と言えます。<br>不正解の選択肢についての説明：<br>選択肢：個々の予測を説明するためにローカルサロゲートモデルを訓練します<br>この選択肢が正しくない理由は以下の通りです。<br>ローカルサロゲートモデルは、大きなブラックボックスモデルの動作を近似するための解釈可能なモデルを訓練することに使われますが、個々の予測値を説明するためには不適切です。<br>一方で、Vertex Explainable AIのShapley説明は個々の予測に対する特徴の影響を量化するため、予測の説明に適しています。<br>選択肢：Vertex Explainable AIに統合されたグラデーションの説明を設定します<br>この選択肢が正しくない理由は以下の通りです。<br>グラデーションの説明は特徴量の重要度を可視化する方法でありますが、それはローカルな予測のためのどの特徴が影響を与えたかについての具体的な洞察を提供しません。この要件ではローカルな予測に対する解釈が求められていて、それはShapley値を使用して満足させることができます。<br>選択肢：各特徴の効果を、特徴の重みに特徴値を掛けたものとして測定します<br>この選択肢が正しくない理由は以下の通りです。<br>各特徴の効果を、特徴の重みに特徴値を掛けたものとして測定する方法は、具体的な特徴の寄与を理解するには直感的ではなく、全体の予測モデルの解釈において限定的です。<br>一方、Vertex Explainable AIのShapley説明は、特徴の貢献度を厳密に算出でき、各顧客に対する個々の予測の理解に役立ちます。'>
<div class='choice'> 各特徴の効果を、特徴の重みに特徴値を掛けたものとして測定します</div>
<div class='choice'> 個々の予測を説明するためにローカルサロゲートモデルを訓練します</div>
<div class='choice'> Vertex Explainable AIでサンプリングされたShapley説明を構成します</div>
<div class='choice'> Vertex Explainable AIに統合されたグラデーションの説明を設定します</div>
</div>

<div class='question' data-multiple='false' data-question='問題10<br>あなたは大手小売企業に勤務しており、購買行動によって顧客をセグメント化するよう依頼されています。全顧客の購買履歴はBigQueryにアップロードされています。あなたは、顧客セグメントが複数存在するのではないかと考えていますが、その数は不明であり、その行動の共通点もまだ理解していません。また、最も効率的なソリューションを見つけたいと考えています。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='2' data-explanation='解説<br>正解は「BigQuery MLを使用してk-meansクラスタリングモデルを作成します。BigQueryがクラスター数を自動的に最適化できるようにします」です。<br>この問題では、大手小売企業に勤務しており、全顧客の購買履歴がBigQueryに保存されており、顧客の購買行動に基づいてセグメンテーションを行うことが求められています。この要件を満たすための効率的な解決策を求められています。顧客のセグメント数が不明であり、その行動の共通性もまだ理解できていないという情報から、教師なし機械学習の手法が有効であることが示唆されています。ソリューションを選ぶ際には、顧客セグメントを自動的に検出し、最適化することができるツールや技術を重視すべきです。<br>基本的な概念や原則：<br>BigQuery：Google Cloudのフルマネージドなエンタープライズデータウェアハウスで、大規模データを扱うことができ、SQLクエリを使って分析を行うことができます。<br>BigQuery ML：BigQueryのデータに対して機械学習を適用する機能です。SQLのみを用いてMLモデルを作成、評価、予測することができます。<br>k-meansクラスタリング：教師なし学習の一つ。データをk個のクラスターに分けるために使用します。一般的に、類似性のあるデータを同じクラスターに、類似性のないデータを別のクラスターに分けます。<br>Dataprep: データをビジュアル化して探索し、クリーニングや変換を行うのに役立つインタラクティブなWebベースのツールです。<br>Data Labeling Service: データにラベルを付けるためのサービスで、機械学習の学習データを作る際に利用されます。<br>AutoML Tables: テーブル形式のデータを利用して自動的に機械学習モデルを作成するためのサービスです。<br>Data Studio: ビジュアル化するための報告ツールで、BigQueryやその他のソースからのデータを簡単にビジュアル化することができます。<br>正解についての説明：<br>（選択肢）<br>・BigQuery MLを使用してk-meansクラスタリングモデルを作成します。BigQueryがクラスター数を自動的に最適化できるようにします<br>この選択肢が正解の理由は以下の通りです。<br>まず、クラスタリングはデータセットを自然なグループに分けるための一般的なテクニックであり、この問題での要件、つまり何個の顧客セグメントが存在するか不明で、その行動の共通点が理解できていないという状況に非常に適しています。k-meansクラスタリングはこのような状況に対してソリューションを提供する最も一般的なアルゴリズムの一つであり、データポイントをk個のクラスターに分けることを試みます。<br>次に、BigQuery MLはBigQuery内で直接機械学習モデルを作成、訓練、評価、予測する機能を提供するツールであり、この問題の要件に一致しています。全顧客の購買履歴がBigQueryにアップロードされており、また最も効率的なソリューションを見つけたいという依頼があるため、BigQuery MLを使用すれば、データの移動や別途の機械学習プラットフォームの導入などの手間を省いて効率的に解析を行うことが可能になります。<br>また、BigQuery MLはk-meansクラスタリングモデルの作成をサポートしており、クラスターの数を自動的に最適化する機能も提供しています。これにより、クラスターの数が不明な状況でも最適な解を見つけ出すことができます。<br>不正解の選択肢についての説明：<br>選択肢：DataprepでBigQueryテーブルを参照する新しいデータセットを作成します。Dataprepを使用して、各カラム内の類似性を識別します<br>この選択肢が正しくない理由は以下の通りです。<br>Dataprepはデータクレンジングやトランスフォームのためのツールであり、k-meansのような高度なクラスタリングアルゴリズムを実行する能力はありません。そのため、顧客の行動パターンの共通点を理解し、不特定のクラスター数でセグメンテーションするための最も効率的なソリューションはBigQuery MLとなります。<br>選択肢：Data Labeling Serviceを使用して、BigQueryの各顧客レコードにラベルを付けます。AutoML Tablesを使用して、ラベル付けされたデータに対してモデルをトレーニングします。評価メトリクスを確認し、データに根本的なパターンがあるかどうかを理解します<br>この選択肢が正しくない理由は以下の通りです。<br>まず、Data Labeling ServiceとAutoML Tablesは教師あり学習を使用し、既知のラベルが必要です。<br>しかし、今回セグメントの数や特性は不明であるため、この方法が適切ではありません。対してBigQuery MLのk-meansクラスタリングは、未知のクラスターを見つける教師なし学習の一種で、この問題に適しています。<br>選択肢：あなたの会社のマーケティングチームから顧客セグメントのリストを入手します。Data Labeling Serviceを使用して、リストに従ってBigQueryの各顧客レコードにラベルを付けます。Data Studioを使用して、データセット内のラベルの分布を分析します<br>この選択肢が正しくない理由は以下の通りです。<br>この選択肢では手動でのラベル付けを必要とし、その後にデータ分析を行います。これは効率的な方法ではありません。<br>また、最初に顧客のセグメント数が不明で行動の共通点もまだ理解されていないことを考慮すると、BigQuery MLを使用したクラスタリングが自動的で効率的な解です。'>
<div class='choice'> DataprepでBigQueryテーブルを参照する新しいデータセットを作成します。Dataprepを使用して、各カラム内の類似性を識別します</div>
<div class='choice'> Data Labeling Serviceを使用して、BigQueryの各顧客レコードにラベルを付けます。AutoML Tablesを使用して、ラベル付けされたデータに対してモデルをトレーニングします。評価メトリクスを確認し、データに根本的なパターンがあるかどうかを理解します</div>
<div class='choice'> BigQuery MLを使用してk-meansクラスタリングモデルを作成します。BigQueryがクラスター数を自動的に最適化できるようにします</div>
<div class='choice'> あなたの会社のマーケティングチームから顧客セグメントのリストを入手します。Data Labeling Serviceを使用して、リストに従ってBigQueryの各顧客レコードにラベルを付けます。Data Studioを使用して、データセット内のラベルの分布を分析します</div>
</div>

<div class='question' data-multiple='false' data-question='問題11<br>あなたは最近、何千ものデータセットを持つエンタープライズ規模の企業に入社しました。あなたはBigQueryの各テーブルに正確な説明があることを知っており、AI Platformで構築するモデルに使用する適切なBigQueryテーブルを探しています。<br>必要なデータはどのように見つけるべきですか？' data-answer='1' data-explanation='解説<br>正解は「Data Catalogを使用して、テーブルの説明のキーワードを使用してBigQueryデータセットを検索します」です。<br>この問題では、エンタープライズ規模の企業でAIモデル構築のために適切なテーブルを効率良く探す方法が求められています。Google BigQueryのテーブルには説明が存在しており、それを活用する必要があります。この情報に従って、Google Cloudのサービスの中からデータを探すための最適なツールを選択することが求められています。また、各データ管理手法のコストや効率性、考慮すべき点などを総合的に考えながら選択することが必要です。ここで注意すべきは、情報検索のための手法が直訳すると明らかになるわけではなく、読み解きを通じて選択肢を理解することです。<br>基本的な概念や原則：<br>Data Catalog：Google Cloudのメタデータ管理サービスです。適切なデータアセットの検索や利用、ガバナンスのために作成されています。BigQueryテーブルのようなデータアセットについての集中的なメタデータのビューを提供します。<br>BigQuery：Google Cloudのフルマネージドな大規模データ分析サービスです。SQLクエリを使用して大量のデータに対して高速に分析を行うことができます。<br>AI Platform：Google Cloudの機械学習モデルのトレーニング、デプロイ、予測を行うプラットフォームです。BigQueryのテーブルからデータを取得してモデルをトレーニングすることができます。<br>タグ付け：Google Cloudのリソースに追加情報を付加する方法です。ただし、この場合のようにデータアセットの検索には適していません。<br>ルックアップテーブル：特定の情報を迅速に検索するためのデータ構造です。ただし、大量のテーブルを管理する場合には適していません。<br>INFORMATION_SCHEMA：データベースのメタデータを提供する一連のビューです。ただし、大量のテーブルを横断的に検索するには不適切です。<br>正解についての説明：<br>（選択肢）<br>・Data Catalogを使用して、テーブルの説明のキーワードを使用してBigQueryデータセットを検索します<br>この選択肢が正解の理由は以下の通りです。<br>Data CatalogはGoogle Cloudが提供する、データの発見と理解を助けるメタデータ管理サービスです。このサービスは、Google Cloudに存在するデータ資源に対する一元化された視点を提供し、ユーザーが容易にデータを見つけることができるようにします。BigQueryテーブルの場合、Data Catalogを使用することで、各テーブルに関連するメタデータや説明を含む情報を照会し、特定のキーワードに基づいてデータセットを検索することが可能になります。数千のデータセットの中から適切なものを探すためには、このような検索やフィルタリング機能が必須となりますので、Data Catalogの使用が推奨されます。なお、BigQueryデータセット内のテーブルには通常、その内容や使用目的に関する説明が含まれているので、これを利用することで必要なデータを短時間で見つけ出すことができます。<br>不正解の選択肢についての説明：<br>選択肢：AI Platform上のモデルとバージョンの各リソースに、トレーニングに使用したBigQueryテーブルの名前をタグ付けします<br>この選択肢が正しくない理由は以下の通りです。<br>AI Platform上のモデルとバージョンのリソースにタグを付ける行為は、既に存在するテーブルを特定するのに役立つかもしれませんが、新たに適切なテーブルを見つけることはできません。逆にData Catalogはメタデータの検索機能を提供しており、キーワードによるテーブルの探索が可能なため、このシナリオに適しています。<br>選択肢：テーブルの説明とテーブルIDを対応付けるルックアップテーブルをBigQueryで管理します。ルックアップテーブルをクエリして、必要なデータの正しいテーブルIDを見つけます<br>この選択肢が正しくない理由は以下の通りです。<br>ルックアップテーブルをBigQueryで管理するという方法は、多数のデータセットを手動で管理する労力が必要になり、効率が悪く時間もかかります。<br>一方、Data Catalogを使用すれば、自動的にメタデータを集約・管理し、キーワード検索が可能で効率的に必要なデータを見つけることができます。<br>選択肢：BigQueryでクエリを実行し、BigQueryネイティブのINFORMATION_SCHEMAメタデータテーブルを使用して、プロジェクトに存在するすべてのテーブル名を取得します。その結果を使用して、必要なテーブルを見つけます<br>この選択肢が正しくない理由は以下の通りです。<br>BigQueryのINFORMATION_SCHEMAでテーブル名を取得することは可能ですが、これだけではテーブルの説明や使用するべきデータがそのテーブルに存在するかどうかは分かりません。<br>一方、Data Catalogを使えば、テーブルの説明やメタデータを検索し、より具体的なデータを探し出すことができます。'>
<div class='choice'> テーブルの説明とテーブルIDを対応付けるルックアップテーブルをBigQueryで管理します。ルックアップテーブルをクエリして、必要なデータの正しいテーブルIDを見つけます</div>
<div class='choice'> Data Catalogを使用して、テーブルの説明のキーワードを使用してBigQueryデータセットを検索します</div>
<div class='choice'> BigQueryでクエリを実行し、BigQueryネイティブのINFORMATION_SCHEMAメタデータテーブルを使用して、プロジェクトに存在するすべてのテーブル名を取得します。その結果を使用して、必要なテーブルを見つけます</div>
<div class='choice'> AI Platform上のモデルとバージョンの各リソースに、トレーニングに使用したBigQueryテーブルの名前をタグ付けします</div>
</div>

<div class='question' data-multiple='false' data-question='問題12<br>あなたは、幅広い要因に基づいて株式市場の動向を予測するMLモデルを構築しています。データを探索するうちに、いくつかの特徴が大きな範囲を持っていることに気づきます。あなたは、最大の大きさを持つ特徴がモデルにオーバーフィットしないようにしたいです。<br>あなたはこの要件を満たすために、どうすればよいですか？' data-answer='1' data-explanation='解説<br>正解は「データが0と1の間の値になるようにスケーリングして正規化します」です。<br>この問題では、特定の特徴が過度に大きい範囲を持つ機械学習モデルのデータへの影響と、その対策について理解しておく必要があります。この種の問題を解く際には、機械学習モデルを改善するための一般的なテクニックと、その適用範囲についての知識を当てはめることが重要です。この状況において最も適切な対策を考えるためには、それぞれの選択肢がモデルの性能とオーバーフィットにどのような影響を与えるかを考慮することが求められます。<br>基本的な概念や原則：<br>正規化：データセット内の特徴量が異なる範囲にわたる場合、それらを一貫した範囲にスケーリングするプロセスです。一般的には0と1の間ます。これは、特定の特徴量の大きさによる過学習を防ぎます。<br>オーバーフィッティング：学習データに対して過剰に適合した結果、新しいデータに対する汎化能力が低下する現象です。正規化は、オーバーフィッティングを防ぐためのツールの一つです。<br>標準化：データを平均0、標準偏差1の分布に変換することです。これは異なる特徴間のスケールを揃え、メトリクスによる学習を容易にしますが、特徴の範囲が非常に大きい場合、過学習への影響を十分に制御できないかもしれません。<br>主成分分析（PCA）：データの次元を減らして視覚化に役立たせる手法ですが、特徴の影響を最小化するためには、問題の本質的なパターンを描き出すための主要な変分をキャプチャする場合に限ります。<br>ビニング：連続変数を離散的なビン（カテゴリまたは"バケット"）に分割するデータ整理戦略です。ビニングは一部の分析で有用ですが、特徴の範囲が大きい場合には、必ずしも過学習の問題を解決しない可能性があります。<br>正解についての説明：<br>（選択肢）<br>・データが0と1の間の値になるようにスケーリングして正規化します<br>この選択肢が正解の理由は以下の通りです。<br>特徴量のスケールが大きいと、それがモデルの学習に過度に影響を与え、オーバーフィットの原因となることがあります。これは特に、モデルが特徴量の大小に依存する学習アルゴリズム（例えば、勾配降下法を用いた機械学習モデル）を使用する場合に特に顕著です。<br>特徴量のスケーリングと正規化は、この問題を緩和するための一般的な手法です。特徴量を0と1の間の値にスケーリングすることで、すべての特徴量が同などの重みを持つようになり、大きな範囲を持つ特徴量がモデルに過度の影響を与えるのを防ぐことができます。これはMLモデルがデータからパターンを学習する際にバランスを取るのに役立ちます。<br>したがって、データを0と1の間の値にスケーリングして正規化するという選択肢は、大きな範囲を持つ特徴量がモデルにオーバーフィットしないようにするための理想的な手段と言えます。<br>不正解の選択肢についての説明：<br>選択肢：データを対数関数で変換して標準化します<br>この選択肢が正しくない理由は以下の通りです。<br>データを対数関数で変換して標準化することもデータを0と1の間の値にスケーリングすることも、両方ともモデルのオーバーフィッティングを防ぐ方策になります。<br>しかし、問題には大きな範囲を持つ特徴を持つデータに対する対応策としているので、0と1の間に正規化することがより適しています。対数変換では、大きな値と小さな値の差は相対的に保持されるため、大きな範囲を持つ特徴のスケールを効果的に縮小することは難しいです。<br>選択肢：主成分分析（PCA）を適用し、特定の特徴の影響を最小化します<br>この選択肢が正しくない理由は以下の通りです。<br>主成分分析（PCA）はデータの次元削減を行う手法で、特定の特徴の影響を最小化するものではありません。<br>また、PCAはオーバーフィットを防ぐ直接的な手段ではなく、特徴のスケーリングと比べ、分析の複雑さが増します。<br>対して0と1の間の値にスケーリングして正規化することで、特徴間のスケールの違いが影響を及ぼすのを防ぎ、オーバーフィットのリスクを減らすことができます。<br>選択肢：ビニング戦略を使用して、各特徴の大きさを適切なビン番号に置き換えます<br>この選択肢が正しくない理由は以下の通りです。<br>ビニング戦略はデータの大きさを一定の区間にカテゴライズする方法ですが、それだけでは特徴の範囲によるオーバーフィットの問題を解消できません。<br>一方、0と1の間にスケーリングすることで正規化すると、すべての特徴の範囲が統一され、オーバーフィットを防げます。'>
<div class='choice'> 主成分分析（PCA）を適用し、特定の特徴の影響を最小化します</div>
<div class='choice'> データが0と1の間の値になるようにスケーリングして正規化します</div>
<div class='choice'> ビニング戦略を使用して、各特徴の大きさを適切なビン番号に置き換えます</div>
<div class='choice'> データを対数関数で変換して標準化します</div>
</div>


            <!-- 他の問題も同様に追加 -->
        </div>

        <h2 id="question"></h2>
        <ul class="choices" id="choices"></ul>
        <button onclick="checkAnswer()">採点</button>
        <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
        <div class="result" id="result"></div>
    </div>

    <script>
        let currentQuestionIndex = 0;
        let correctCount = 0;
        const questions = [];

        document.addEventListener('DOMContentLoaded', () => {
            const questionElements = document.querySelectorAll('#quiz-data .question');
            questions.push(...Array.from(questionElements).map(questionElement => ({
                question: questionElement.getAttribute('data-question').replace(/\\n/g, '<br>'),
                choices: Array.from(questionElement.querySelectorAll('.choice')).map((choice, index) => ({
                    text: choice.innerHTML.replace(/\\n/g, '<br>'),  // innerHTMLに変更
                    index: index
                })),
                correctAnswer: questionElement.getAttribute('data-answer').split(',').map(Number),
                explanation: questionElement.getAttribute('data-explanation').replace(/\\n/g, '<br>'),
                multiple: questionElement.getAttribute('data-multiple') === 'true'
            })));
            showQuestion();
        });

        function shuffleArray(array) {
            for (let i = array.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [array[i], array[j]] = [array[j], array[i]];
            }
        }

        function showQuestion() {
            const questionElement = document.getElementById('question');
            const choicesContainer = document.getElementById('choices');
            const currentQuestion = questions[currentQuestionIndex];

            shuffleArray(currentQuestion.choices);

            questionElement.innerHTML = currentQuestion.question;
            choicesContainer.innerHTML = '';

            currentQuestion.choices.forEach((choice, i) => {
                const li = document.createElement('li');
                const input = document.createElement('input');
                const label = document.createElement('label');

                input.type = currentQuestion.multiple ? 'checkbox' : 'radio';
                input.name = 'choice';
                input.value = choice.index;
                input.id = 'choice' + i;

                label.htmlFor = 'choice' + i;
                label.innerHTML = choice.text;  // textContentをinnerHTMLに変更

                li.appendChild(input);
                li.appendChild(label);
                choicesContainer.appendChild(li);
            });

            document.getElementById('result').textContent = "";
            document.getElementById('nextButton').style.display = 'none';
        }

        function checkAnswer() {
            const currentQuestion = questions[currentQuestionIndex];
            const selectedChoices = Array.from(document.querySelectorAll('input[name="choice"]:checked'))
                                        .map(checkbox => parseInt(checkbox.value))
                                        .sort();
            const resultElement = document.getElementById('result');
            
            if (selectedChoices.length > 0) {
                const isCorrect = currentQuestion.multiple
                    ? selectedChoices.toString() === currentQuestion.correctAnswer.sort().toString()
                    : selectedChoices.length === 1 && selectedChoices[0] === currentQuestion.correctAnswer[0];
                
                if (isCorrect) {
                    resultElement.innerHTML = "正解です！<br>" + currentQuestion.explanation;
                    resultElement.style.color = "green";
                    correctCount++; // 正解数をカウント
                } else {
                    resultElement.innerHTML = "残念、不正解です。<br>" + currentQuestion.explanation;
                    resultElement.style.color = "red";
                }
                document.getElementById('nextButton').style.display = 'inline';
            } else {
                resultElement.textContent = "回答を選択してください。";
                resultElement.style.color = "orange";
            }
        }

        function nextQuestion() {
            currentQuestionIndex++;
            
            if (currentQuestionIndex < questions.length) {
                showQuestion();
            } else {
                showFinalResult();
            }
        }

        function showFinalResult() {
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2>問題終了！</h2>
                <p>あなたの正解数は ${correctCount} / ${questions.length} です。</p>
                <button onclick="restartQuiz()">再挑戦する</button>
            `;
        }

        function restartQuiz() {
            correctCount = 0;
            currentQuestionIndex = 0;

            // クイズのUI全体を初期化
            const quizContainer = document.querySelector('.quiz-container');
            quizContainer.innerHTML = `
                <h2 id="question"></h2>
                <ul class="choices" id="choices"></ul>
                <button onclick="checkAnswer()">採点</button>
                <button onclick="nextQuestion()" id="nextButton" style="display: none;">次の問題へ</button>
                <div class="result" id="result"></div>
            `;

            // 初期化後に最初の問題を表示
            showQuestion();
        }        
    </script>
</body>
</html>